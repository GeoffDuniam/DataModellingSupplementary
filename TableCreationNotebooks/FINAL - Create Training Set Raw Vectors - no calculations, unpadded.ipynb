{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training set basic vector dataframe - unpadded, no calculations\n",
    "\n",
    "## First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # manual garbag collection to stop leaks on Collect() gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 35\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full raw training set of feature vectors, unpadded, no calculations\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays.\n",
    "\n",
    "#### Note that there is no cast or collect_list to array (array (collect_list(...) )\n",
    "This changes the sghape of the retrieved aray in the programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE!!\n",
    "Be sure that the datatypes are explicitly defined in the collect_list statement!\n",
    "\n",
    "Otherwise is (apparently) loks like hive will implicitly case to a string(? Not sure) but the end result is that if you don't, the table data footprint end up almost twice as large as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,target,meta,specz,\n",
    "collect_list( int( a.kv['passband'])) as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'mjd',                  collect_list(float(a.kv['mjd'])) ,\n",
    "    'flux',                 collect_list(float(a.kv['flux'])) ,\n",
    "    'flux_err',             collect_list(float(a.kv['flux_err'])) ,\n",
    "    'detected',             collect_list(float(a.kv['detected'])) \n",
    "    )\n",
    ") as hist\n",
    "from\n",
    "(\n",
    "select ts.object_id,target,array(ddf,hostgal_specz, hostgal_photoz, hostgal_photoz_err, mwebv) as meta,double(hostgal_specz) as specz,\n",
    "   MAP(\n",
    "        'mjd', mjd,\n",
    "        'passband',passband,\n",
    "        'flux',flux,\n",
    "        'flux_err',flux_err,\n",
    "        'detected',detected\n",
    "   ) AS kv\n",
    "from training_set ts inner join training_set_metadata tsm on ts.object_id = tsm.object_id\n",
    ") a\n",
    "group by object_id, target,meta, specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "struct_df.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- hist: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- mjd: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- passband: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [object_id#5, target#22, meta#0, specz#1, band#3, hist#4]\n",
      "   +- InMemoryRelation [object_id#5, target#22, meta#0, specz#1, band#3, hist#4], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- ObjectHashAggregate(keys=[object_id#5, target#22, meta#0, specz#1], functions=[collect_list(kv#2[passband], 0, 0), collect_list(kv#2[mjd], 0, 0), collect_list(kv#2[passband], 0, 0), collect_list(kv#2[flux], 0, 0), collect_list(kv#2[flux_err], 0, 0), collect_list(kv#2[detected], 0, 0)])\n",
      "            +- Exchange hashpartitioning(object_id#5, target#22, meta#0, specz#1, 200)\n",
      "               +- ObjectHashAggregate(keys=[object_id#5, target#22, meta#0, specz#1], functions=[partial_collect_list(kv#2[passband], 0, 0), partial_collect_list(kv#2[mjd], 0, 0), partial_collect_list(kv#2[passband], 0, 0), partial_collect_list(kv#2[flux], 0, 0), partial_collect_list(kv#2[flux_err], 0, 0), partial_collect_list(kv#2[detected], 0, 0)])\n",
      "                  +- *(2) Project [object_id#5, target#22, array(cast(ddf#16 as double), hostgal_specz#17, hostgal_photoz#18, hostgal_photoz_err#19, mwebv#21) AS meta#0, hostgal_specz#17 AS specz#1, map(mjd, mjd#6, passband, cast(passband#7 as double), flux, flux#8, flux_err, flux_err#9, detected, cast(detected#10 as double)) AS kv#2]\n",
      "                     +- *(2) BroadcastHashJoin [object_id#5], [object_id#11], Inner, BuildRight\n",
      "                        :- *(2) Filter isnotnull(object_id#5)\n",
      "                        :  +- HiveTableScan [object_id#5, mjd#6, passband#7, flux#8, flux_err#9, detected#10], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#5, mjd#6, passband#7, flux#8, flux_err#9, detected#10]\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                           +- *(1) Filter isnotnull(object_id#11)\n",
      "                              +- HiveTableScan [object_id#11, ddf#16, hostgal_specz#17, hostgal_photoz#18, hostgal_photoz_err#19, mwebv#21, target#22], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#11, ra#12, decl#13, gal_l#14, gal_b#15, ddf#16, hostgal_specz#17, hostgal_photoz#18, hostgal_photoz_err#19, distmod#20, mwebv#21, target#22]\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the feature vector table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "#TABLE='training_set_vectors' - original full hist ARRAY - STRUCT - ARRAY\n",
    "TABLE='training_raw_vectors_unpadded_no_calcs'\n",
    "\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
