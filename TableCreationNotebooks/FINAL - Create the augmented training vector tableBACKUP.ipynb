{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the padded training_set\n",
    "We need to create the padded training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = \"training_set\"\n",
    "training_metadata = \"training_set_metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedSQL=\"\"\"\n",
    "with\n",
    "cnt\n",
    "as\n",
    "(\n",
    "    select rownum from \n",
    "    (\n",
    "        select row_number() over (ORDER BY object_id) as rownum\n",
    "        from {}\n",
    "    ) a\n",
    "    where rownum <=256\n",
    "),\n",
    "objects as (select object_id, 0 padMJD, 0 padPassband,0 padFlux, 0 padFlux_err,0 padDetected from {} group by object_id),\n",
    "baseline as (select * from objects CROSS JOIN cnt ), -- cartesian product with 256 values to use as the baseline\n",
    "train_set as (select *, row_number() over (partition by object_id order by mjd desc) as rownum from {}),\n",
    "paddedRev as (\n",
    "    select baseline.object_id, --train_set.mjd, baseline.padMJD,\n",
    "    case when train_set.mjd is null then baseline.padMJD else train_set.mjd end mjd,\n",
    "    case when train_set.passband is null then baseline.padPassband else train_set.passband end passband,\n",
    "    case when train_set.flux is null then baseline.padFlux else train_set.mjd end flux,\n",
    "    case when train_set.flux_err is null then baseline.padFlux_err else train_set.flux_err end flux_err,\n",
    "    case when train_set.detected is null then baseline.padDetected else train_set.detected end detected\n",
    "    from baseline left outer join train_set on baseline.object_id = train_set.object_id and baseline.rownum=train_set.rownum\n",
    "    order by baseline.object_id, mjd desc\n",
    ")\n",
    "select object_id, mjd, passband, flux, flux_err, detected from paddedRev order by object_id, mjd\n",
    "\"\"\".format(training_metadata,training_set,training_set)\n",
    "paddedTrainingSet_DF = sqlContext.sql(paddedSQL)\n",
    "paddedTrainingSet_DF.registerTempTable(\"PADDED_TRAINING_SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataSQL=\"\"\"\n",
    "select object_id,\n",
    "ra,decl,gal_l,gal_b,ddf,hostgal_specz,\n",
    "rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5) hostgal_photoz,\n",
    "hostgal_photoz_err, distmod,mwebv,target\n",
    "from {}\n",
    "\"\"\".format(training_metadata)\n",
    "metadata_DF = sqlContext.sql(metadataSQL)\n",
    "metadata_DF.registerTempTable(\"AUGMENTED_METADATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedSQL=\"\"\"\n",
    "with New_Training_set as(\n",
    "    select ts.object_id, ts.mjd, ts.passband,ts.flux,\n",
    "   rand()*((ts.flux+ts.flux_err)-((ts.flux-ts.flux_err)/1.5))+((ts.flux-ts.flux_err)/1.5)  newFlux,\n",
    "    ts.flux_err,ts.detected,\n",
    "    (1+tsm.hostgal_photoz)/( 1+ (rand()*((tsm.hostgal_photoz+tsm.hostgal_photoz_err)-((tsm.hostgal_photoz-tsm.hostgal_photoz_err)/1.5))+((tsm.hostgal_photoz-tsm.hostgal_photoz_err)/1.5))) dt\n",
    "    from {} ts\n",
    "        inner join training_set_metadata tsm\n",
    "            on ts.object_id = tsm.object_id\n",
    ")\n",
    "select new_training_set.object_id,\n",
    "new_training_set.mjd*new_training_set.dt as mjd,  new_training_set.passband, new_training_set.newflux as flux, \n",
    "new_training_set.flux_err, new_training_set.detected\n",
    "from New_Training_set\n",
    "\"\"\".format(\"PADDED_TRAINING_SET\")\n",
    "\n",
    "augmented_DF = sqlContext.sql(paddedSQL)\n",
    "augmented_DF.registerTempTable(\"AUGMENTED_TRAINING\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def AugmentTheTableSparkAppend(): \n",
    "    augmentInsertSQL=\"\"\"\n",
    "    with \n",
    "        CTE1 as ( \n",
    "            select object_id, mjd,\n",
    "            mjd - first_value(mjd) over w as mjdInt,\n",
    "            case when lag(mjd) OVER w is null then\n",
    "                0\n",
    "            else\n",
    "                mjd - lag(mjd) over w \n",
    "            end as deltaMjd,\n",
    "            passband,\n",
    "            flux,\n",
    "            flux_err,\n",
    "            detected,\n",
    "            row_number() OVER w as rownum\n",
    "            from v_augment_the_training_set\n",
    "            WINDOW w AS (PARTITION BY object_id ORDER BY mjd)\n",
    "        ),\n",
    "            CTE2 as (\n",
    "            select object_id, --mjd, lag(mjd) OVER x as lag,\n",
    "            --first_value(mjd) OVER x as fval,\n",
    "            first_value(mjd) OVER x - mjd as rval,\n",
    "            row_number() OVER x as rownum\n",
    "            from v_augment_the_training_set\n",
    "            WINDOW x AS (PARTITION BY object_id ORDER BY mjd DESC)\n",
    "        ),\n",
    "        meta as (\n",
    "            select object_id, gal_l, gal_b, ddf, hostgal_specz, hostgal_photoz, hostgal_photoz_err, mwebv,target,\n",
    "            case when hostgal_photoz > 0 \n",
    "                then 1  -- CAST(1 AS BOOLEAN)\n",
    "                else 0 --CAST(0 AS BOOLEAN)\n",
    "                end as photoz_positive,\n",
    "            --6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99\n",
    "            case \n",
    "                when target= 6 then 0\n",
    "                when target= 15 then 1\n",
    "                when target= 16 then 2\n",
    "                when target= 42 then 3\n",
    "                when target= 52 then 4\n",
    "                when target= 53 then 5\n",
    "                when target= 62 then 6\n",
    "                when target= 64 then 7\n",
    "                when target= 65 then 8\n",
    "                when target= 67 then 9\n",
    "                when target= 88 then 10\n",
    "                when target= 90 then 11\n",
    "                when target= 92 then 12\n",
    "                when target= 95 then 13\n",
    "                when target= 99 then 14\n",
    "                else 14\n",
    "                end mapped_target\n",
    "\n",
    "            from v_augment_the_metadata\n",
    "        ),\n",
    "        struct as\n",
    "        (\n",
    "            select CTE1.object_id,meta.mapped_target as target,\n",
    "            array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,photoz_positive,0) as meta,\n",
    "            double(hostgal_specz) as specz,\n",
    "            MAP(\n",
    "                'interval', mjdInt,\n",
    "                'deltaMjd', deltaMjd,\n",
    "                'passband',passband,\n",
    "                'rval', rval,\n",
    "                'flux',flux,\n",
    "                'flux_err',flux_err,\n",
    "                'detected',detected,\n",
    "                'received_frequency', case \n",
    "                                        when CTE1.passband = 0 then 300000 /357\n",
    "                                        when CTE1.passband = 1 then 300000 /477\n",
    "                                        when CTE1.passband = 2 then 300000 /621\n",
    "                                        when CTE1.passband = 3 then 300000 /754\n",
    "                                        when CTE1.passband = 4 then 300000 /871\n",
    "                                        else  300000 / 1004\n",
    "                                        end,\n",
    "                'source_wavelength', case \n",
    "                                        when CTE1.passband = 0 then 357 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 1 then 477 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 2 then 621 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 3 then 754 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 4 then 871 / (meta.hostgal_photoz + 1)\n",
    "                                        else 1004 / (meta.hostgal_photoz + 1)\n",
    "                                        end\n",
    "\n",
    "            ) AS kv\n",
    "            from CTE1 \n",
    "                inner join CTE2\n",
    "                    on CTE1.object_id=CTE2.object_id\n",
    "                    and CTE1. rownum=CTE2.rownum\n",
    "                inner join meta\n",
    "                    on CTE1.object_id = meta.object_id\n",
    "        )    \n",
    "    select object_id,target,meta,specz,\n",
    "    array( collect_list(a.kv['passband']) ) as band,\n",
    "    ARRAY(NAMED_STRUCT(\n",
    "        'interval',             array( collect_list(a.kv['interval']) ),\n",
    "        'deltaMjd',             array( collect_list(a.kv['deltaMjd']) ),\n",
    "        'rval',                 array( collect_list(a.kv['rval']) ),\n",
    "        'flux',                 array( collect_list(a.kv['flux']) ),\n",
    "        'flux_err',             array( collect_list(a.kv['flux_err']) ),\n",
    "        'detected',             array( collect_list(a.kv['detected']) ),\n",
    "        'source_wavelength',    array( collect_list(a.kv['source_wavelength']) ),\n",
    "        'received_wavelength',  array( collect_list(a.kv['received_wavelength']) )\n",
    "        )\n",
    "    ) as hist\n",
    "    from struct a\n",
    "    group by object_id, target,meta, specz\n",
    "    \"\"\"\n",
    "    augmented_df=sqlContext.sql(augmentInsertSQL)\n",
    "    \n",
    "    MODE='append'\n",
    "    FORMAT='parquet'\n",
    "    TABLE='training_set_augmented_vectors'\n",
    "    \n",
    "    augmented_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AugmentTheTableSparkAppend(): \n",
    "    augmentInsertSQL=\"\"\"\n",
    "    with \n",
    "        CTE1 as ( \n",
    "            select object_id, mjd,\n",
    "            mjd - first_value(mjd) over w as mjdInt,\n",
    "            case when lag(mjd) OVER w is null then\n",
    "                0\n",
    "            else\n",
    "                mjd - lag(mjd) over w \n",
    "            end as deltaMjd,\n",
    "            passband,\n",
    "            flux,\n",
    "            flux_err,\n",
    "            detected,\n",
    "            row_number() OVER w as rownum\n",
    "            from {}\n",
    "            WINDOW w AS (PARTITION BY object_id ORDER BY mjd)\n",
    "        ),\n",
    "            CTE2 as (\n",
    "            select object_id, --mjd, lag(mjd) OVER x as lag,\n",
    "            --first_value(mjd) OVER x as fval,\n",
    "            first_value(mjd) OVER x - mjd as rval,\n",
    "            row_number() OVER x as rownum\n",
    "            from {}\n",
    "            WINDOW x AS (PARTITION BY object_id ORDER BY mjd DESC)\n",
    "        ),\n",
    "        meta as (\n",
    "            select object_id, gal_l, gal_b, ddf, hostgal_specz, hostgal_photoz, hostgal_photoz_err, mwebv,target,\n",
    "            case when hostgal_photoz > 0 \n",
    "                then 1  -- CAST(1 AS BOOLEAN)\n",
    "                else 0 --CAST(0 AS BOOLEAN)\n",
    "                end as photoz_positive,\n",
    "            --6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99\n",
    "            case \n",
    "                when target= 6 then 0\n",
    "                when target= 15 then 1\n",
    "                when target= 16 then 2\n",
    "                when target= 42 then 3\n",
    "                when target= 52 then 4\n",
    "                when target= 53 then 5\n",
    "                when target= 62 then 6\n",
    "                when target= 64 then 7\n",
    "                when target= 65 then 8\n",
    "                when target= 67 then 9\n",
    "                when target= 88 then 10\n",
    "                when target= 90 then 11\n",
    "                when target= 92 then 12\n",
    "                when target= 95 then 13\n",
    "                when target= 99 then 14\n",
    "                else 14\n",
    "                end mapped_target\n",
    "\n",
    "            from {}\n",
    "        ),\n",
    "        struct as\n",
    "        (\n",
    "            select CTE1.object_id,meta.mapped_target as target,\n",
    "            array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,photoz_positive,0) as meta,\n",
    "            double(hostgal_specz) as specz,\n",
    "            MAP(\n",
    "                'interval', mjdInt,\n",
    "                'deltaMjd', deltaMjd,\n",
    "                'passband',passband,\n",
    "                'rval', rval,\n",
    "                'flux',flux,\n",
    "                'flux_err',flux_err,\n",
    "                'detected',detected,\n",
    "                'received_frequency', case \n",
    "                                        when CTE1.passband = 0 then 300000 /357\n",
    "                                        when CTE1.passband = 1 then 300000 /477\n",
    "                                        when CTE1.passband = 2 then 300000 /621\n",
    "                                        when CTE1.passband = 3 then 300000 /754\n",
    "                                        when CTE1.passband = 4 then 300000 /871\n",
    "                                        else  300000 / 1004\n",
    "                                        end,\n",
    "                'source_wavelength', case \n",
    "                                        when CTE1.passband = 0 then 357 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 1 then 477 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 2 then 621 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 3 then 754 / (meta.hostgal_photoz + 1)\n",
    "                                        when CTE1.passband = 4 then 871 / (meta.hostgal_photoz + 1)\n",
    "                                        else 1004 / (meta.hostgal_photoz + 1)\n",
    "                                        end\n",
    "\n",
    "            ) AS kv\n",
    "            from CTE1 \n",
    "                inner join CTE2\n",
    "                    on CTE1.object_id=CTE2.object_id\n",
    "                    and CTE1. rownum=CTE2.rownum\n",
    "                inner join meta\n",
    "                    on CTE1.object_id = meta.object_id\n",
    "        )    \n",
    "    select object_id,meta,target,specz,\n",
    "    collect_list(a.kv['passband'])  as band,\n",
    "    collect_list(a.kv['interval']) as interval,\n",
    "    collect_list(a.kv['deltaMjd']) as deltaMjd,\n",
    "    collect_list(a.kv['rval']) as rval,\n",
    "    collect_list(a.kv['flux']) as flux,\n",
    "    collect_list(a.kv['flux_err']) as flux_err,\n",
    "    collect_list(a.kv['detected']) as detected,\n",
    "    collect_list(a.kv['source_wavelength']) as source_wavelength,\n",
    "    collect_list(a.kv['received_wavelength']) as received_wavelength\n",
    "\n",
    "    from struct a\n",
    "    group by object_id, meta, target,specz\n",
    "    \"\"\".format(\"AUGMENTED_TRAINING\",\"AUGMENTED_TRAINING\",\"AUGMENTED_METADATA\")\n",
    "    augmented_df=sqlContext.sql(augmentInsertSQL)\n",
    "    #print(augmented_df.printSchema())\n",
    "    \n",
    "    MODE='append'\n",
    "    FORMAT='parquet'\n",
    "    #TABLE='training_set_augmented_vectors'\n",
    "    TABLE='training_set_flat_augmented_vectors'\n",
    "    \n",
    "    augmented_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "done!\n",
      "1\n",
      "done!\n",
      "2\n",
      "done!\n",
      "3\n",
      "done!\n",
      "4\n",
      "done!\n",
      "5\n",
      "done!\n",
      "6\n",
      "done!\n",
      "7\n",
      "done!\n",
      "8\n",
      "done!\n",
      "9\n",
      "done!\n",
      "10\n",
      "done!\n",
      "11\n",
      "done!\n",
      "--- Full augment - Elapsed - 1165.3623864650726 seconds - Cpu seconds 0.5361880000000001 ---\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "startCpu=time.clock()\n",
    "\n",
    "# first off, we copy the initial set of trining vectrs into the augment table\n",
    "#trainingVectorsDF=sqlContext.sql(\"select * from training_set_vectors\")- original full hist ARRAY - STRUCT - ARRAY\n",
    "trainingVectorsDF=sqlContext.sql(\"select * from training_set_flat_vectors\")\n",
    "    \n",
    "#MODE='overwrite'\n",
    "#FORMAT='parquet'\n",
    "#TABLE='training_set_augmented_vectors'  - original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_flat_augmented_vectors'\n",
    "\n",
    "\n",
    "# comment out the next two lines if you're just augmenting, not creting from scratch\n",
    "#trainingVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n",
    "#print(\"Initial insertion complete...\")\n",
    "\n",
    "# and now, we augment!\n",
    "for i in range(12):\n",
    "    print(i)\n",
    "    AugmentTheTableSparkAppend()\n",
    "    print(\"done!\")\n",
    "\n",
    "end=time.time()-start\n",
    "endCpu=time.clock()-startCpu\n",
    "print(\"--- Full augment - Elapsed - %s seconds - Cpu seconds %s ---\" % (end, endCpu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
