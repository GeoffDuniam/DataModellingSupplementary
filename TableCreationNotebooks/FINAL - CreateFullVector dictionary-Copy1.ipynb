{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full feature vector dataframe \n",
    "## Optimised to broadcast joins and trying to avoid data spills to disk in the executiors\n",
    "\n",
    "### First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '6g'),\\\n",
    "                           ('spark.driver.memory', '10g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.sql.autoBroadcastJoinThreshold', 52428800), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','FullVectorTesting - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('LoadD1_Pictures_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # manual garbag collection to stop leaks on Collect() gc.collect()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from keras.layers import *\n",
    "#from keras.models import Model, load_model\n",
    "#from keras.optimizers import Adam, Nadam, SGD\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "#from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 35\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sqlContext.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# set up hive session for broadcast joins\n",
    "optSql=\"\"\"\n",
    "set hive.cbo.enable=true;\n",
    "set hive.auto.convert.join=true;\n",
    "set hive.auto.convert.join.noconditionaltask=true;\n",
    "set hive.auto.convert.join.noconditionaltask.size=20971520\n",
    "set hive.auto.convert.join.use.nonstaged=true;\n",
    "set hive.mapjoin.smalltable.filesize = 30000000; \n",
    "\"\"\"\n",
    "sqlContext.sql(optSql)\n",
    "\n",
    "select * from training_set_augmented_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the augmented vector training set\n",
    "vectorTable=\"training_set_augmented_vectors\"\n",
    "trainingVectorsDF=sqlContext.sql(\"select * from {}\".format(vectorTable)).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get some data on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "def _to_java_object_rdd(rdd):  \n",
    "    \"\"\" Return a JavaRDD of Object by unpickling\n",
    "    It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "    RDD is serialized in batch or not.\n",
    "    \"\"\"\n",
    "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.763792"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JavaObj = _to_java_object_rdd(trainingVectorsDF.rdd)\n",
    "nbytes = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)\n",
    "nbytes/10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- meta: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hist: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- interval: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- deltaMjd: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- rval: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- source_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- received_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [object_id#0, meta#1, target#2, specz#3, band#4, hist#5]\n",
      "   +- InMemoryRelation [object_id#0, meta#1, target#2, specz#3, band#4, hist#5], true, 10000, StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(1) FileScan parquet plasticc.training_set_augmented_vectors[object_id#0,meta#1,target#2,specz#3,band#4,hist#5] Batched: false, Format: Parquet, Location: InMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/plasticc.db/train..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<object_id:int,meta:array<double>,target:int,specz:double,band:array<array<double>>,hist:ar...\n"
     ]
    }
   ],
   "source": [
    "trainingVectorsDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the training, test and validation splits\n",
    "We'll do this on the metadata table as we use this table for the master joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42 # seed=0L\n",
    "train_df, validation_df, test_df = trainingVectorsDF.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idArr=np.array(train_df.select('object_id').collect(), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c=idArr.shape\n",
    "idArr.reshape(r,)\n",
    "meta_len=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaArr=np.array(train_df.select('meta').collect(), dtype='float32').reshape(r,meta_len)\n",
    "bandArr= np.array(train_df.select('band').collect() , dtype='int32').reshape(r,sequence_len)\n",
    "\n",
    "histArray=np.zeros((r,sequence_len,8), dtype='float32') \n",
    "# this will work brilliantly as get_keras_data sets three columns to zeros anyway\n",
    "\n",
    "#mjdInt=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "deltaMjd=np.array(train_df.select('hist.deltaMjd').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "rval=np.array(train_df.select('hist.rval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "fluxTest=np.array(train_df.select('hist.flux').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "flux_err_test=np.array(train_df.select('hist.flux_err').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "#detected=np.array(train_df.select('hist.detected').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "source_wavelength=np.array(train_df.select('hist.source_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "#received_wavelength=np.array(train_df.select('hist.received_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    X['hist'][:,:,0] = 0 # remove abs time\n",
    "#    X['hist'][:,:,1] = 0 # remove flux\n",
    "#    X['hist'][:,:,2] = 0 # remove flux err\n",
    "    X['hist'][:,:,3] = 0 # remove detected flag\n",
    "#    X['hist'][:,:,4] = 0 # remove fwd intervals\n",
    "#    X['hist'][:,:,5] = 0 # remove bwd intervals\n",
    "#    X['hist'][:,:,6] = 0 # remove source wavelength\n",
    "    X['hist'][:,:,7] = 0 # remove received wavelength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### as per the baseline program, we remove the abs time, detected and receoved_wavelength data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#histArray[:,:,0]=mjdInt\n",
    "histArray[:,:,1]=fluxTest\n",
    "histArray[:,:,2]=flux_err_test\n",
    "#histArray[:,:,3]=detected\n",
    "histArray[:,:,4]=deltaMjd\n",
    "histArray[:,:,5]=rval\n",
    "histArray[:,:,6]=source_wavelength\n",
    "#histArray[:,:,7]=received_wavelength"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(idArr.shape)\n",
    "print(metaArr.shape)\n",
    "print(bandArr.shape)\n",
    "print(histArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final vector dictionary\n",
    "X = {\n",
    "        'id': idArr,\n",
    "        'meta': metaArr,\n",
    "        'band': bandArr,\n",
    "        'hist': histArray\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X['id'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X['hist'][0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(np.array(train_df.select('target').collect(), dtype='int32'), num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_elapsed=time.time() - pgm_start\n",
    "pgm_elapsedCpu=time.clock() - pgm_startCpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.39227175712585\n",
      "124.59657700000001\n"
     ]
    }
   ],
   "source": [
    "print(pgm_elapsed)\n",
    "print(pgm_elapsedCpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split testing below. you can delete after we work it out\n",
    "\n",
    "https://stackoverflow.com/questions/37077432/how-to-estimate-dataframe-real-size-in-pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "def _to_java_object_rdd(rdd):  \n",
    "    \"\"\" Return a JavaRDD of Object by unpickling\n",
    "    It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "    RDD is serialized in batch or not.\n",
    "    \"\"\"\n",
    "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "JavaObj = _to_java_object_rdd(vectors_df.rdd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nbytes = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nbytes/10**6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors_df.unpersist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors_df.select('hist.interval').take(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable('full_train_vectors_temp')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### We'll create the split for the training, test, and validation sets here; \n",
    "weights = [.8, .1, .1]\n",
    "seed = 42 # seed=0L\n",
    "train_df, validation_df, test_df = vectors_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colDict={}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colDict['id'] = vectors_df.select('object_id').rdd.flatMap(lambda x: x)\n",
    "colDict['meta'] = vectors_df.select('meta').rdd.flatMap(lambda x: x)\n",
    "colDict['band'] = vectors_df.select('band').rdd.flatMap(lambda x: x)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colDict['hist'] = train_df.select('hist').rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colDict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colDict['meta']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "id=vectors_df.select('object_id').rdd.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parsedData=vectors_df.select('meta').rdd.map(lambda row: np.array([ row[0], row[1], row[2] ]) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parsedData.take(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colDict['id'].map(lambda x: arr.append(x[0]))\n",
    "#colDict['id'].map(lambda x: arr.append(x[i]) for in in range(256))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.select('object_id').rdd.flatMap(lambda x: arr.append(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idArray=np.array()\n",
    "testArr = train_df.select('object_id').rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arr = np.array()\n",
    "train_df.select('object_id').rdd.flatMap(lambda x: np.append(arr, x))\n",
    "#rdd.map(lambda x: np.append(arr, x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "histTest = train_df.select('hist.interval').rdd.flatMap(lambda x: x)\n",
    "fluxTest = train_df.select('hist.flux').rdd.flatMap(lambda x: x)\n",
    "flux_err_test = train_df.select('hist_flux_err').rdd.flatMap(lambda x: x)\n",
    "\n",
    "histArray=np.zeros((recCount,sequence_len,8), dtype='float32') \n",
    "# this will work brilliantly as get_keras_data sets three columns to zeros anyway\n",
    "\n",
    "histTest=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(recCount,sequence_len)\n",
    "fluxTest=np.array(vectors_df.select('hist.flux').collect(), dtype='float32').reshape(recCount,sequence_len)\n",
    "flux_err_test=np.array(vectors_df.select('hist.flux_err').collect(), dtype='float32').reshape(recCount,sequence_len)\n",
    "\n",
    "\n",
    "histTest=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(recCount,sequence_len)\n",
    "fluxTest=np.array(vectors_df.select('hist.flux').collect(), dtype='float32').reshape(recCount,sequence_len)\n",
    "flux_err_test=np.array(vectors_df.select('hist.flux_err').collect(), dtype='float32').reshape(recCount,sequence_len)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
