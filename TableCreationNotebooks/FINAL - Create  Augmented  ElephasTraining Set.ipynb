{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full test vector dataframe using Spark temp tables\n",
    "\n",
    "## First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # manual garbag collection to stop leaks on Collect() gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 35\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to this great article, SQL at Scale with Apache Spark SQL and DataFrames\n",
    "\n",
    "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f\n",
    "\n",
    "#### Benchmark snippets that may be usefull\n",
    "https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# set up hive session for broadcast joins\n",
    "optSql=\"\"\"\n",
    "set hive.cbo.enable=true;\n",
    "set hive.auto.convert.join=true;\n",
    "set hive.auto.convert.join.noconditionaltask=true;\n",
    "set hive.auto.convert.join.noconditionaltask.size=20971520\n",
    "set hive.auto.convert.join.use.nonstaged=true;\n",
    "set hive.mapjoin.smalltable.filesize = 30000000; \n",
    "\"\"\"\n",
    "sqlContext.sql(optSql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the test vector set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testVectorsDF=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select ts.object_id,\n",
    "        0 target,\n",
    "       array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "       double(hostgal_specz) as specz,\n",
    "       MAP(\n",
    "            'mjd', 0,\n",
    "            'passband',passband,\n",
    "            'flux',flux / mods.HistModifier,\n",
    "            'flux_err',flux_err / mods.HistModifier,\n",
    "            'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'detected',0,\n",
    "            'source_wavelength', case \n",
    "                                    when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    end,\n",
    "            'received_wavelength', 0\n",
    "    \n",
    "       ) AS kv\n",
    "    from test_set_compressed ts \n",
    "        inner join test_set_metadata tsm \n",
    "            on ts.object_id = tsm.object_id \n",
    "        inner join (\n",
    "            select\n",
    "            object_id,\n",
    "            log2(max(flux)- min(flux)) flux_pow,\n",
    "            pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "            log2(max(flux)- min(flux))/10 as metaVal\n",
    "            from test_set_compressed\n",
    "            group by object_id\n",
    "    \n",
    "        ) mods\n",
    "        on ts.object_id = mods.object_id\n",
    "    WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    ") \n",
    "select object_id, target,meta,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "collect_list(float(a.kv['mjd'])) as mjd,\n",
    "collect_list(float(a.kv['flux'])) as flux,\n",
    "collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "collect_list(int(a.kv['detected'])) as detected,\n",
    "collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "from rawData a\n",
    "group by object_id, target,meta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='overwrite'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "TABLE='elephas_test_set' #- original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_elephas'\n",
    "\n",
    "testVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the base table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select ts.object_id,\n",
    "        case \n",
    "                when target= 6 then 0\n",
    "                when target= 15 then 1\n",
    "                when target= 16 then 2\n",
    "                when target= 42 then 3\n",
    "                when target= 52 then 4\n",
    "                when target= 53 then 5\n",
    "                when target= 62 then 6\n",
    "                when target= 64 then 7\n",
    "                when target= 65 then 8\n",
    "                when target= 67 then 9\n",
    "                when target= 88 then 10\n",
    "                when target= 90 then 11\n",
    "                when target= 92 then 12\n",
    "                when target= 95 then 13\n",
    "                when target= 99 then 14\n",
    "                else 14\n",
    "                end target,\n",
    "       array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "       double(hostgal_specz) as specz,\n",
    "       MAP(\n",
    "            'mjd', 0,\n",
    "            'passband',passband,\n",
    "            'flux',flux / mods.HistModifier,\n",
    "            'flux_err',flux_err / mods.HistModifier,\n",
    "            'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'detected',0,\n",
    "            'source_wavelength', case \n",
    "                                    when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    end,\n",
    "            'received_wavelength', 0\n",
    "    \n",
    "       ) AS kv\n",
    "    from training_set ts \n",
    "        inner join training_set_metadata tsm \n",
    "            on ts.object_id = tsm.object_id \n",
    "        inner join (\n",
    "            select\n",
    "            object_id,\n",
    "            log2(max(flux)- min(flux)) flux_pow,\n",
    "            pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "            log2(max(flux)- min(flux))/10 as metaVal\n",
    "            from training_set\n",
    "            group by object_id\n",
    "    \n",
    "        ) mods\n",
    "        on ts.object_id = mods.object_id\n",
    "    WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    ") \n",
    "select object_id, target,meta,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "collect_list(float(a.kv['mjd'])) as mjd,\n",
    "collect_list(float(a.kv['flux'])) as flux,\n",
    "collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "collect_list(int(a.kv['detected'])) as detected,\n",
    "collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "from rawData a\n",
    "group by object_id, target,meta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='overwrite'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "TABLE='elephas_training_set' #- original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_elephas'\n",
    "\n",
    "trainingVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the highest object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "130779836\n",
      "finished!\n",
      "1\n",
      "130779836\n",
      "finished!\n",
      "2\n",
      "130779836\n",
      "finished!\n",
      "3\n",
      "130779836\n",
      "finished!\n",
      "4\n",
      "130779836\n",
      "finished!\n",
      "5\n",
      "130779836\n",
      "finished!\n",
      "6\n",
      "130779836\n",
      "finished!\n",
      "7\n",
      "130779836\n",
      "finished!\n",
      "8\n",
      "130779836\n",
      "finished!\n",
      "9\n",
      "130779836\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    #objAdjuster=sqlContext.sql(\"select max(object_id) adjustment from elephas_training_set\").collect()[0][0]\n",
    "    print(objAdjuster)\n",
    "    sql= \"\"\"\n",
    "    with NEW_Metadata as (\n",
    "    select object_id,\n",
    "    ra,decl,gal_l,gal_b,ddf,hostgal_specz,\n",
    "    rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5) hostgal_photoz,\n",
    "    hostgal_photoz_err, distmod,mwebv,target\n",
    "    from training_set_metadata\n",
    "    )\n",
    "    select object_id, ra,decl,gal_l,gal_b,ddf,hostgal_specz,hostgal_photoz,hostgal_photoz_err,distmod,mwebv,target\n",
    "    from NEW_Metadata    \n",
    "    \"\"\"\n",
    "    \n",
    "    newMetadataDF=sqlContext.sql(sql)\n",
    "    newMetadataDF.registerTempTable(\"AUGMENTED_METADATA\")\n",
    "    \n",
    "    sql=\"\"\"\n",
    "    with New_Training_set as(\n",
    "        select ts.object_id, mjd, passband,flux,\n",
    "        rand()*((flux+flux_err)-((flux-flux_err)/1.5))+((flux-flux_err)/1.5)  newFlux,\n",
    "        flux_err,detected,\n",
    "        (1+hostgal_photoz)/( 1+ (rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5))) dt\n",
    "        from training_set ts\n",
    "            inner join training_set_metadata tsm\n",
    "                on ts.object_id = tsm.object_id\n",
    "    )\n",
    "    select object_id,\n",
    "    mjd*dt as mjd,  passband, newFlux as flux, flux_err, detected\n",
    "    from New_Training_set    \n",
    "    \"\"\"\n",
    "    \n",
    "    newTrainingSetDF=sqlContext.sql(sql)\n",
    "    newTrainingSetDF.registerTempTable(\"AUGMENTED_TRAINING_SET\")\n",
    "    \n",
    "    sql=\"\"\"\n",
    "    with rawData as\n",
    "    (\n",
    "        select ts.object_id,\n",
    "            case \n",
    "                    when target= 6 then 0\n",
    "                    when target= 15 then 1\n",
    "                    when target= 16 then 2\n",
    "                    when target= 42 then 3\n",
    "                    when target= 52 then 4\n",
    "                    when target= 53 then 5\n",
    "                    when target= 62 then 6\n",
    "                    when target= 64 then 7\n",
    "                    when target= 65 then 8\n",
    "                    when target= 67 then 9\n",
    "                    when target= 88 then 10\n",
    "                    when target= 90 then 11\n",
    "                    when target= 92 then 12\n",
    "                    when target= 95 then 13\n",
    "                    when target= 99 then 14\n",
    "                    else 14\n",
    "                    end target,\n",
    "           array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "           double(hostgal_specz) as specz,\n",
    "           MAP(\n",
    "                'mjd', 0,\n",
    "                'passband',passband,\n",
    "                'flux',flux / mods.HistModifier,\n",
    "                'flux_err',flux_err / mods.HistModifier,\n",
    "                'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "                'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "                'detected',0,\n",
    "                'source_wavelength', case \n",
    "                                        when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        end,\n",
    "                'received_wavelength', 0\n",
    "\n",
    "           ) AS kv\n",
    "        from {} ts \n",
    "            inner join {} tsm \n",
    "                on ts.object_id = tsm.object_id \n",
    "            inner join (\n",
    "                select\n",
    "                object_id,\n",
    "                log2(max(flux)- min(flux)) flux_pow,\n",
    "                pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "                log2(max(flux)- min(flux))/10 as metaVal\n",
    "                from training_set\n",
    "                group by object_id\n",
    "\n",
    "            ) mods\n",
    "            on ts.object_id = mods.object_id\n",
    "        WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    "    ) \n",
    "    select object_id, target,meta,\n",
    "    collect_list(int(a.kv['passband']))as band,\n",
    "    collect_list(float(a.kv['mjd'])) as mjd,\n",
    "    collect_list(float(a.kv['flux'])) as flux,\n",
    "    collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "    collect_list(int(a.kv['detected'])) as detected,\n",
    "    collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "    collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "    collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "    collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "    from rawData a\n",
    "    group by object_id, target,meta    \n",
    "    \"\"\".format(\"AUGMENTED_TRAINING_SET\", \"AUGMENTED_METADATA\")\n",
    "    \n",
    "    trainingVectorsDF=sqlContext.sql(sql)\n",
    "    trainingVectorsDF.count()\n",
    "    MODE='append'\n",
    "    FORMAT='parquet'\n",
    "    TABLE='elephas_training_set'\n",
    "    trainingVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n",
    "    \n",
    "    print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196200"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF=sqlContext.sql(\"select * from elephas_training_set\")\n",
    "testDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below is just testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "objAdjuster=sqlContext.sql(\"select max(object_id) adjustment from elephas_training_set\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we create the new metadata table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "with NEW_Metadata as (\n",
    "select object_id,\n",
    "ra,decl,gal_l,gal_b,ddf,hostgal_specz,\n",
    "rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5) hostgal_photoz,\n",
    "hostgal_photoz_err, distmod,mwebv,target\n",
    "from training_set_metadata\n",
    ")\n",
    "select object_id, ra,decl,gal_l,gal_b,ddf,hostgal_specz,hostgal_photoz,hostgal_photoz_err,distmod,mwebv,target\n",
    "from NEW_Metadata\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "newMetadataDF=sqlContext.sql(sql)\n",
    "newMetadataDF.registerTempTable(\"AUGMENTED_METADATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7848"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newMetadataDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now create the augmented training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "with New_Training_set as(\n",
    "    select ts.object_id, mjd, passband,flux,\n",
    "    rand()*((flux+flux_err)-((flux-flux_err)/1.5))+((flux-flux_err)/1.5)  newFlux,\n",
    "    flux_err,detected,\n",
    "    (1+hostgal_photoz)/( 1+ (rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5))) dt\n",
    "    from training_set ts\n",
    "        inner join training_set_metadata tsm\n",
    "            on ts.object_id = tsm.object_id\n",
    ")\n",
    "select object_id,\n",
    "mjd*dt as mjd,  passband, newFlux as flux, flux_err, detected\n",
    "from New_Training_set\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith New_Training_set as(\\n    select ts.object_id, mjd, passband,flux,\\n    rand()*((flux+flux_err)-((flux-flux_err)/1.5))+((flux-flux_err)/1.5)  newFlux,\\n    flux_err,detected,\\n    (1+hostgal_photoz)/( 1+ (rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5))) dt\\n    from training_set ts\\n        inner join training_set_metadata tsm\\n            on ts.object_id = tsm.object_id\\n)\\nselect object_id+130779836+1 object_id,\\nmjd*dt as mjd,  passband, newFlux as flux, flux_err, detected\\nfrom New_Training_set\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainingSetDF=sqlContext.sql(sql)\n",
    "newTrainingSetDF.registerTempTable(\"AUGMENTED_TRAINING_SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1421705"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTrainingSetDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select ts.object_id,\n",
    "        case \n",
    "                when target= 6 then 0\n",
    "                when target= 15 then 1\n",
    "                when target= 16 then 2\n",
    "                when target= 42 then 3\n",
    "                when target= 52 then 4\n",
    "                when target= 53 then 5\n",
    "                when target= 62 then 6\n",
    "                when target= 64 then 7\n",
    "                when target= 65 then 8\n",
    "                when target= 67 then 9\n",
    "                when target= 88 then 10\n",
    "                when target= 90 then 11\n",
    "                when target= 92 then 12\n",
    "                when target= 95 then 13\n",
    "                when target= 99 then 14\n",
    "                else 14\n",
    "                end target,\n",
    "       array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "       double(hostgal_specz) as specz,\n",
    "       MAP(\n",
    "            'mjd', 0,\n",
    "            'passband',passband,\n",
    "            'flux',flux / mods.HistModifier,\n",
    "            'flux_err',flux_err / mods.HistModifier,\n",
    "            'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'detected',0,\n",
    "            'source_wavelength', case \n",
    "                                    when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    end,\n",
    "            'received_wavelength', 0\n",
    "    \n",
    "       ) AS kv\n",
    "    from {} ts \n",
    "        inner join {} tsm \n",
    "            on ts.object_id = tsm.object_id \n",
    "        inner join (\n",
    "            select\n",
    "            object_id,\n",
    "            log2(max(flux)- min(flux)) flux_pow,\n",
    "            pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "            log2(max(flux)- min(flux))/10 as metaVal\n",
    "            from training_set\n",
    "            group by object_id\n",
    "    \n",
    "        ) mods\n",
    "        on ts.object_id = mods.object_id\n",
    "    WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    ") \n",
    "select object_id, target,meta,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "collect_list(float(a.kv['mjd'])) as mjd,\n",
    "collect_list(float(a.kv['flux'])) as flux,\n",
    "collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "collect_list(int(a.kv['detected'])) as detected,\n",
    "collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "from rawData a\n",
    "group by object_id, target,meta\n",
    "\"\"\".format(\"AUGMENTED_TRAINING_SET\", \"AUGMENTED_METADATA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF=sqlContext.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7848"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingVectorsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = false)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- mjd: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- flux: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- flux_err: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- detected: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- fwd_int: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- bwd_int: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- source_wavelength: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- received_wavelength: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingVectorsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "TABLE='elephas_training_set' #- original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_elephas'\n",
    "\n",
    "trainingVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
