{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50)\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','FifthModelTest - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 25\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df):\n",
    "    #x=np.array(raw_vectorsDF.select('target').collect())\n",
    "    \n",
    "    all_y = np.array(df.select('target').collect(), dtype = 'int32')\n",
    "\n",
    "    y_count = np.unique(all_y, return_counts=True)[1]\n",
    "\n",
    "    wtable = np.ones(len(classes))\n",
    "\n",
    "    for i in range(0, y_count.shape[0]):\n",
    "        wtable[i] = y_count[i] / all_y.shape[0]\n",
    "\n",
    "    return wtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(sc,vectors_df):\n",
    "    idArr=np.array(vectors_df.select('object_id').collect(), dtype='float32')\n",
    "    r, c = idArr.shape\n",
    "    r,c=idArr.shape\n",
    "    idArr.reshape(r,)\n",
    "    meta_len=10\n",
    "    \n",
    "    metaArr=np.array(vectors_df.select('meta').collect(), dtype='float32').reshape(r,meta_len)\n",
    "    bandArr= np.array(vectors_df.select('band').collect() , dtype='int32').reshape(r,sequence_len)\n",
    "\n",
    "    histArray=np.zeros((r,sequence_len,8), dtype='float32') \n",
    "    # this will work brilliantly as get_keras_data sets three columns to zeros anyway\n",
    "\n",
    "    #mjdInt=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    deltaMjd=np.array(vectors_df.select('hist.deltaMjd').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    rval=np.array(vectors_df.select('hist.rval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    fluxTest=np.array(vectors_df.select('hist.flux').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    flux_err_test=np.array(vectors_df.select('hist.flux_err').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    #detected=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    source_wavelength=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    #received_wavelength=np.array(vectors_df.select('hist.interval').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    \n",
    "    #as per the baseline program, we remove the abs time, detected and receoved_wavelength data\n",
    "\n",
    "    #histArray[:,:,0]=mjdInt\n",
    "    histArray[:,:,1]=fluxTest\n",
    "    histArray[:,:,2]=flux_err_test\n",
    "    #histArray[:,:,3]=detected\n",
    "    histArray[:,:,4]=deltaMjd\n",
    "    histArray[:,:,5]=rval\n",
    "    histArray[:,:,6]=source_wavelength\n",
    "    #histArray[:,:,7]=received_wavelength\n",
    "    \n",
    "    # Create the final vector dictionary\n",
    "    X = {\n",
    "            'id': idArr,\n",
    "            'meta': metaArr,\n",
    "            'band': bandArr,\n",
    "            'hist': histArray\n",
    "        }\n",
    "    # and the encoded target vector\n",
    "    Y = to_categorical(np.array(vectors_df.select('target').collect(), dtype='int32'), num_classes=len(classes))  \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idArr=np.array(raw_vectorsDF.select('object_id').collect(), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_keras_data(sc,raw_vectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(i, samples_train, samples_valid):\n",
    "    start_augment=time.time()\n",
    "    start_augmentCpu=time.clock()\n",
    "    \n",
    "    #samples_train += augmentate(samples_train, augment_count, augment_count)\n",
    "    \n",
    "    elapsed_augment=time.time() - start_augment\n",
    "    elapsed_augmentCpu=time.clock() - start_augmentCpu\n",
    "\n",
    "    patience = 1000000 // len(samples_train) + 5\n",
    "\n",
    "    start_trainingVectors=time.time()\n",
    "    start_trainingVectorsCpu=time.clock()\n",
    "\n",
    "    train_x, train_y = get_keras_data(samples_train)\n",
    "\n",
    "    elapsed_training_Vectors=time.time() - start_trainingVectors\n",
    "    elapsed_training_VectorsCpu=time.clock() - start_trainingVectorsCpu\n",
    "\n",
    "    print(len(samples_train))\n",
    "    \n",
    "    del samples_train\n",
    "    \n",
    "    start_validationVectors=time.time()\n",
    "    start_validationVectorsCpu=time.clock()\n",
    "\n",
    "    valid_x, valid_y = get_keras_data(samples_valid)\n",
    "    del samples_valid\n",
    "    \n",
    "    elapsed_validation_Vectors=time.time() - start_validationVectors\n",
    "    elapsed_validation_VectorsCpu=time.clock() - start_validationVectorsCpu\n",
    "    \n",
    "    return  elapsed_augment,elapsed_augmentCpu,\\\n",
    "            elapsed_training_Vectors,elapsed_training_VectorsCpu,\\\n",
    "            elapsed_validation_Vectors,elapsed_validation_VectorsCpu, \\\n",
    "            train_x, train_y\n",
    "\n",
    "    \n",
    "    ## THIS IS AS FAR AS WE NEED TO GO FOR THIS TEST\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    model = get_model(train_x, train_y)\n",
    "\n",
    "    if i == 1: model.summary()\n",
    "    model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    print('Training model {0} of {1}, Patience: {2}'.format(i, num_models, patience))\n",
    "    filename = 'model_{0:03d}.hdf5'.format(i)\n",
    "    callbacks = [EarlyStopping(patience=patience, verbose=1), ModelCheckpoint(filename, save_best_only=True)]\n",
    "\n",
    "    model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=max_epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "\n",
    "    model = load_model(filename, custom_objects={'mywloss': mywloss})\n",
    "\n",
    "    preds = model.predict(valid_x, batch_size=batch_size2)\n",
    "    loss = multi_weighted_logloss(valid_y, preds, wtable)\n",
    "    acc = accuracy_score(np.argmax(valid_y, axis=1), np.argmax(preds,axis=1))\n",
    "    print('MW Loss: {0:.4f}, Accuracy: {1:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Loading train data...')\n",
    "\n",
    "train_meta = pd.read_csv('../input/training_set_metadata.csv')\n",
    "train_data = pd.read_csv('../input/training_set.csv')\n",
    "\n",
    "\n",
    "wtable = get_wtable(train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from hive...\n"
     ]
    }
   ],
   "source": [
    "print('Loading train data from hive...')\n",
    "\n",
    "start_train=time.time()\n",
    "start_trainCpu=time.clock()\n",
    "\n",
    "raw_vectorsSQL=\"\"\"\n",
    "select * from training_set_augmented_vectors\n",
    "\"\"\"\n",
    "\n",
    "raw_vectorsDF=sqlContext.sql(raw_vectorsSQL)\n",
    "\n",
    "elapsed_train=time.time()-start_train\n",
    "elapsed_trainCpu=time.clock()-start_trainCpu\n",
    "#wtable = get_wtable(raw_vectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- meta: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hist: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- interval: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- deltaMjd: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- rval: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- source_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- received_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_vectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_train)\n",
    "print(elapsed_trainCpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_samples=time.time()\n",
    "start_samplesC=time.clock()\n",
    "    \n",
    "# No get data \n",
    "elapsed_samples=time.time() - start_samples\n",
    "elapsed_samplesC=time.clock() - start_samplesC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_models+1):\n",
    "\n",
    "    samples_train, samples_valid = train_test_split(samples, test_size=valid_size, random_state=42*i)\n",
    "    len(samples_train)\n",
    "    \n",
    "    start_train=time.time()\n",
    "    elapsed_augment,elapsed_augmentCpu,\\\n",
    "            elapsed_training_Vectors,elapsed_training_VectorsCpu,\\\n",
    "            elapsed_validation_Vectors,elapsed_validation_VectorsCpu, \\\n",
    "            train_x, train_y = \\\n",
    "            train_model(i, samples_train, samples_valid)\n",
    "    elapsed_train=time.time()-start_train\n",
    "    print(elapsed_train)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, samples_valid = train_test_split(samples, test_size=valid_size, random_state=42*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_keras_data(samples_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_x\n",
    "Y=train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=X['hist'][0].shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['meta'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_input = Input(shape=X['hist'][0].shape, name='hist')\n",
    "meta_input = Input(shape=X['meta'][0].shape, name='meta')\n",
    "band_input = Input(shape=X['band'][0].shape, name='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_emb = Embedding(8, 8)(band_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = concatenate([hist_input, band_emb])\n",
    "hist = TimeDistributed(Dense(40, activation='relu'))(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectorsDF.select(\"meta\").toPandas().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff below is final vector analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectorsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting dataframes to dictionaries\n",
    "https://stackoverflow.com/questions/41206255/convert-pyspark-sql-dataframe-dataframe-type-dataframe-to-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o159.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:280)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:276)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:276)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:298)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:297)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3197)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-27480c84963a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_vectorsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o159.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:280)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:276)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:276)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:298)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:297)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3197)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3197)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "list_objects = map(lambda row: row.asDict(), raw_vectorsDF.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_vectors = {object['object_id']: object for object in list_objects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_specz=False\n",
    "\n",
    "for key in object_vectors.keys():\n",
    "    print(key)\n",
    "    i=object_vectors.get(key)\n",
    "    \n",
    "    id=i.get('object_id')\n",
    "\n",
    "    sample = {}\n",
    "    sample['id'] = int(id)\n",
    "    \n",
    "    # 'object_id', 'target', 'meta', 'specz', 'band', 'hist'\n",
    "    \n",
    "    sample['target'] = int(i.get('target'))\n",
    "    \n",
    "    meta=np.array(i.get('meta'), dtype='float32')\n",
    "  \n",
    "    sample['meta'] = np.zeros(10, dtype = 'float32')\n",
    "\n",
    "    sample['meta'][4] = meta[0]\n",
    "    sample['meta'][5] = meta[2]\n",
    "    sample['meta'][6] = meta[3]\n",
    "    sample['meta'][7] = meta[4]\n",
    "    sample['meta'][8] = float(meta[2]) > 0\n",
    "\n",
    "    sample['specz'] = float(meta[1])    \n",
    "    \n",
    "    if use_specz:\n",
    "        sample['meta'][5] = float(meta['hostgal_specz'])\n",
    "        sample['meta'][6] = 0.0\n",
    "\n",
    "    z = float(sample['meta'][5])\n",
    "\n",
    "    j=i.get('hist')\n",
    "    mjd=np.array(j[0][0], dtype='float32')\n",
    "    r,c=mjd.shape\n",
    "    mjd.reshape(c,)\n",
    "    band=np.array(  j[0][1], dtype='int32').reshape(c,) # passband\n",
    "    flux=np.array( j[0][2], dtype='float32').reshape(c,) # flux\n",
    "    flux_err=np.array( j[0][3], dtype='float32').reshape(c,) # flux_err\n",
    "    detected=np.array( j[0][4], dtype='int32').reshape(c,) # Detected\n",
    "\n",
    "    mjd -= mjd[0]\n",
    "    mjd /= 100 # Earth time shift in day*100\n",
    "    mjd /= (z + 1) # Object time shift in day*100\n",
    "\n",
    "\n",
    "    received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "    received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "    source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "\n",
    "\n",
    "    sample['band'] = band + 1\n",
    "\n",
    "    sample['hist'] = np.zeros((flux.shape[0], 8), dtype='float32')\n",
    "    sample['hist'][:,0] = mjd\n",
    "    sample['hist'][:,1] = flux\n",
    "    sample['hist'][:,2] = flux_err\n",
    "    sample['hist'][:,3] = detected\n",
    "\n",
    "    sample['hist'][:,6] = (source_wavelength/1000)\n",
    "    sample['hist'][:,7] = (received_wavelength/1000)\n",
    "\n",
    "    set_intervals(sample)\n",
    "    \n",
    "    flux_max = np.max(flux)\n",
    "    flux_min = np.min(flux)\n",
    "    flux_pow = math.log2(flux_max - flux_min)\n",
    "    sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "    sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "    sample['meta'][9] = flux_pow / 10\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "    if len(samples) % 1000 == 0:\n",
    "        print('Converting data {0}'.format(len(samples)), end='\\r')\n",
    "\n",
    "    if len(samples) >= limit:\n",
    "        break\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.where(classes == int(target))[0][0] # positional index of the classes array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=object_vectors.get(615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[[0,1,3], :][:, [0,2]]  # Selects the columns you want as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta=meta=np.array(i.get('meta'), dtype='float32')\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=dict_persons.get(6266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_persons.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in i.keys():\n",
    "  print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.get('object_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(i.get('band')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=i.get('hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, sublist in enumerate(j):\n",
    "    for column, item in enumerate(sublist):\n",
    "        if item:\n",
    "            print((row, column))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0][0] -- mjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0][1] # passband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0][2] # flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0][3] # flux_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0][4] # Detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, sublist in enumerate(k):\n",
    "    for column, item in enumerate(sublist):\n",
    "        if item:\n",
    "            print((row, column))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train_x['id'].shape)\n",
    "print(train_x['meta'].shape)\n",
    "print(train_x['band'].shape)\n",
    "print(train_x['hist'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so for hist - each row, we have 8 arrays palanced out to 256 entries - pad stuff! and the arrays are 0 mjd 1 flux 2 flux_err 3 etc 4 etc 5 etc 6 7\n",
    "\n",
    "so \n",
    "\n",
    "- train_x['hist'][0,:,0] is mjd\n",
    "- train_x['hist'][0,:,1] is flux\n",
    "- train_x['hist'][0,:,2] is flux_err\n",
    "- train_x['hist'][0,:,3] is detected\n",
    "- train_x['hist'][0,:,4] is mjd_deltas\n",
    "- train_x['hist'][0,:,5] is mjd_reverse_deltas\n",
    "- train_x['hist'][0,:,6] is source_wavelength\n",
    "- train_x['hist'][0,:,7] is received_wavelength\n",
    "\n",
    "Just remember that in get_keras_data, mjd, detected and received_wavelength values are removed and set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x['hist'][0,:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjd=[1,2,4,8,16,32,64]\n",
    "\n",
    "mjd1=np.ediff1d(mjd, to_begin = [0]) - mjd deltas between elements\n",
    "mjd2=np.ediff1d(mjd, to_end = [0]) - reverse mjd from end\n",
    "\n",
    "#hist[:,4] = np.ediff1d(hist[:,0], to_begin = [0])\n",
    "#hist[:,5] = np.ediff1d(hist[:,0], to_end = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSetSQL=\"\"\"\n",
    "select * from training_set_all_padded\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF =sqlContext.sql(fullSetSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {\n",
    "        'id': np.array(fullDF.select('object_id').collect(), dtype='int32'),\n",
    "        'meta': np.array(fullDF.select('meta').collect(), dtype='float32'),\n",
    "        'band': np.array(fullDF.select('band').collect() , dtype='int32').reshape(7848,256)\n",
    "#        'hist': pad_sequences([i['hist'] for i in itemslist], maxlen=sequence_len, dtype='float32'),\n",
    "    }\n",
    "#print('creating Y')\n",
    "#Y = to_categorical([i['target'] for i in itemslist], num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histTest=np.array(fullDF.select('hist.interval').collect(), dtype='float32').reshape(7848,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxTest=np.array(fullDF.select('hist.flux').collect(), dtype='float32').reshape(7848,256)\n",
    "flux_err_test=np.array(fullDF.select('hist.flux_err').collect(), dtype='float32').reshape(7848,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_err_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histArray=np.zeros((7848,256,8), dtype='float32') \n",
    "# this will work brilliantly as get_keras_data sets three columns to zeros anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histArray[:,:,0]=histTest\n",
    "histArray[:,:,1]=fluxTest\n",
    "histArray[:,:,1]=flux_err_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetArray=fluxTest=np.array(fullDF.select('target').collect(), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(classes == 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(3, num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDF.select('meta').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental - testing mods for get data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extragalactic=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "# You have to perform an aggregation on the Spark dataframe and collect the results before you can iterate \n",
    "# This i not necessary with pandas\n",
    "groups = train_mdf_data.groupby('object_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in groups:\n",
    "    id=g[0]\n",
    "    \n",
    "    sample = {}\n",
    "    sample['id'] = int(id)\n",
    "    \n",
    "    meta = train_meta.loc[train_meta['object_id'] == id]\n",
    "    \n",
    "    # NEW We need to get the arrays for each pivoted dataframe\n",
    "    #mjd = train_mjd_data.loc[train_mjd_data['object_id'] == id]\n",
    "    #passband=train_passband_data.loc[train_passband_data['object_id'] == id]\n",
    "    #flux=train_flux_data.loc[train_flux_data['object_id'] == id]\n",
    "    #flux_err=train_flux_err_data.loc[train_flux_err_data['object_id'] == id]\n",
    "    #detected=train_detected_data.loc[train_detected_data['object_id'] == id]\n",
    "    \n",
    "    if extragalactic == True and float(meta['hostgal_photoz']) == 0:\n",
    "        continue\n",
    "\n",
    "    if extragalactic == False and float(meta['hostgal_photoz']) > 0:\n",
    "        continue    \n",
    "    \n",
    "    if 'target' in meta:\n",
    "        sample['target'] = np.where(classes == int(meta['target']))[0][0]\n",
    "    else:\n",
    "        sample['target'] = len(classes) - 1   \n",
    "\n",
    "    sample['meta'] = np.zeros(10, dtype = 'float32')\n",
    "\n",
    "    sample['meta'][4] = meta['ddf']\n",
    "    sample['meta'][5] = meta['hostgal_photoz']\n",
    "    sample['meta'][6] = meta['hostgal_photoz_err']\n",
    "    sample['meta'][7] = meta['mwebv']\n",
    "    sample['meta'][8] = float(meta['hostgal_photoz']) > 0\n",
    "\n",
    "    sample['specz'] = float(meta['hostgal_specz'])\n",
    "\n",
    "    \n",
    "    if use_specz:\n",
    "        sample['meta'][5] = float(meta['hostgal_specz'])\n",
    "        sample['meta'][6] = 0.0\n",
    "\n",
    "    z = float(sample['meta'][5])\n",
    "    \n",
    "    # we need to drop the object_id from the pivot records. We can use any of the pivot dataframes,\n",
    "    # because they all have the same shape, coming from a Hive table definition. We'll use the MJD\n",
    "    # pivot dataframe to set up the indexes we want. How we do this - in stages\n",
    "    # 1. Create a data frame for the pivot dataframes, on for each object_id\n",
    "    # 2. Use dropna to remove NAN column values\n",
    "    # 3. Cast that dataframe to a numpy array and get the shape\n",
    "    # 4. Use the mjd array as a base, create an index list of the columns we want - ie we're dropping the object_id\n",
    "    # 5. Use the index ro truncate the object_id column from the rest of the arrays\n",
    "    # 6. finally, we need to reshape the arrays from [1;cols] to [cols,]   \n",
    "    \n",
    "    mjd = np.array(train_mjd_data.loc[train_mjd_data['object_id'] == id].dropna(axis='columns'), dtype='float32')\n",
    "    r,c=mjd.shape\n",
    "\n",
    "    idx_OUT_columns = [0]\n",
    "    idx_IN_columns = [i for i in range(np.shape(mjd)[1]) if i not in idx_OUT_columns]\n",
    "\n",
    "    mjd = mjdArray[:,idx_IN_columns].reshape(c-1,)\n",
    "    band = np.array(train_passband_data.loc[train_passband_data['object_id'] == id].dropna(axis='columns') , dtype='int32')[:,idx_IN_columns].reshape(c-1,)\n",
    "    flux = np.array(train_flux_data.loc[train_flux_data['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "    flux_err = np.array(train_flux_err_data.loc[train_flux_err_data['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "    detected = np.array(train_detected_data.loc[train_detected_data['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "    #mjd      = np.array(g[1]['mjd'],      dtype='float32')\n",
    "    #band     = np.array(g[1]['passband'], dtype='int32')\n",
    "    #flux     = np.array(g[1]['flux'],     dtype='float32')\n",
    "    #flux_err = np.array(g[1]['flux_err'], dtype='float32')\n",
    "    #detected = np.array(g[1]['detected'], dtype='float32')  \n",
    "\n",
    "    \n",
    "    mjd -= mjd[0]\n",
    "    mjd /= 100 # Earth time shift in day*100\n",
    "    mjd /= (z + 1) # Object time shift in day*100\n",
    "\n",
    "    \n",
    "    received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "    received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "    source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "\n",
    "    \n",
    "    sample['band'] = band + 1\n",
    "\n",
    "    sample['hist'] = np.zeros((flux.shape[0], 8), dtype='float32')\n",
    "    sample['hist'][:,0] = mjd\n",
    "    sample['hist'][:,1] = flux\n",
    "    sample['hist'][:,2] = flux_err\n",
    "    sample['hist'][:,3] = detected\n",
    "\n",
    "    sample['hist'][:,6] = (source_wavelength/1000)\n",
    "    sample['hist'][:,7] = (received_wavelength/1000)\n",
    "    \n",
    "    set_intervals(sample)\n",
    "\n",
    "\n",
    "    flux_max = np.max(flux)\n",
    "    flux_min = np.min(flux)\n",
    "    flux_pow = math.log2(flux_max - flux_min)\n",
    "    sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "    sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "    sample['meta'][9] = flux_pow / 10\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "    if len(samples) % 1000 == 0:\n",
    "        print('Converting data {0}'.format(len(samples)), end='\\r')\n",
    "\n",
    "    if len(samples) >= limit:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=713\n",
    "print(id)\n",
    "sample = {}\n",
    "sample['id'] = int(id)\n",
    "\n",
    "meta = train_meta.loc[train_meta['object_id'] == id]\n",
    "\n",
    "# NEW We need to get the arrays for each pivoted dataframe\n",
    "#mjd = train_mjd_data.loc[train_mjd_data['object_id'] == id]\n",
    "#passband=train_passband_data.loc[train_passband_data['object_id'] == id]\n",
    "#flux=train_flux_data.loc[train_flux_data['object_id'] == id]\n",
    "#flux_err=train_flux_err_data.loc[train_flux_err_data['object_id'] == id]\n",
    "#detected=train_detected_data.loc[train_detected_data['object_id'] == id]\n",
    "\n",
    "if extragalactic == True and float(meta['hostgal_photoz']) == 0:\n",
    "    print('Hi there')\n",
    "\n",
    "if extragalactic == False and float(meta['hostgal_photoz']) > 0:\n",
    "    print('Hi there again')  \n",
    "\n",
    "if 'target' in meta:\n",
    "    sample['target'] = np.where(classes == int(meta['target']))[0][0]\n",
    "else:\n",
    "    sample['target'] = len(classes) - 1   \n",
    "\n",
    "sample['meta'] = np.zeros(10, dtype = 'float32')\n",
    "\n",
    "sample['meta'][4] = meta['ddf']\n",
    "sample['meta'][5] = meta['hostgal_photoz']\n",
    "sample['meta'][6] = meta['hostgal_photoz_err']\n",
    "sample['meta'][7] = meta['mwebv']\n",
    "sample['meta'][8] = float(meta['hostgal_photoz']) > 0\n",
    "\n",
    "sample['specz'] = float(meta['hostgal_specz'])\n",
    "\n",
    "\n",
    "if use_specz:\n",
    "    sample['meta'][5] = float(meta['hostgal_specz'])\n",
    "    sample['meta'][6] = 0.0\n",
    "\n",
    "z = float(sample['meta'][5])\n",
    "\n",
    "# we need to drop the object_id from the pivot records. We can use any of the pivot dataframes,\n",
    "# because they all have the same shape, coming from a Hive table definition. We'll use the MJD\n",
    "# pivot dataframe to set up the indexes we want. How we do this - in stages\n",
    "# 1. Create a data frame for the pivot dataframes, on for each object_id\n",
    "# 2. Use dropna to remove NAN column values\n",
    "# 3. Cast that dataframe to a numpy array and get the shape\n",
    "# 4. Use the mjd array as a base, create an index list of the columns we want - ie we're dropping the object_id\n",
    "# 5. Use the index ro truncate the object_id column from the rest of the arrays\n",
    "# 6. finally, we need to reshape the arrays from [1;cols] to [cols,],\n",
    "mjd = np.array(train_mjd_data.loc[train_mjd_data['object_id'] == id].dropna(axis='columns'), dtype='float32')\n",
    "r,c=mjd.shape\n",
    "\n",
    "idx_OUT_columns = [0]\n",
    "idx_IN_columns = [i for i in range(np.shape(mjd)[1]) if i not in idx_OUT_columns]\n",
    "\n",
    "mjd = mjdArray[:,idx_IN_columns].reshape(c-1,)\n",
    "band = np.array(train_passband_data.loc[train_passband_data['object_id'] == id].dropna(axis='columns') , dtype='int32')[:,idx_IN_columns].reshape(c-1,)\n",
    "flux = np.array(train_flux_data.loc[train_flux_data['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "flux_err = np.array(train_flux_err_data.loc[train_flux_err_data['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "detected = np.array(train_detected_data.loc[train_detected_data['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "\n",
    "#mjd      = np.array(g[1]['mjd'],      dtype='float32')\n",
    "#band     = np.array(g[1]['passband'], dtype='int32')\n",
    "#flux     = np.array(g[1]['flux'],     dtype='float32')\n",
    "#flux_err = np.array(g[1]['flux_err'], dtype='float32')\n",
    "#detected = np.array(g[1]['detected'], dtype='float32')  \n",
    "\n",
    "# Now we need to reshape to columns\n",
    "#mjd=mjd.reshape(352,)\n",
    "\n",
    "mjd -= mjd[0]\n",
    "mjd /= 100 # Earth time shift in day*100\n",
    "mjd /= (z + 1) # Object time shift in day*100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiThere = train_mjd_data.loc[train_mjd_data['object_id'] == id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mjd_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiThere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiThere.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjd = np.array(train_mjd_data.loc[train_mjd_data['object_id'] == id].dropna(axis='columns'), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c=mjd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
