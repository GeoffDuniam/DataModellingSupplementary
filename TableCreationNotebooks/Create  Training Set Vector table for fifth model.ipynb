{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full test vector dataframe using Spark temp tables\n",
    "\n",
    "## First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registered tables\n",
    "\n",
    "this notebooks utilised registered Spark tables - these are Spark dataframes that are registered in memory; from there, they can then be utililsed in forther dataframe creation statements.\n",
    "\n",
    "There is an excellent discussion here -\n",
    "\n",
    "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we create the modifiers for the flux values and the flux value to be added to the metadata table\n",
    "\n",
    "This inplements the code from lines 282 - 287 in the original program, but as a set (not iteratively).\n",
    "\n",
    "        flux_max = np.max(flux)\n",
    "        flux_min = np.min(flux)\n",
    "        flux_pow = math.log2(flux_max - flux_min)\n",
    "        sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "        sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "        sample['meta'][9] = flux_pow / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiersSQL=\"\"\"\n",
    "select\n",
    "object_id,\n",
    "log2(max(flux)- min(flux)) flux_pow,\n",
    "pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "log2(max(flux)- min(flux))/10 as metaVal\n",
    "from training_set\n",
    "group by object_id\n",
    "\"\"\"\n",
    "modifiersDF=sqlContext.sql(modifiersSQL)\n",
    "\n",
    "# Create the in memory table\n",
    "modifiersDF.registerTempTable(\"MODIFIERS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the padded training set\n",
    "We do this to create a standard set of features for all objects; in this case, the value provided by sequence_len (which in this case is 256).\n",
    "\n",
    "We're also going to use Spark registered tables to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = \"training_set\"\n",
    "training_metadata = \"training_set_metadata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the SQL equivalent of the keras PAD_SEQUENCES function. \n",
    "\n",
    "We create a baseline table consisting of object_ids and a 0 value for every feature as well as a rownumber. This is accomplished by a cartesian join (cross join in Hive) between the \"cnt\" nested table and the \"objects\" nested table, resulting the baseline nested table which has 256 records for each object, with all features set to zero.\n",
    "\n",
    "Next, we create a \"train_set\" nested table containing all the training set information, ordered by the mjd value descending. this pads the data from the last value to the first as per PAD_SEQUENCES,\n",
    "\n",
    "We then create the padded training set as a left join from the baseline nested table to the train_set nested table, padded to a consistent 236 values for each feature.\n",
    "\n",
    "Finally, we include the pow modifier onto the flux and flux_err values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedSQL=\"\"\"\n",
    "with\n",
    "cnt\n",
    "as\n",
    "(\n",
    "    select rownum from \n",
    "    (\n",
    "        select row_number() over (ORDER BY object_id) as rownum\n",
    "        from {}\n",
    "    ) a\n",
    "    where rownum <=256\n",
    "),\n",
    "objects as (select object_id, 0 padMJD, 0 padPassband,0 padFlux, 0 padFlux_err,0 padDetected from {} group by object_id),\n",
    "baseline as (select * from objects CROSS JOIN cnt ), -- cartesian product with 256 values to use as the baseline\n",
    "train_set as (select *, row_number() over (partition by object_id order by mjd desc) as rownum from {}),\n",
    "paddedRev as (\n",
    "    select baseline.object_id, --train_set.mjd, baseline.padMJD,\n",
    "    case when train_set.mjd is null then baseline.padMJD else train_set.mjd end mjd,\n",
    "    case when train_set.passband is null then baseline.padPassband else train_set.passband end passband,\n",
    "    case when train_set.flux is null then baseline.padFlux else train_set.flux end flux,\n",
    "    case when train_set.flux_err is null then baseline.padFlux_err else train_set.flux_err end flux_err,\n",
    "    case when train_set.detected is null then baseline.padDetected else train_set.detected end detected\n",
    "    from baseline left outer join train_set on baseline.object_id = train_set.object_id and baseline.rownum=train_set.rownum\n",
    "    order by baseline.object_id, mjd desc\n",
    ")\n",
    "select \n",
    "    pr.object_id, \n",
    "    mjd, \n",
    "    passband, \n",
    "    flux / HistModifier as flux, \n",
    "    flux_err/ HistModifier as flux_err, \n",
    "    detected \n",
    "from paddedRev pr\n",
    "    inner join {} mods\n",
    "        on pr.object_id = mods.object_id\n",
    "order by pr.object_id, mjd\n",
    "\"\"\".format(training_metadata,training_set,training_set, \"MODIFIERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the padded training data dataframe and create a Spark temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedTrainingSet_DF = sqlContext.sql(paddedSQL)\n",
    "paddedTrainingSet_DF.registerTempTable(\"PADDED_TRAINING_SET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the augmented data\n",
    "\n",
    "We take the padded data set, and create the augmented values as per the Augment function in the original program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentSQL=\"\"\"\n",
    "with New_Training_set as(\n",
    "    select ts.object_id, ts.mjd, ts.passband,ts.flux,\n",
    "   rand()*((ts.flux+ts.flux_err)-((ts.flux-ts.flux_err)/1.5))+((ts.flux-ts.flux_err)/1.5)  newFlux,\n",
    "    ts.flux_err,ts.detected,\n",
    "    (1+tsm.hostgal_photoz)/( 1+ (rand()*((tsm.hostgal_photoz+tsm.hostgal_photoz_err)-((tsm.hostgal_photoz-tsm.hostgal_photoz_err)/1.5))+((tsm.hostgal_photoz-tsm.hostgal_photoz_err)/1.5))) dt\n",
    "    from {} ts\n",
    "        inner join training_set_metadata tsm\n",
    "            on ts.object_id = tsm.object_id\n",
    ")\n",
    "select new_training_set.object_id,\n",
    "new_training_set.mjd*new_training_set.dt as mjd,  new_training_set.passband, new_training_set.newflux as flux, \n",
    "new_training_set.flux_err, new_training_set.detected\n",
    "from New_Training_set\n",
    "\"\"\".format(\"PADDED_TRAINING_SET\")\n",
    "\n",
    "augmented_DF = sqlContext.sql(augmentSQL)\n",
    "augmented_DF.registerTempTable(\"AUGMENTED_TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the training set metadata\n",
    " And add in the modifier field metaVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataSQL=\"\"\"\n",
    "select ts.*,\n",
    "    mod.metaVal\n",
    "from {} ts\n",
    "    INNER JOIN {} mod\n",
    "        ON ts.object_id=mod.object_id\n",
    "        \"\"\".format(training_metadata, \"MODIFIERS\")\n",
    "metadata_DF = sqlContext.sql(metadataSQL)\n",
    "metadata_DF.registerTempTable(\"TRAINING_SET_METADATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, now we have the padded, augmented training set and the augmented training set metadata\n",
    "\n",
    "#### Create the training set padded feature vectors dataframe and instantiate it as a hive table.\n",
    "\n",
    "Points to note.\n",
    "\n",
    "- The following SQL could all be incorporated into one statement, but in the interests of clarity, we have broken it down into relevent component parts\n",
    "- Calculating the MJD intervals utilises SQL WINDOW functionality. This needs to be used carefully, because window functionality will cause Spark to perform a hash sort which is potentially a very expensive operation and on larger datasets can cause a spill to disk which is to be avoided if at all possible. \n",
    "- Two separate tables need to be utilised for these sorts, because Hive doesn not support a WINDOW statement on the same field (mjd) with different ORDER BY clauses in the OVER (PARTITION BY ... ORDER BY ...) WINDOW. Hence, we have CTE1 and CTE2 tables for this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How these modifiers are applied - we join the modifiers table to the padded training (lines 15-17) set and run the calculation in the select statement. (lines 11 and 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the basic data from the padded training_set we created earlier, \"PADDED_TRAINING_SET\"\n",
    "CTE1_sql=\"\"\"\n",
    "        select ts.object_id, mjd,\n",
    "        mjd - first_value(mjd) over w as mjdInt,\n",
    "        case when lag(mjd) OVER w is null then\n",
    "            0\n",
    "        else\n",
    "            mjd - lag(mjd) over w \n",
    "        end as deltaMjd,\n",
    "        passband,\n",
    "        flux,\n",
    "        flux_err,\n",
    "        detected,\n",
    "        row_number() OVER w as rownum\n",
    "        from {} ts\n",
    "        WINDOW w AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    "\"\"\".format(\"AUGMENTED_TRAINING\")\n",
    "\n",
    "CTE1_df=sqlContext.sql(CTE1_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in memory table\n",
    "CTE1_df.registerTempTable(\"CTE1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTE2_sql=\"\"\"\n",
    "        select object_id,\n",
    "        first_value(mjd) OVER x - mjd as rval,\n",
    "        row_number() OVER x as rownum\n",
    "        from {}\n",
    "        WINDOW x AS (PARTITION BY object_id ORDER BY mjd DESC)\n",
    "\"\"\".format(\"AUGMENTED_TRAINING\")\n",
    "\n",
    "CTE2_df=sqlContext.sql(CTE2_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in memory table\n",
    "CTE2_df.registerTempTable(\"CTE2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So in here we include the calculated fields for the metadata, as well as creating the augmented hostgal_photoz field.\n",
    "\n",
    "We do need to add modifier values based on minimum and maximum flux values for each observed object - this is the metaVal field from the MODIFIERS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metadata we need for the feature vectors\n",
    "meta_sql=\"\"\"\n",
    "        select meta.object_id, gal_l, gal_b, ddf, hostgal_specz,\n",
    "        rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5) hostgal_photoz,\n",
    "        hostgal_photoz_err, mwebv,target,\n",
    "        case when hostgal_photoz > 0 \n",
    "            then 1  -- CAST(1 AS BOOLEAN)\n",
    "            else 0 --CAST(0 AS BOOLEAN)\n",
    "            end as photoz_positive,\n",
    "        --6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99\n",
    "        case \n",
    "            when target= 6 then 0\n",
    "            when target= 15 then 1\n",
    "            when target= 16 then 2\n",
    "            when target= 42 then 3\n",
    "            when target= 52 then 4\n",
    "            when target= 53 then 5\n",
    "            when target= 62 then 6\n",
    "            when target= 64 then 7\n",
    "            when target= 65 then 8\n",
    "            when target= 67 then 9\n",
    "            when target= 88 then 10\n",
    "            when target= 90 then 11\n",
    "            when target= 92 then 12\n",
    "            when target= 95 then 13\n",
    "            when target= 99 then 14\n",
    "            else 14\n",
    "            end mapped_target,\n",
    "            metaVal\n",
    "\n",
    "        from {} meta\n",
    "\"\"\".format(\"TRAINING_SET_METADATA\")\n",
    "\n",
    "meta_df=sqlContext.sql(meta_sql)\n",
    "meta_df.registerTempTable(\"meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the intermediate table\n",
    "\n",
    "This table sets up the arrays for the object metadata, spaced out to ten elements and create the key value pairs for the mjd, passband, flux etc arrays that will be created in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_sql=\"\"\"\n",
    "        select CTE1.object_id,mapped_target as target,\n",
    "        array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,photoz_positive,metaVal) as meta,\n",
    "        double(hostgal_specz) as specz,\n",
    "        MAP(\n",
    "            'interval', mjdInt,\n",
    "            'deltaMjd', deltaMjd,\n",
    "            'passband',passband,\n",
    "            'rval', rval,\n",
    "            'flux',flux,\n",
    "            'flux_err',flux_err,\n",
    "            'detected',detected,\n",
    "            'received_wavelength', case \n",
    "                                    when CTE1.passband = 0 then 357/1000\n",
    "                                    when CTE1.passband = 1 then 477/1000\n",
    "                                    when CTE1.passband = 2 then 621/1000\n",
    "                                    when CTE1.passband = 3 then 754/1000\n",
    "                                    when CTE1.passband = 4 then 871/1000\n",
    "                                    else  1004/1000\n",
    "                                    end,\n",
    "            'source_wavelength', case \n",
    "                                    when CTE1.passband = 0 then 357 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 1 then 477 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 2 then 621 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 3 then 754 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 4 then 871 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    end\n",
    "        \n",
    "        ) AS kv\n",
    "        from CTE1 \n",
    "            inner join CTE2\n",
    "                on CTE1.object_id=CTE2.object_id\n",
    "                and CTE1. rownum=CTE2.rownum\n",
    "            inner join meta\n",
    "                on CTE1.object_id = meta.object_id\n",
    "\"\"\"\n",
    "\n",
    "struct_df=sqlContext.sql(struct_sql)\n",
    "struct_df.registerTempTable(\"struct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full training set of feature vectors, padded to 256 elements.\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,target,specz,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'interval',             collect_list(float(a.kv['interval'])) ,\n",
    "    'deltaMjd',             collect_list(float(a.kv['deltaMjd'])) ,\n",
    "    'rval',                 collect_list(float(a.kv['rval'])) ,\n",
    "    'flux',                 collect_list(float(a.kv['flux'])) ,\n",
    "    'flux_err',             collect_list(float(a.kv['flux_err'])) ,\n",
    "    'detected',             collect_list(int(a.kv['detected'])) ,\n",
    "    'source_wavelength',    collect_list(float(a.kv['source_wavelength'])) ,\n",
    "    'received_wavelength',  collect_list(float(a.kv['received_wavelength'])) \n",
    "    )\n",
    ") as hist\n",
    "from struct a\n",
    "group by object_id, meta,target,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- target: integer (nullable = false)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- hist: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- interval: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- deltaMjd: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- rval: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- source_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- received_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [object_id#46, meta#153, target#152, specz#154, band#167, hist#168]\n",
      "   +- InMemoryRelation [object_id#46, meta#153, target#152, specz#154, band#167, hist#168], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- ObjectHashAggregate(keys=[object_id#46, meta#153, target#152, specz#154], functions=[collect_list(cast(kv#155[passband] as int), 0, 0), collect_list(cast(kv#155[interval] as float), 0, 0), collect_list(cast(kv#155[deltaMjd] as float), 0, 0), collect_list(cast(kv#155[rval] as float), 0, 0), collect_list(cast(kv#155[flux] as float), 0, 0), collect_list(cast(kv#155[flux_err] as float), 0, 0), collect_list(cast(kv#155[detected] as int), 0, 0), collect_list(cast(kv#155[source_wavelength] as float), 0, 0), collect_list(cast(kv#155[received_wavelength] as float), 0, 0)])\n",
      "            +- ObjectHashAggregate(keys=[object_id#46, meta#153, target#152, specz#154], functions=[partial_collect_list(cast(kv#155[passband] as int), 0, 0), partial_collect_list(cast(kv#155[interval] as float), 0, 0), partial_collect_list(cast(kv#155[deltaMjd] as float), 0, 0), partial_collect_list(cast(kv#155[rval] as float), 0, 0), partial_collect_list(cast(kv#155[flux] as float), 0, 0), partial_collect_list(cast(kv#155[flux_err] as float), 0, 0), partial_collect_list(cast(kv#155[detected] as int), 0, 0), partial_collect_list(cast(kv#155[source_wavelength] as float), 0, 0), partial_collect_list(cast(kv#155[received_wavelength] as float), 0, 0)])\n",
      "               +- *(41) Project [object_id#46, mapped_target#139 AS target#152, array(0.0, 0.0, 0.0, 0.0, cast(ddf#92 as double), hostgal_specz#93, hostgal_photoz#137, mwebv#97, cast(photoz_positive#138 as double), metaVal#2) AS meta#153, hostgal_specz#93 AS specz#154, map(interval, mjdInt#112, deltaMjd, deltaMjd#113, passband, cast(passband#29 as double), rval, rval#129, flux, flux#66, flux_err, flux_err#20, detected, cast(detected#32 as double), received_wavelength, CASE WHEN (passband#29 = 0) THEN 0.357 WHEN (passband#29 = 1) THEN 0.477 WHEN (passband#29 = 2) THEN 0.621 WHEN (passband#29 = 3) THEN 0.754 WHEN (passband#29 = 4) THEN 0.871 ELSE 1.004 END, source_wavelength, CASE WHEN (passband#29 = 0) THEN ((357.0 / (hostgal_photoz#137 + 1.0)) / 1000.0) WHEN (passband#29 = 1) THEN ((477.0 / (hostgal_photoz#137 + 1.0)) / 1000.0) WHEN (passband#29 = 2) THEN ((621.0 / (hostgal_photoz#137 + 1.0)) / 1000.0) WHEN (passband#29 = 3) THEN ((754.0 / (hostgal_photoz#137 + 1.0)) / 1000.0) WHEN (passband#29 = 4) THEN ((871.0 / (hostgal_photoz#137 + 1.0)) / 1000.0) ELSE ((1004.0 / (hostgal_photoz#137 + 1.0)) / 1000.0) END) AS kv#155]\n",
      "                  +- *(41) SortMergeJoin [object_id#46], [object_id#87], Inner\n",
      "                     :- *(36) Sort [object_id#46 ASC NULLS FIRST], false, 0\n",
      "                     :  +- Exchange hashpartitioning(object_id#46, 200)\n",
      "                     :     +- *(35) Project [object_id#46, mjdInt#112, deltaMjd#113, passband#29, flux#66, flux_err#20, detected#32, rval#129]\n",
      "                     :        +- *(35) SortMergeJoin [object_id#46, rownum#114], [object_id#156, rownum#130], Inner\n",
      "                     :           :- *(18) Sort [object_id#46 ASC NULLS FIRST, rownum#114 ASC NULLS FIRST], false, 0\n",
      "                     :           :  +- Exchange hashpartitioning(object_id#46, rownum#114, 200)\n",
      "                     :           :     +- *(17) Project [object_id#46, (mjd#65 - _we0#117) AS mjdInt#112, CASE WHEN isnull(_we1#118) THEN 0.0 ELSE (mjd#65 - _we2#119) END AS deltaMjd#113, passband#29, flux#66, flux_err#20, detected#32, rownum#114]\n",
      "                     :           :        +- *(17) Filter isnotnull(rownum#114)\n",
      "                     :           :           +- Window [first(mjd#65, false) windowspecdefinition(object_id#46, mjd#65 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#117, lag(mjd#65, 1, null) windowspecdefinition(object_id#46, mjd#65 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we1#118, lag(mjd#65, 1, null) windowspecdefinition(object_id#46, mjd#65 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we2#119, row_number() windowspecdefinition(object_id#46, mjd#65 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#114], [object_id#46], [mjd#65 ASC NULLS FIRST]\n",
      "                     :           :              +- *(16) Sort [object_id#46 ASC NULLS FIRST, mjd#65 ASC NULLS FIRST], false, 0\n",
      "                     :           :                 +- Exchange hashpartitioning(object_id#46, 200)\n",
      "                     :           :                    +- *(15) Project [object_id#46, (mjd#28 * dt#68) AS mjd#65, passband#29, newflux#67 AS flux#66, flux_err#20, detected#32]\n",
      "                     :           :                       +- *(15) Project [object_id#46, mjd#28, passband#29, ((rand(7647355542523003168) * ((flux#19 + flux_err#20) - ((flux#19 - flux_err#20) / 1.5))) + ((flux#19 - flux_err#20) / 1.5)) AS newFlux#67, flux_err#20, detected#32, ((1.0 + hostgal_photoz#76) / (1.0 + ((rand(-7386144625099023058) * ((hostgal_photoz#76 + hostgal_photoz_err#77) - ((hostgal_photoz#76 - hostgal_photoz_err#77) / 1.5))) + ((hostgal_photoz#76 - hostgal_photoz_err#77) / 1.5)))) AS dt#68]\n",
      "                     :           :                          +- *(15) BroadcastHashJoin [object_id#46], [object_id#69], Inner, BuildRight\n",
      "                     :           :                             :- *(15) Sort [object_id#46 ASC NULLS FIRST, mjd#28 ASC NULLS FIRST], true, 0\n",
      "                     :           :                             :  +- Exchange rangepartitioning(object_id#46 ASC NULLS FIRST, mjd#28 ASC NULLS FIRST, 200)\n",
      "                     :           :                             :     +- *(13) Project [object_id#46, mjd#28, passband#29, (flux#30 / HistModifier#1) AS flux#19, (flux_err#31 / HistModifier#1) AS flux_err#20, detected#32]\n",
      "                     :           :                             :        +- *(13) SortMergeJoin [object_id#46], [object_id#3], Inner\n",
      "                     :           :                             :           :- *(10) Sort [object_id#46 ASC NULLS FIRST], false, 0\n",
      "                     :           :                             :           :  +- Exchange hashpartitioning(object_id#46, 200)\n",
      "                     :           :                             :           :     +- *(9) Sort [object_id#46 ASC NULLS FIRST, mjd#28 DESC NULLS LAST], true, 0\n",
      "                     :           :                             :           :        +- Exchange rangepartitioning(object_id#46 ASC NULLS FIRST, mjd#28 DESC NULLS LAST, 200)\n",
      "                     :           :                             :           :           +- *(8) Project [object_id#46, CASE WHEN isnull(mjd#54) THEN cast(padMJD#22 as double) ELSE mjd#54 END AS mjd#28, CASE WHEN isnull(passband#55) THEN padPassband#23 ELSE cast(passband#55 as int) END AS passband#29, CASE WHEN isnull(flux#56) THEN cast(padFlux#24 as double) ELSE flux#56 END AS flux#30, CASE WHEN isnull(flux_err#57) THEN cast(padFlux_err#25 as double) ELSE flux_err#57 END AS flux_err#31, CASE WHEN isnull(detected#58) THEN padDetected#26 ELSE cast(detected#58 as int) END AS detected#32]\n",
      "                     :           :                             :           :              +- SortMergeJoin [object_id#46, rownum#21], [object_id#53, rownum#27], LeftOuter\n",
      "                     :           :                             :           :                 :- *(5) Sort [object_id#46 ASC NULLS FIRST, rownum#21 ASC NULLS FIRST], false, 0\n",
      "                     :           :                             :           :                 :  +- Exchange hashpartitioning(object_id#46, rownum#21, 200)\n",
      "                     :           :                             :           :                 :     +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "                     :           :                             :           :                 :        :- *(2) HashAggregate(keys=[object_id#46], functions=[])\n",
      "                     :           :                             :           :                 :        :  +- Exchange hashpartitioning(object_id#46, 200)\n",
      "                     :           :                             :           :                 :        :     +- *(1) HashAggregate(keys=[object_id#46], functions=[])\n",
      "                     :           :                             :           :                 :        :        +- *(1) Filter isnotnull(object_id#46)\n",
      "                     :           :                             :           :                 :        :           +- HiveTableScan [object_id#46], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#46, mjd#47, passband#48, flux#49, flux_err#50, detected#51]\n",
      "                     :           :                             :           :                 :        +- BroadcastExchange IdentityBroadcastMode\n",
      "                     :           :                             :           :                 :           +- *(4) Project [rownum#21]\n",
      "                     :           :                             :           :                 :              +- *(4) Filter (isnotnull(rownum#21) && (rownum#21 <= 256))\n",
      "                     :           :                             :           :                 :                 +- Window [row_number() windowspecdefinition(object_id#34 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#21], [object_id#34 ASC NULLS FIRST]\n",
      "                     :           :                             :           :                 :                    +- *(3) Sort [object_id#34 ASC NULLS FIRST], false, 0\n",
      "                     :           :                             :           :                 :                       +- Exchange SinglePartition\n",
      "                     :           :                             :           :                 :                          +- HiveTableScan [object_id#34], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#34, ra#35, decl#36, gal_l#37, gal_b#38, ddf#39, hostgal_specz#40, hostgal_photoz#41, hostgal_photoz_err#42, distmod#43, mwebv#44, target#45]\n",
      "                     :           :                             :           :                 +- *(7) Sort [object_id#53 ASC NULLS FIRST, rownum#27 ASC NULLS FIRST], false, 0\n",
      "                     :           :                             :           :                    +- Exchange hashpartitioning(object_id#53, rownum#27, 200)\n",
      "                     :           :                             :           :                       +- Window [row_number() windowspecdefinition(object_id#53, mjd#54 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#27], [object_id#53], [mjd#54 DESC NULLS LAST]\n",
      "                     :           :                             :           :                          +- *(6) Sort [object_id#53 ASC NULLS FIRST, mjd#54 DESC NULLS LAST], false, 0\n",
      "                     :           :                             :           :                             +- Exchange hashpartitioning(object_id#53, 200)\n",
      "                     :           :                             :           :                                +- HiveTableScan [object_id#53, mjd#54, passband#55, flux#56, flux_err#57, detected#58], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#53, mjd#54, passband#55, flux#56, flux_err#57, detected#58]\n",
      "                     :           :                             :           +- *(12) Sort [object_id#3 ASC NULLS FIRST], false, 0\n",
      "                     :           :                             :              +- *(12) HashAggregate(keys=[object_id#3], functions=[max(flux#6), min(flux#6)])\n",
      "                     :           :                             :                 +- Exchange hashpartitioning(object_id#3, 200)\n",
      "                     :           :                             :                    +- *(11) HashAggregate(keys=[object_id#3], functions=[partial_max(flux#6), partial_min(flux#6)])\n",
      "                     :           :                             :                       +- *(11) Filter isnotnull(object_id#3)\n",
      "                     :           :                             :                          +- HiveTableScan [object_id#3, flux#6], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#3, mjd#4, passband#5, flux#6, flux_err#7, detected#8]\n",
      "                     :           :                             +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                     :           :                                +- *(14) Filter isnotnull(object_id#69)\n",
      "                     :           :                                   +- HiveTableScan [object_id#69, hostgal_photoz#76, hostgal_photoz_err#77], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#69, ra#70, decl#71, gal_l#72, gal_b#73, ddf#74, hostgal_specz#75, hostgal_photoz#76, hostgal_photoz_err#77, distmod#78, mwebv#79, target#80]\n",
      "                     :           +- *(34) Sort [object_id#156 ASC NULLS FIRST, rownum#130 ASC NULLS FIRST], false, 0\n",
      "                     :              +- Exchange hashpartitioning(object_id#156, rownum#130, 200)\n",
      "                     :                 +- *(33) Project [object_id#156, (_we0#133 - mjd#65) AS rval#129, rownum#130]\n",
      "                     :                    +- *(33) Filter isnotnull(rownum#130)\n",
      "                     :                       +- Window [first(mjd#65, false) windowspecdefinition(object_id#156, mjd#65 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#133, row_number() windowspecdefinition(object_id#156, mjd#65 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#130], [object_id#156], [mjd#65 DESC NULLS LAST]\n",
      "                     :                          +- *(32) Sort [object_id#156 ASC NULLS FIRST, mjd#65 DESC NULLS LAST], false, 0\n",
      "                     :                             +- Exchange hashpartitioning(object_id#156, 200)\n",
      "                     :                                +- *(31) Project [object_id#156, (mjd#28 * dt#68) AS mjd#65]\n",
      "                     :                                   +- *(31) Project [object_id#156, mjd#28, ((1.0 + hostgal_photoz#76) / (1.0 + ((rand(-7386144625099023058) * ((hostgal_photoz#76 + hostgal_photoz_err#77) - ((hostgal_photoz#76 - hostgal_photoz_err#77) / 1.5))) + ((hostgal_photoz#76 - hostgal_photoz_err#77) / 1.5)))) AS dt#68]\n",
      "                     :                                      +- *(31) BroadcastHashJoin [object_id#156], [object_id#69], Inner, BuildRight\n",
      "                     :                                         :- *(31) Sort [object_id#156 ASC NULLS FIRST, mjd#28 ASC NULLS FIRST], true, 0\n",
      "                     :                                         :  +- Exchange rangepartitioning(object_id#156 ASC NULLS FIRST, mjd#28 ASC NULLS FIRST, 200)\n",
      "                     :                                         :     +- *(29) Project [object_id#156, mjd#28]\n",
      "                     :                                         :        +- *(29) BroadcastHashJoin [object_id#156], [object_id#3], Inner, BuildRight\n",
      "                     :                                         :           :- *(29) Sort [object_id#156 ASC NULLS FIRST, mjd#28 DESC NULLS LAST], true, 0\n",
      "                     :                                         :           :  +- Exchange rangepartitioning(object_id#156 ASC NULLS FIRST, mjd#28 DESC NULLS LAST, 200)\n",
      "                     :                                         :           :     +- *(26) Project [object_id#156, CASE WHEN isnull(mjd#54) THEN cast(padMJD#22 as double) ELSE mjd#54 END AS mjd#28]\n",
      "                     :                                         :           :        +- SortMergeJoin [object_id#156, rownum#21], [object_id#53, rownum#27], LeftOuter\n",
      "                     :                                         :           :           :- *(23) Sort [object_id#156 ASC NULLS FIRST, rownum#21 ASC NULLS FIRST], false, 0\n",
      "                     :                                         :           :           :  +- Exchange hashpartitioning(object_id#156, rownum#21, 200)\n",
      "                     :                                         :           :           :     +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "                     :                                         :           :           :        :- *(20) HashAggregate(keys=[object_id#156], functions=[])\n",
      "                     :                                         :           :           :        :  +- ReusedExchange [object_id#156], Exchange hashpartitioning(object_id#46, 200)\n",
      "                     :                                         :           :           :        +- ReusedExchange [rownum#21], BroadcastExchange IdentityBroadcastMode\n",
      "                     :                                         :           :           +- *(25) Sort [object_id#53 ASC NULLS FIRST, rownum#27 ASC NULLS FIRST], false, 0\n",
      "                     :                                         :           :              +- Exchange hashpartitioning(object_id#53, rownum#27, 200)\n",
      "                     :                                         :           :                 +- Window [row_number() windowspecdefinition(object_id#53, mjd#54 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#27], [object_id#53], [mjd#54 DESC NULLS LAST]\n",
      "                     :                                         :           :                    +- *(24) Sort [object_id#53 ASC NULLS FIRST, mjd#54 DESC NULLS LAST], false, 0\n",
      "                     :                                         :           :                       +- Exchange hashpartitioning(object_id#53, 200)\n",
      "                     :                                         :           :                          +- HiveTableScan [object_id#53, mjd#54], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#53, mjd#54, passband#55, flux#56, flux_err#57, detected#58]\n",
      "                     :                                         :           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "                     :                                         :              +- *(28) HashAggregate(keys=[object_id#3], functions=[])\n",
      "                     :                                         :                 +- ReusedExchange [object_id#3], Exchange hashpartitioning(object_id#46, 200)\n",
      "                     :                                         +- ReusedExchange [object_id#69, hostgal_photoz#76, hostgal_photoz_err#77], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                     +- *(40) Sort [object_id#87 ASC NULLS FIRST], false, 0\n",
      "                        +- Exchange hashpartitioning(object_id#87, 200)\n",
      "                           +- *(39) Project [object_id#87, ddf#92, hostgal_specz#93, ((rand(5888548146644204981) * ((hostgal_photoz#94 + hostgal_photoz_err#95) - ((hostgal_photoz#94 - hostgal_photoz_err#95) / 1.5))) + ((hostgal_photoz#94 - hostgal_photoz_err#95) / 1.5)) AS hostgal_photoz#137, mwebv#97, CASE WHEN (hostgal_photoz#94 > 0.0) THEN 1 ELSE 0 END AS photoz_positive#138, CASE WHEN (target#98 = 6) THEN 0 WHEN (target#98 = 15) THEN 1 WHEN (target#98 = 16) THEN 2 WHEN (target#98 = 42) THEN 3 WHEN (target#98 = 52) THEN 4 WHEN (target#98 = 53) THEN 5 WHEN (target#98 = 62) THEN 6 WHEN (target#98 = 64) THEN 7 WHEN (target#98 = 65) THEN 8 WHEN (target#98 = 67) THEN 9 WHEN (target#98 = 88) THEN 10 WHEN (target#98 = 90) THEN 11 WHEN (target#98 = 92) THEN 12 WHEN (target#98 = 95) THEN 13 WHEN (target#98 = 99) THEN 14 ELSE 14 END AS mapped_target#139, metaVal#2]\n",
      "                              +- *(39) BroadcastHashJoin [object_id#87], [object_id#3], Inner, BuildLeft\n",
      "                                 :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                                 :  +- *(37) Filter isnotnull(object_id#87)\n",
      "                                 :     +- HiveTableScan [object_id#87, ddf#92, hostgal_specz#93, hostgal_photoz#94, hostgal_photoz_err#95, mwebv#97, target#98], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#87, ra#88, decl#89, gal_l#90, gal_b#91, ddf#92, hostgal_specz#93, hostgal_photoz#94, hostgal_photoz_err#95, distmod#96, mwebv#97, target#98]\n",
      "                                 +- *(39) HashAggregate(keys=[object_id#3], functions=[max(flux#6), min(flux#6)])\n",
      "                                    +- ReusedExchange [object_id#3, max#208, min#209], Exchange hashpartitioning(object_id#3, 200)\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the feature vector table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "TABLE='training_set_augmented_vectors' #- original full hist ARRAY - STRUCT - ARRAY\n",
    "\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain plan comparisons\n",
    "\n",
    "What we can see here is the advantage of physically instantiating these vector creation scripts into a physical table. Compare the explain plan below with the plan above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AugmentDF=sqlContext.sql(\"select * from training_set_flat_augmented_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet plasticc.training_set_flat_augmented_vectors[object_id#0,meta#1,target#2,specz#3,band#4,interval#5,deltaMjd#6,rval#7,flux#8,flux_err#9,detected#10,source_wavelength#11,received_wavelength#12] Batched: false, Format: Parquet, Location: InMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/plasticc.db/train..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<object_id:int,meta:array<double>,target:int,specz:double,band:array<double>,interval:array...\n"
     ]
    }
   ],
   "source": [
    "AugmentDF.explain()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
