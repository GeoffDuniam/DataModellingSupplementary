{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model test\n",
    "\n",
    "This notebook illustrates the changes make to incorporate the first data models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## UNNECESSARY - because we're pulling the data from Hive\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 25\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1 # For testing only! Was 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data(list_x, list_y = None):\n",
    "    X = {}\n",
    "    for k in list_x[0].keys():\n",
    "\n",
    "        list = [x[k] for x in list_x]\n",
    "        X[k] = np.concatenate(list)\n",
    "\n",
    "    if list_y is None:\n",
    "        return X\n",
    "    else:\n",
    "        return X, np.concatenate(list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df):\n",
    "    \n",
    "    all_y = np.array(df['target'], dtype = 'int32')\n",
    "\n",
    "    y_count = np.unique(all_y, return_counts=True)[1]\n",
    "\n",
    "    wtable = np.ones(len(classes))\n",
    "\n",
    "    for i in range(0, y_count.shape[0]):\n",
    "        wtable[i] = y_count[i] / all_y.shape[0]\n",
    "\n",
    "    return wtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(itemslist):\n",
    "\n",
    "    keys = itemslist[0].keys()\n",
    "    print('creating X')\n",
    "    X = {\n",
    "            'id': np.array([i['id'] for i in itemslist], dtype='int32'),\n",
    "            'meta': np.array([i['meta'] for i in itemslist]),\n",
    "            'band': pad_sequences([i['band'] for i in itemslist], maxlen=sequence_len, dtype='int32'),\n",
    "            'hist': pad_sequences([i['hist'] for i in itemslist], maxlen=sequence_len, dtype='float32'),\n",
    "        }\n",
    "    print('creating Y')\n",
    "    Y = to_categorical([i['target'] for i in itemslist], num_classes=len(classes))\n",
    "\n",
    "    X['hist'][:,:,0] = 0 # remove abs time\n",
    "#    X['hist'][:,:,1] = 0 # remove flux\n",
    "#    X['hist'][:,:,2] = 0 # remove flux err\n",
    "    X['hist'][:,:,3] = 0 # remove detected flag\n",
    "#    X['hist'][:,:,4] = 0 # remove fwd intervals\n",
    "#    X['hist'][:,:,5] = 0 # remove bwd intervals\n",
    "#    X['hist'][:,:,6] = 0 # remove source wavelength\n",
    "    X['hist'][:,:,7] = 0 # remove received wavelength\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_intervals(sample):\n",
    "\n",
    "    hist = sample['hist']\n",
    "    band = sample['band']\n",
    "\n",
    "    hist[:,4] = np.ediff1d(hist[:,0], to_begin = [0])\n",
    "    hist[:,5] = np.ediff1d(hist[:,0], to_end = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_sample(s, augmentate=True):\n",
    "    c = copy.deepcopy(s)\n",
    "\n",
    "    if not augmentate:\n",
    "        return c\n",
    "\n",
    "    band = []\n",
    "    hist = []\n",
    "\n",
    "    drop_rate = 0.3\n",
    "\n",
    "    # drop some records\n",
    "    for k in range(s['band'].shape[0]):\n",
    "        if random.uniform(0, 1) >= drop_rate:\n",
    "            band.append(s['band'][k])\n",
    "            hist.append(s['hist'][k])\n",
    "\n",
    "    c['hist'] = np.array(hist, dtype='float32')\n",
    "    c['band'] = np.array(band, dtype='int32')\n",
    "\n",
    "    set_intervals(c)\n",
    "            \n",
    "    new_z = random.normalvariate(c['meta'][5], c['meta'][6] / 1.5) # hostgal_photoz and hostgal_photoz_err\n",
    "    new_z = max(new_z, 0)\n",
    "    new_z = min(new_z, 5)\n",
    "\n",
    "    dt = (1 + c['meta'][5]) / (1 + new_z) # hostgal_photoz\n",
    "    c['meta'][5] = new_z\n",
    "\n",
    "    # augmentation for flux\n",
    "    c['hist'][:,1] = np.random.normal(c['hist'][:,1], c['hist'][:,2] / 1.5) # flux and flux_err\n",
    "\n",
    "    # multiply time intervals and wavelength to apply augmentation for red shift\n",
    "    c['hist'][:,0] *= dt\n",
    "    c['hist'][:,4] *= dt\n",
    "    c['hist'][:,5] *= dt\n",
    "    c['hist'][:,6] *= dt\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_counts(samples, wtable, augmentate):\n",
    "    maxpr = np.max(wtable)\n",
    "    counts = maxpr / wtable\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    for s in samples:\n",
    "\n",
    "        index += 1\n",
    "        print('Normalizing {0}/{1}   '.format(index, len(samples)), end='\\r')\n",
    "\n",
    "        res.append(s)\n",
    "        count = int(3 * counts[s['target']]) - 1\n",
    "\n",
    "        for i in range(0, count):\n",
    "            res.append(copy_sample(s, augmentate))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentate(samples, gl_count, exgl_count):\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    for s in samples:\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "        if index % 1000 == 0:\n",
    "            print('Augmenting {0}/{1}   '.format(index, len(samples)), end='\\r')\n",
    "\n",
    "        count = gl_count if (s['meta'][8] == 0) else exgl_count\n",
    "\n",
    "        for i in range(0, count):\n",
    "            res.append(copy_sample(s))\n",
    "\n",
    "    print()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(mjd_df, passband_df, flux_df, flux_err_df, detected_df, meta_df, extragalactic=None, use_specz=False):\n",
    "\n",
    "    samples = []\n",
    "    groups = mjd_df.groupby('object_id')\n",
    "\n",
    "    for g in groups:\n",
    "        id=g[0]\n",
    "\n",
    "        sample = {}\n",
    "        sample['id'] = int(id)\n",
    "\n",
    "        meta = meta_df.loc[meta_df['object_id'] == id]\n",
    "\n",
    "        if extragalactic == True and float(meta['hostgal_photoz']) == 0:\n",
    "            continue\n",
    "\n",
    "        if extragalactic == False and float(meta['hostgal_photoz']) > 0:\n",
    "            continue    \n",
    "\n",
    "        if 'target' in meta:\n",
    "            sample['target'] = np.where(classes == int(meta['target']))[0][0]\n",
    "        else:\n",
    "            sample['target'] = len(classes) - 1   \n",
    "\n",
    "        sample['meta'] = np.zeros(10, dtype = 'float32')\n",
    "\n",
    "        sample['meta'][4] = meta['ddf']\n",
    "        sample['meta'][5] = meta['hostgal_photoz']\n",
    "        sample['meta'][6] = meta['hostgal_photoz_err']\n",
    "        sample['meta'][7] = meta['mwebv']\n",
    "        sample['meta'][8] = float(meta['hostgal_photoz']) > 0 # returns True or false\n",
    "\n",
    "        sample['specz'] = float(meta['hostgal_specz'])\n",
    "\n",
    "\n",
    "        if use_specz:\n",
    "            sample['meta'][5] = float(meta['hostgal_specz'])\n",
    "            sample['meta'][6] = 0.0\n",
    "\n",
    "        z = float(sample['meta'][5])\n",
    "\n",
    "        # we need to drop the object_id from the pivot records. We can use any of the pivot dataframes,\n",
    "        # because they all have the same shape, coming from a Hive table definition. We'll use the MJD\n",
    "        # pivot dataframe to set up the indexes we want. How we do this - in stages\n",
    "        # 1. Create a data frame for the pivot dataframes, on for each object_id\n",
    "        # 2. Use dropna to remove NAN column values\n",
    "        # 3. Cast that dataframe to a numpy array and get the shape\n",
    "        # 4. Use the mjd array as a base, create an index list of the columns we want - ie we're dropping the object_id\n",
    "        # 5. Use the index ro truncate the object_id column from the rest of the arrays\n",
    "        # 6. finally, we need to reshape the arrays from [1;cols] to [cols,]   \n",
    "\n",
    "        mjd = np.array(mjd_df.loc[mjd_df['object_id'] == id].dropna(axis='columns'), dtype='float32')\n",
    "        r,c=mjd.shape\n",
    "\n",
    "        idx_OUT_columns = [0]\n",
    "        idx_IN_columns = [i for i in range(np.shape(mjd)[1]) if i not in idx_OUT_columns]\n",
    "\n",
    "        mjd = mjd[:,idx_IN_columns].reshape(c-1,)\n",
    "        band = np.array(passband_df.loc[passband_df['object_id'] == id].dropna(axis='columns') , dtype='int32')[:,idx_IN_columns].reshape(c-1,)\n",
    "        flux = np.array(flux_df.loc[flux_df['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "        flux_err = np.array(flux_err_df.loc[flux_err_df['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "        detected = np.array(detected_df.loc[detected_df['object_id'] == id].dropna(axis='columns') , dtype='float32')[:,idx_IN_columns].reshape(c-1,)\n",
    "        #mjd      = np.array(g[1]['mjd'],      dtype='float32')\n",
    "        #band     = np.array(g[1]['passband'], dtype='int32')\n",
    "        #flux     = np.array(g[1]['flux'],     dtype='float32')\n",
    "        #flux_err = np.array(g[1]['flux_err'], dtype='float32')\n",
    "        #detected = np.array(g[1]['detected'], dtype='float32')  \n",
    "\n",
    "\n",
    "        mjd -= mjd[0]\n",
    "        mjd /= 100 # Earth time shift in day*100\n",
    "        mjd /= (z + 1) # Object time shift in day*100\n",
    "\n",
    "\n",
    "        received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "        received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "        source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "\n",
    "\n",
    "        sample['band'] = band + 1\n",
    "\n",
    "        sample['hist'] = np.zeros((flux.shape[0], 8), dtype='float32')\n",
    "        sample['hist'][:,0] = mjd\n",
    "        sample['hist'][:,1] = flux\n",
    "        sample['hist'][:,2] = flux_err\n",
    "        sample['hist'][:,3] = detected\n",
    "\n",
    "        sample['hist'][:,6] = (source_wavelength/1000)\n",
    "        sample['hist'][:,7] = (received_wavelength/1000)\n",
    "\n",
    "        set_intervals(sample)\n",
    "\n",
    "\n",
    "        flux_max = np.max(flux)\n",
    "        flux_min = np.min(flux)\n",
    "        flux_pow = math.log2(flux_max - flux_min)\n",
    "        sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "        sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "        sample['meta'][9] = flux_pow / 10\n",
    "\n",
    "        samples.append(sample)\n",
    "\n",
    "        if len(samples) % 1000 == 0:\n",
    "            print('Converting data {0}'.format(len(samples)), end='\\r')\n",
    "\n",
    "        if len(samples) >= limit:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    print()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mywloss(y_true,y_pred):\n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def multi_weighted_logloss(y_ohe, y_p, wtable):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    nb_pos = wtable\n",
    "\n",
    "    if nb_pos[-1] == 0:\n",
    "        nb_pos[-1] = 1\n",
    "\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss / y_ohe.shape[0]\n",
    "\n",
    "def get_model(X, Y, size=80):\n",
    "\n",
    "    hist_input = Input(shape=X['hist'][0].shape, name='hist')\n",
    "    meta_input = Input(shape=X['meta'][0].shape, name='meta')\n",
    "    band_input = Input(shape=X['band'][0].shape, name='band')\n",
    "\n",
    "    band_emb = Embedding(8, 8)(band_input)\n",
    "\n",
    "    hist = concatenate([hist_input, band_emb])\n",
    "    hist = TimeDistributed(Dense(40, activation='relu'))(hist)\n",
    "\n",
    "    rnn = Bidirectional(GRU(size, return_sequences=True))(hist)\n",
    "    rnn = SpatialDropout1D(0.5)(rnn)\n",
    "\n",
    "    gmp = GlobalMaxPool1D()(rnn)\n",
    "    gmp = Dropout(0.5)(gmp)\n",
    "\n",
    "    x = concatenate([meta_input, gmp])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    output = Dense(15, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[hist_input, meta_input, band_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(i, samples_train, samples_valid):\n",
    "    start_augment=time.time()\n",
    "    start_augmentCpu=time.clock()\n",
    "    \n",
    "    #samples_train += augmentate(samples_train, augment_count, augment_count)\n",
    "    \n",
    "    elapsed_augment=time.time() - start_augment\n",
    "    elapsed_augmentCpu=time.clock() - start_augmentCpu\n",
    "\n",
    "    patience = 1000000 // len(samples_train) + 5\n",
    "\n",
    "    start_trainingVectors=time.time()\n",
    "    start_trainingVectorsCpu=time.clock()\n",
    "\n",
    "    train_x, train_y = get_keras_data(samples_train)\n",
    "\n",
    "    elapsed_training_Vectors=time.time() - start_trainingVectors\n",
    "    elapsed_training_VectorsCpu=time.clock() - start_trainingVectorsCpu\n",
    "\n",
    "    print(len(samples_train))\n",
    "    \n",
    "    del samples_train\n",
    "    \n",
    "    start_validationVectors=time.time()\n",
    "    start_validationVectorsCpu=time.clock()\n",
    "\n",
    "    valid_x, valid_y = get_keras_data(samples_valid)\n",
    "    del samples_valid\n",
    "    \n",
    "    elapsed_validation_Vectors=time.time() - start_validationVectors\n",
    "    elapsed_validation_VectorsCpu=time.clock() - start_validationVectorsCpu\n",
    "\n",
    "    model = get_model(train_x, train_y)\n",
    "\n",
    "    if i == 1: model.summary()\n",
    "    model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    print('Training model {0} of {1}, Patience: {2}'.format(i, num_models, patience))\n",
    "    filename = 'model_{0:03d}.hdf5'.format(i)\n",
    "    callbacks = [EarlyStopping(patience=patience, verbose=1), ModelCheckpoint(filename, save_best_only=True)]\n",
    "\n",
    "    model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=max_epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "\n",
    "    model = load_model(filename, custom_objects={'mywloss': mywloss})\n",
    "\n",
    "    preds = model.predict(valid_x, batch_size=batch_size2)\n",
    "    loss = multi_weighted_logloss(valid_y, preds, wtable)\n",
    "    acc = accuracy_score(np.argmax(valid_y, axis=1), np.argmax(preds,axis=1))\n",
    "    print('MW Loss: {0:.4f}, Accuracy: {1:.4f}'.format(loss, acc))\n",
    "    \n",
    "    \n",
    "    return  elapsed_augment,elapsed_augmentCpu,\\\n",
    "            elapsed_training_Vectors,elapsed_training_VectorsCpu,\\\n",
    "            elapsed_validation_Vectors,elapsed_validation_VectorsCpu, \\\n",
    "            train_x, train_y\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training meta data from hive\n"
     ]
    }
   ],
   "source": [
    "start_meta=time.time()\n",
    "start_metaCpu=time.clock()\n",
    "print('Loading the training meta data from hive')\n",
    "train_meta=sqlContext.sql(\"select * from training_set_metadata\").toPandas()\n",
    "elapsed_meta=time.time()-start_meta\n",
    "elapsed_metaCpu=time.clock()-start_metaCpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from hive...\n"
     ]
    }
   ],
   "source": [
    "print('Loading train data from hive...')\n",
    "\n",
    "start_train=time.time()\n",
    "start_trainCpu=time.clock()\n",
    "train_mjd_data=sqlContext.sql(\"select * from full_mjd_pivot\").toPandas()\n",
    "train_passband_data=sqlContext.sql(\"select * from full_passband_pivot\").toPandas()\n",
    "train_flux_data=sqlContext.sql(\"select * from full_flux_pivot\").toPandas()\n",
    "train_flux_err_data=sqlContext.sql(\"select * from full_flux_err_pivot\").toPandas()\n",
    "train_detected_data=sqlContext.sql(\"select * from full_detected_pivot\").toPandas()\n",
    "\n",
    "elapsed_train=time.time()-start_train\n",
    "elapsed_trainCpu=time.clock()-start_trainCpu\n",
    "wtable = get_wtable(train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data 7000\n"
     ]
    }
   ],
   "source": [
    "start_samples=time.time()\n",
    "start_samplesC=time.clock()\n",
    "    \n",
    "samples =  get_data(train_mjd_data, train_passband_data, \\\n",
    "                    train_flux_data, train_flux_err_data, \\\n",
    "                    train_detected_data, train_meta, \\\n",
    "                    extragalactic=None, use_specz=use_specz)\n",
    "\n",
    "elapsed_samples=time.time() - start_samples\n",
    "elapsed_samplesC=time.clock() - start_samplesC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1206 01:36:48.792671 139742447740672 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1206 01:36:48.849272 139742447740672 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1206 01:36:48.852107 139742447740672 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Y\n",
      "7063\n",
      "creating X\n",
      "creating Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1206 01:36:49.430671 139742447740672 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1206 01:36:49.443725 139742447740672 deprecation.py:506] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1206 01:36:49.595030 139742447740672 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "band (InputLayer)               (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hist (InputLayer)               (None, 256, 8)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 8)       64          band[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 16)      0           hist[0][0]                       \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 256, 40)      680         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 160)     58080       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 256, 160)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 160)          0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "meta (InputLayer)               (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 160)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 170)          0           meta[0][0]                       \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          21888       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 15)           1935        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 99,159\n",
      "Trainable params: 99,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training model 1 of 1, Patience: 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1206 01:36:49.849272 139742447740672 deprecation.py:323] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1206 01:36:51.759230 139742447740672 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7063 samples, validate on 785 samples\n",
      "Epoch 1/1\n",
      " - 57s - loss: 2.4744 - acc: 0.0917 - val_loss: 2.4892 - val_acc: 0.1809\n",
      "MW Loss: 2.4745, Accuracy: 0.1809\n",
      "70.24784922599792\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, num_models+1):\n",
    "\n",
    "    samples_train, samples_valid = train_test_split(samples, test_size=valid_size, random_state=42*i)\n",
    "    len(samples_train)\n",
    "    \n",
    "    start_train=time.time()\n",
    "    elapsed_augment,elapsed_augmentCpu,\\\n",
    "            elapsed_training_Vectors,elapsed_training_VectorsCpu,\\\n",
    "            elapsed_validation_Vectors,elapsed_validation_VectorsCpu, \\\n",
    "            train_x, train_y = \\\n",
    "            train_model(i, samples_train, samples_valid)\n",
    "    elapsed_train=time.time()-start_train\n",
    "    print(elapsed_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
