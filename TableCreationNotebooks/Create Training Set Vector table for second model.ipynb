{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training set basic vector dataframe\n",
    "\n",
    "## Second model\n",
    "* Unpadded, \n",
    "* No calculations\n",
    "* Metadata table is not incorporated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - a customised Spark Context\n",
    "\n",
    "Next cell is included as an example on how to customise the Spark Context within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the default database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full raw training set of feature vectors, unpadded, no calculations\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays.\n",
    "\n",
    "#### Note that there is no cast or collect_list to array (array (collect_list(...) )\n",
    "This changes the shape of the retrieved aray in the programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,\n",
    "array( collect_list(float(a.kv['mjd'])) ) as mjd,\n",
    "array( collect_list(int(a.kv['passband'])) ) as passband,\n",
    "array( collect_list( float(  a.kv['flux'] )  ) ) as flux,\n",
    "array( collect_list( float(  a.kv['flux_err']  ) ) ) as flux_err,\n",
    "array( collect_list(int(   a.kv['detected']   )) ) as detected\n",
    "from\n",
    "(\n",
    "select object_id,\n",
    "   MAP('mjd', mjd,'passband',passband,'flux',flux,'flux_err',flux_err,'detected',detected) AS kv\n",
    "from training_set\n",
    "\n",
    ") a\n",
    "group by object_id\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- mjd: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- passband: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- flux: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- flux_err: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- detected: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "ObjectHashAggregate(keys=[object_id#6], functions=[collect_list(cast(kv#0[mjd] as float), 0, 0), collect_list(cast(kv#0[passband] as int), 0, 0), collect_list(cast(kv#0[flux] as float), 0, 0), collect_list(cast(kv#0[flux_err] as float), 0, 0), collect_list(cast(kv#0[detected] as int), 0, 0)])\n",
      "+- Exchange hashpartitioning(object_id#6, 200)\n",
      "   +- ObjectHashAggregate(keys=[object_id#6], functions=[partial_collect_list(cast(kv#0[mjd] as float), 0, 0), partial_collect_list(cast(kv#0[passband] as int), 0, 0), partial_collect_list(cast(kv#0[flux] as float), 0, 0), partial_collect_list(cast(kv#0[flux_err] as float), 0, 0), partial_collect_list(cast(kv#0[detected] as int), 0, 0)])\n",
      "      +- *(1) Project [object_id#6, map(mjd, mjd#7, passband, cast(passband#8 as double), flux, flux#9, flux_err, flux_err#10, detected, cast(detected#11 as double)) AS kv#0]\n",
      "         +- HiveTableScan [detected#11, flux#9, flux_err#10, mjd#7, object_id#6, passband#8], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#6, mjd#7, passband#8, flux#9, flux_err#10, detected#11]\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the vector table in hive\n",
    "\n",
    "Modes are insert or append."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "TABLE='full_training_pivot'\n",
    "\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain plan comparison\n",
    "\n",
    "This next cell demonstrates the explain plan when we read the prepared data from the instantiated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet plasticc.full_training_pivot[object_id#39,mjd#40,passband#41,flux#42,flux_err#43,detected#44] Batched: false, Format: Parquet, Location: InMemoryFileIndex[hdfs://athena-1.nimbus.pawsey.org.au:8020/user/hive/warehouse/plasticc.db/full_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<object_id:int,mjd:array<array<float>>,passband:array<array<int>>,flux:array<array<float>>,...\n"
     ]
    }
   ],
   "source": [
    "pivotDF=sqlContext.sql(\"\"\"select * from full_training_pivot\"\"\")\n",
    "pivotDF.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
