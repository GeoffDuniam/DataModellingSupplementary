{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full test vector dataframe using Spark temp tables\n",
    "\n",
    "## First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # manual garbag collection to stop leaks on Collect() gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 35\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to this great article, SQL at Scale with Apache Spark SQL and DataFrames\n",
    "\n",
    "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f\n",
    "\n",
    "#### Benchmark snippets that may be usefull\n",
    "https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# set up hive session for broadcast joins\n",
    "optSql=\"\"\"\n",
    "set hive.cbo.enable=true;\n",
    "set hive.auto.convert.join=true;\n",
    "set hive.auto.convert.join.noconditionaltask=true;\n",
    "set hive.auto.convert.join.noconditionaltask.size=20971520\n",
    "set hive.auto.convert.join.use.nonstaged=true;\n",
    "set hive.mapjoin.smalltable.filesize = 30000000; \n",
    "\"\"\"\n",
    "sqlContext.sql(optSql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the padded training set\n",
    "We do this to create a standard set of features for all objects; in this case, the value provided by sequence_len (which in this case is 256).\n",
    "\n",
    "We're also going to use Spark registered tables to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = \"training_set\"\n",
    "training_metadata = \"training_set_metadata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the SQL equivalent of the keras PAD_SEQUENCES function. \n",
    "\n",
    "We create a baseline table consisting of object_ids and a 0 value for every feature as well as a rownumber. This is accomplished by a cartesian join (cross join in Hive) between the cnt nested table and the objects nesed table, resulting the baseline nested table which has 256 records for each object, with all features set to zero.\n",
    "\n",
    "Next, we create a train_set nested table containing all the training set information, ordered by the mjd value descending. this pads the data from the last value to the first as per PAD_SEQUENCES,\n",
    "\n",
    "We then create the padded training set as a left join from the baseline nested table to the train_set nested table, padded to a consistent 236 values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedSQL=\"\"\"\n",
    "with\n",
    "cnt\n",
    "as\n",
    "(\n",
    "    select rownum from \n",
    "    (\n",
    "        select row_number() over (ORDER BY object_id) as rownum\n",
    "        from {}\n",
    "    ) a\n",
    "    where rownum <=256\n",
    "),\n",
    "objects as (select object_id, 0 padMJD, 0 padPassband,0 padFlux, 0 padFlux_err,0 padDetected from {} group by object_id),\n",
    "baseline as (select * from objects CROSS JOIN cnt ), -- cartesian product with 256 values to use as the baseline\n",
    "train_set as (select *, row_number() over (partition by object_id order by mjd desc) as rownum from {}),\n",
    "paddedRev as (\n",
    "    select baseline.object_id, --train_set.mjd, baseline.padMJD,\n",
    "    case when train_set.mjd is null then baseline.padMJD else train_set.mjd end mjd,\n",
    "    case when train_set.passband is null then baseline.padPassband else train_set.passband end passband,\n",
    "    case when train_set.flux is null then baseline.padFlux else train_set.mjd end flux,\n",
    "    case when train_set.flux_err is null then baseline.padFlux_err else train_set.flux_err end flux_err,\n",
    "    case when train_set.detected is null then baseline.padDetected else train_set.detected end detected\n",
    "    from baseline left outer join train_set on baseline.object_id = train_set.object_id and baseline.rownum=train_set.rownum\n",
    "    order by baseline.object_id, mjd desc\n",
    ")\n",
    "select object_id, mjd, passband, flux, flux_err, detected from paddedRev order by object_id, mjd\n",
    "\"\"\".format(training_metadata,training_set,training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the padded training data dataframe and create a Spark temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedTrainingSet_DF = sqlContext.sql(paddedSQL)\n",
    "paddedTrainingSet_DF.registerTempTable(\"PADDED_TRAINING_SET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training set metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataSQL=\"\"\"select * from {}\"\"\".format(training_metadata)\n",
    "metadata_DF = sqlContext.sql(metadataSQL)\n",
    "metadata_DF.registerTempTable(\"TRAINING_SET_METADATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, now we have the padded training set and the training set metadata\n",
    "\n",
    "#### Create the training set padded feature vectors dataframe and instantiate it as a hive table.\n",
    "\n",
    "Points to note.\n",
    "\n",
    "- The following SQL could all be inciorporated into one statement, but in the interests of clarity, we have broken it down into relevent component parts\n",
    "- Calculating the MJD intervals utilises SQL WINDOW functionality. This needs to be used carefully, because wondow functionality will cause SPark to perform a hash sort which is a very expensive operation and on larger datasets can cause a spill to disk which is to be avoided if at all possible. \n",
    "- Two separate tables need to be utilised for these sorts, because Hive doesn not support a WINDOW statement on the same field (mjd) with different ORDER BY clauses in the OVER (PARTITION BY ... ORDER BY ...) WINDOW. Hence, we have CTE1 and CTE2 tables for this.\n",
    "\n",
    "#### next, we create the modifiers for the flux values and the flux value to be added to the metadata table\n",
    "\n",
    "This inplements the code from lines 282 - 287 in the original program, but as a set (not iteratively).\n",
    "\n",
    "        flux_max = np.max(flux)\n",
    "        flux_min = np.min(flux)\n",
    "        flux_pow = math.log2(flux_max - flux_min)\n",
    "        sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "        sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "        sample['meta'][9] = flux_pow / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiersSQL=\"\"\"\n",
    "select\n",
    "object_id,\n",
    "log2(max(flux)- min(flux)) flux_pow,\n",
    "pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "log2(max(flux)- min(flux))/10 as metaVal\n",
    "from training_set\n",
    "group by object_id\n",
    "\"\"\"\n",
    "modifiersDF=sqlContext.sql(modifiersSQL)\n",
    "\n",
    "# Create the in memory table\n",
    "modifiersDF.registerTempTable(\"MODIFIERS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How these modifiers are applied - we join the modifiers table to the padded training (lines 15-17) set and run the calculation in the select statement. (lines 11 and 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the basic data from the padded training_set we created earlier, \"PADDED_TRAINING_SET\"\n",
    "CTE1_sql=\"\"\"\n",
    "        select ts.object_id, mjd,\n",
    "        mjd - first_value(mjd) over w as mjdInt,\n",
    "        case when lag(mjd) OVER w is null then\n",
    "            0\n",
    "        else\n",
    "            mjd - lag(mjd) over w \n",
    "        end as deltaMjd,\n",
    "        passband,\n",
    "        flux / HistModifier as flux,\n",
    "        flux_err /HistModifier as flux_err,\n",
    "        detected,\n",
    "        row_number() OVER w as rownum\n",
    "        from {} ts\n",
    "            INNER JOIN {} mods\n",
    "                ON ts.object_id = mods.object_id\n",
    "        WINDOW w AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    "\"\"\".format(\"PADDED_TRAINING_SET\", \"MODIFIERS\")\n",
    "\n",
    "CTE1_df=sqlContext.sql(CTE1_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in memory table\n",
    "CTE1_df.registerTempTable(\"CTE1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTE2_sql=\"\"\"\n",
    "        select object_id,\n",
    "        first_value(mjd) OVER x - mjd as rval,\n",
    "        row_number() OVER x as rownum\n",
    "        from {}\n",
    "        WINDOW x AS (PARTITION BY object_id ORDER BY mjd DESC)\n",
    "\"\"\".format(\"PADDED_TRAINING_SET\")\n",
    "\n",
    "CTE2_df=sqlContext.sql(CTE2_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in memory table\n",
    "CTE2_df.registerTempTable(\"CTE2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So in here we include the calculated fields for the metadata as well.\n",
    "\n",
    "We do need to add modifier values based on minimum and maximum flux values for each observed object - this is the metaVal field from the MODIFIERS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metadata we need for the feature vectors\n",
    "meta_sql=\"\"\"\n",
    "        select meta.object_id, gal_l, gal_b, ddf, hostgal_specz, hostgal_photoz, hostgal_photoz_err, mwebv,target,\n",
    "        case when hostgal_photoz > 0 \n",
    "            then 1  -- CAST(1 AS BOOLEAN)\n",
    "            else 0 --CAST(0 AS BOOLEAN)\n",
    "            end as photoz_positive,\n",
    "        --6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99\n",
    "        case \n",
    "            when target= 6 then 0\n",
    "            when target= 15 then 1\n",
    "            when target= 16 then 2\n",
    "            when target= 42 then 3\n",
    "            when target= 52 then 4\n",
    "            when target= 53 then 5\n",
    "            when target= 62 then 6\n",
    "            when target= 64 then 7\n",
    "            when target= 65 then 8\n",
    "            when target= 67 then 9\n",
    "            when target= 88 then 10\n",
    "            when target= 90 then 11\n",
    "            when target= 92 then 12\n",
    "            when target= 95 then 13\n",
    "            when target= 99 then 14\n",
    "            else 14\n",
    "            end mapped_target,\n",
    "            metaVal\n",
    "\n",
    "        from {} meta\n",
    "            INNER JOIN {} mods\n",
    "                ON meta.object_id = mods.object_id\n",
    "\n",
    "\"\"\".format(\"TRAINING_SET_METADATA\", \"MODIFIERS\")\n",
    "\n",
    "meta_df=sqlContext.sql(meta_sql)\n",
    "meta_df.registerTempTable(\"meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the intermediate table\n",
    "\n",
    "This table sets up the arrays for the object metadata, spaced out to ten elements and create the key value pairs for the mjd, passband, flux etc arrays that will be created in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_sql=\"\"\"\n",
    "        select CTE1.object_id,mapped_target as target,\n",
    "        array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,photoz_positive,metaVal) as meta,\n",
    "        double(hostgal_specz) as specz,\n",
    "        MAP(\n",
    "            'interval', mjdInt,\n",
    "            'deltaMjd', deltaMjd,\n",
    "            'passband',passband,\n",
    "            'rval', rval,\n",
    "            'flux',flux,\n",
    "            'flux_err',flux_err,\n",
    "            'detected',detected,\n",
    "            'received_wavelength', case \n",
    "                                    when CTE1.passband = 0 then 357/1000\n",
    "                                    when CTE1.passband = 1 then 477/1000\n",
    "                                    when CTE1.passband = 2 then 621/1000\n",
    "                                    when CTE1.passband = 3 then 754/1000\n",
    "                                    when CTE1.passband = 4 then 871/1000\n",
    "                                    else  1004/1000\n",
    "                                    end,\n",
    "            'source_wavelength', case \n",
    "                                    when CTE1.passband = 0 then 357 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 1 then 477 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 2 then 621 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 3 then 754 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    when CTE1.passband = 4 then 871 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (meta.hostgal_photoz + 1)/1000\n",
    "                                    end\n",
    "        \n",
    "        ) AS kv\n",
    "        from CTE1 \n",
    "            inner join CTE2\n",
    "                on CTE1.object_id=CTE2.object_id\n",
    "                and CTE1. rownum=CTE2.rownum\n",
    "            inner join meta\n",
    "                on CTE1.object_id = meta.object_id\n",
    "\"\"\"\n",
    "\n",
    "struct_df=sqlContext.sql(struct_sql)\n",
    "struct_df.registerTempTable(\"struct\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "        received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "        source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "        \n",
    "                    'received_frequency', case \n",
    "                                    when CTE1.passband = 0 then 300000 /357\n",
    "                                    when CTE1.passband = 1 then 300000 /477\n",
    "                                    when CTE1.passband = 2 then 300000 /621\n",
    "                                    when CTE1.passband = 3 then 300000 /754\n",
    "                                    when CTE1.passband = 4 then 300000 /871\n",
    "                                    else  300000 / 1004\n",
    "                                    end,\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CTE1_df.unpersist()\n",
    "CTE2_df.unpersist()\n",
    "meta_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full training set of feature vectors, padded to 256 elements.\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,target,specz,\n",
    "array( collect_list(a.kv['passband']) ) as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'interval',             array( collect_list(a.kv['interval']) ),\n",
    "    'deltaMjd',             array( collect_list(a.kv['deltaMjd']) ),\n",
    "    'rval',                 array( collect_list(a.kv['rval']) ),\n",
    "    'flux',                 array( collect_list(a.kv['flux']) ),\n",
    "    'flux_err',             array( collect_list(a.kv['flux_err']) ),\n",
    "    'detected',             array( collect_list(a.kv['detected']) ),\n",
    "    'source_wavelength',    array( collect_list(a.kv['source_wavelength']) ),\n",
    "    'received_wavelength',  array( collect_list(a.kv['received_wavelength']) )\n",
    "    )\n",
    ") as hist\n",
    "from struct a\n",
    "group by object_id, meta,target,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the next well illustrates the simplified structure, for ease of creating assembled feature vectors for Elephas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,target,specz,\n",
    "collect_list(a.kv['passband'])  as band,\n",
    "collect_list(a.kv['interval']) as interval,\n",
    "collect_list(a.kv['deltaMjd']) as deltaMjd,\n",
    "collect_list(a.kv['rval']) as rval,\n",
    "collect_list(a.kv['flux']) as flux,\n",
    "collect_list(a.kv['flux_err']) as flux_err,\n",
    "collect_list(a.kv['detected']) as detected,\n",
    "collect_list(a.kv['source_wavelength']) as source_wavelength,\n",
    "collect_list(a.kv['received_wavelength']) as received_wavelength\n",
    "\n",
    "from struct a\n",
    "group by object_id, meta,target,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This on is an experiment for the Keras Elepha pipeline, using the custom model from the plasticc_rnn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,target,specz,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'interval',             collect_list(float(a.kv['interval'])) ,\n",
    "    'deltaMjd',             collect_list(float(a.kv['deltaMjd'])) ,\n",
    "    'rval',                 collect_list(float(a.kv['rval'])) ,\n",
    "    'flux',                 collect_list(float(a.kv['flux'])) ,\n",
    "    'flux_err',             collect_list(float(a.kv['flux_err'])) ,\n",
    "    'detected',             collect_list(int(a.kv['detected'])) ,\n",
    "    'source_wavelength',    collect_list(float(a.kv['source_wavelength'])) ,\n",
    "    'received_wavelength',  collect_list(float(a.kv['received_wavelength'])) \n",
    "    )\n",
    ") as hist\n",
    "from struct a\n",
    "group by object_id, meta,target,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql)#.cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "struct_df.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- target: integer (nullable = false)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- hist: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- interval: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- deltaMjd: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- rval: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- source_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- received_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "ObjectHashAggregate(keys=[object_id#25, meta#129, target#128, specz#130], functions=[collect_list(cast(kv#131[passband] as int), 0, 0), collect_list(cast(kv#131[interval] as float), 0, 0), collect_list(cast(kv#131[deltaMjd] as float), 0, 0), collect_list(cast(kv#131[rval] as float), 0, 0), collect_list(cast(kv#131[flux] as float), 0, 0), collect_list(cast(kv#131[flux_err] as float), 0, 0), collect_list(cast(kv#131[detected] as int), 0, 0), collect_list(cast(kv#131[source_wavelength] as float), 0, 0), collect_list(cast(kv#131[received_wavelength] as float), 0, 0)])\n",
      "+- ObjectHashAggregate(keys=[object_id#25, meta#129, target#128, specz#130], functions=[partial_collect_list(cast(kv#131[passband] as int), 0, 0), partial_collect_list(cast(kv#131[interval] as float), 0, 0), partial_collect_list(cast(kv#131[deltaMjd] as float), 0, 0), partial_collect_list(cast(kv#131[rval] as float), 0, 0), partial_collect_list(cast(kv#131[flux] as float), 0, 0), partial_collect_list(cast(kv#131[flux_err] as float), 0, 0), partial_collect_list(cast(kv#131[detected] as int), 0, 0), partial_collect_list(cast(kv#131[source_wavelength] as float), 0, 0), partial_collect_list(cast(kv#131[received_wavelength] as float), 0, 0)])\n",
      "   +- *(36) Project [object_id#25, mapped_target#115 AS target#128, array(0.0, 0.0, 0.0, 0.0, cast(ddf#49 as double), hostgal_specz#50, hostgal_photoz#51, mwebv#54, cast(photoz_positive#114 as double), metaVal#70) AS meta#129, hostgal_specz#50 AS specz#130, map(interval, mjdInt#87, deltaMjd, deltaMjd#88, passband, cast(passband#8 as double), rval, rval#106, flux, flux#89, flux_err, flux_err#90, detected, cast(detected#11 as double), received_wavelength, CASE WHEN (passband#8 = 0) THEN 0.357 WHEN (passband#8 = 1) THEN 0.477 WHEN (passband#8 = 2) THEN 0.621 WHEN (passband#8 = 3) THEN 0.754 WHEN (passband#8 = 4) THEN 0.871 ELSE 1.004 END, source_wavelength, CASE WHEN (passband#8 = 0) THEN ((357.0 / (hostgal_photoz#51 + 1.0)) / 1000.0) WHEN (passband#8 = 1) THEN ((477.0 / (hostgal_photoz#51 + 1.0)) / 1000.0) WHEN (passband#8 = 2) THEN ((621.0 / (hostgal_photoz#51 + 1.0)) / 1000.0) WHEN (passband#8 = 3) THEN ((754.0 / (hostgal_photoz#51 + 1.0)) / 1000.0) WHEN (passband#8 = 4) THEN ((871.0 / (hostgal_photoz#51 + 1.0)) / 1000.0) ELSE ((1004.0 / (hostgal_photoz#51 + 1.0)) / 1000.0) END) AS kv#131]\n",
      "      +- *(36) SortMergeJoin [object_id#25], [object_id#44], Inner\n",
      "         :- *(31) Sort [object_id#25 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(object_id#25, 200)\n",
      "         :     +- *(30) Project [object_id#25, mjdInt#87, deltaMjd#88, passband#8, flux#89, flux_err#90, detected#11, rval#106]\n",
      "         :        +- *(30) SortMergeJoin [object_id#25, rownum#91], [object_id#132, rownum#107], Inner\n",
      "         :           :- *(16) Sort [object_id#25 ASC NULLS FIRST, rownum#91 ASC NULLS FIRST], false, 0\n",
      "         :           :  +- Exchange hashpartitioning(object_id#25, rownum#91, 200)\n",
      "         :           :     +- *(15) Project [object_id#25, (mjd#7 - _we0#94) AS mjdInt#87, CASE WHEN isnull(_we1#95) THEN 0.0 ELSE (mjd#7 - _we2#96) END AS deltaMjd#88, passband#8, flux#89, flux_err#90, detected#11, rownum#91]\n",
      "         :           :        +- *(15) Filter isnotnull(rownum#91)\n",
      "         :           :           +- Window [first(mjd#7, false) windowspecdefinition(object_id#25, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#94, lag(mjd#7, 1, null) windowspecdefinition(object_id#25, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we1#95, lag(mjd#7, 1, null) windowspecdefinition(object_id#25, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we2#96, row_number() windowspecdefinition(object_id#25, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#91], [object_id#25], [mjd#7 ASC NULLS FIRST]\n",
      "         :           :              +- *(14) Sort [object_id#25 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST], false, 0\n",
      "         :           :                 +- *(14) Project [object_id#25, mjd#7, passband#8, (flux#9 / HistModifier#69) AS flux#89, (flux_err#10 / HistModifier#69) AS flux_err#90, detected#11]\n",
      "         :           :                    +- *(14) SortMergeJoin [object_id#25], [object_id#71], Inner\n",
      "         :           :                       :- *(11) Sort [object_id#25 ASC NULLS FIRST], false, 0\n",
      "         :           :                       :  +- Exchange hashpartitioning(object_id#25, 200)\n",
      "         :           :                       :     +- *(10) Sort [object_id#25 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST], true, 0\n",
      "         :           :                       :        +- Exchange rangepartitioning(object_id#25 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST, 200)\n",
      "         :           :                       :           +- *(9) Sort [object_id#25 ASC NULLS FIRST, mjd#7 DESC NULLS LAST], true, 0\n",
      "         :           :                       :              +- Exchange rangepartitioning(object_id#25 ASC NULLS FIRST, mjd#7 DESC NULLS LAST, 200)\n",
      "         :           :                       :                 +- *(8) Project [object_id#25, CASE WHEN isnull(mjd#33) THEN cast(padMJD#1 as double) ELSE mjd#33 END AS mjd#7, CASE WHEN isnull(passband#34) THEN padPassband#2 ELSE cast(passband#34 as int) END AS passband#8, CASE WHEN isnull(flux#35) THEN cast(padFlux#3 as double) ELSE mjd#33 END AS flux#9, CASE WHEN isnull(flux_err#36) THEN cast(padFlux_err#4 as double) ELSE flux_err#36 END AS flux_err#10, CASE WHEN isnull(detected#37) THEN padDetected#5 ELSE cast(detected#37 as int) END AS detected#11]\n",
      "         :           :                       :                    +- SortMergeJoin [object_id#25, rownum#0], [object_id#32, rownum#6], LeftOuter\n",
      "         :           :                       :                       :- *(5) Sort [object_id#25 ASC NULLS FIRST, rownum#0 ASC NULLS FIRST], false, 0\n",
      "         :           :                       :                       :  +- Exchange hashpartitioning(object_id#25, rownum#0, 200)\n",
      "         :           :                       :                       :     +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "         :           :                       :                       :        :- *(2) HashAggregate(keys=[object_id#25], functions=[])\n",
      "         :           :                       :                       :        :  +- Exchange hashpartitioning(object_id#25, 200)\n",
      "         :           :                       :                       :        :     +- *(1) HashAggregate(keys=[object_id#25], functions=[])\n",
      "         :           :                       :                       :        :        +- *(1) Filter isnotnull(object_id#25)\n",
      "         :           :                       :                       :        :           +- HiveTableScan [object_id#25], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#25, mjd#26, passband#27, flux#28, flux_err#29, detected#30]\n",
      "         :           :                       :                       :        +- BroadcastExchange IdentityBroadcastMode\n",
      "         :           :                       :                       :           +- *(4) Project [rownum#0]\n",
      "         :           :                       :                       :              +- *(4) Filter (isnotnull(rownum#0) && (rownum#0 <= 256))\n",
      "         :           :                       :                       :                 +- Window [row_number() windowspecdefinition(object_id#13 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#0], [object_id#13 ASC NULLS FIRST]\n",
      "         :           :                       :                       :                    +- *(3) Sort [object_id#13 ASC NULLS FIRST], false, 0\n",
      "         :           :                       :                       :                       +- Exchange SinglePartition\n",
      "         :           :                       :                       :                          +- HiveTableScan [object_id#13], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#13, ra#14, decl#15, gal_l#16, gal_b#17, ddf#18, hostgal_specz#19, hostgal_photoz#20, hostgal_photoz_err#21, distmod#22, mwebv#23, target#24]\n",
      "         :           :                       :                       +- *(7) Sort [object_id#32 ASC NULLS FIRST, rownum#6 ASC NULLS FIRST], false, 0\n",
      "         :           :                       :                          +- Exchange hashpartitioning(object_id#32, rownum#6, 200)\n",
      "         :           :                       :                             +- Window [row_number() windowspecdefinition(object_id#32, mjd#33 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#6], [object_id#32], [mjd#33 DESC NULLS LAST]\n",
      "         :           :                       :                                +- *(6) Sort [object_id#32 ASC NULLS FIRST, mjd#33 DESC NULLS LAST], false, 0\n",
      "         :           :                       :                                   +- Exchange hashpartitioning(object_id#32, 200)\n",
      "         :           :                       :                                      +- HiveTableScan [object_id#32, mjd#33, passband#34, flux#35, flux_err#36, detected#37], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#32, mjd#33, passband#34, flux#35, flux_err#36, detected#37]\n",
      "         :           :                       +- *(13) Sort [object_id#71 ASC NULLS FIRST], false, 0\n",
      "         :           :                          +- *(13) HashAggregate(keys=[object_id#71], functions=[max(flux#74), min(flux#74)])\n",
      "         :           :                             +- Exchange hashpartitioning(object_id#71, 200)\n",
      "         :           :                                +- *(12) HashAggregate(keys=[object_id#71], functions=[partial_max(flux#74), partial_min(flux#74)])\n",
      "         :           :                                   +- *(12) Filter isnotnull(object_id#71)\n",
      "         :           :                                      +- HiveTableScan [object_id#71, flux#74], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#71, mjd#72, passband#73, flux#74, flux_err#75, detected#76]\n",
      "         :           +- *(29) Sort [object_id#132 ASC NULLS FIRST, rownum#107 ASC NULLS FIRST], false, 0\n",
      "         :              +- Exchange hashpartitioning(object_id#132, rownum#107, 200)\n",
      "         :                 +- *(28) Project [object_id#132, (_we0#110 - mjd#7) AS rval#106, rownum#107]\n",
      "         :                    +- *(28) Filter isnotnull(rownum#107)\n",
      "         :                       +- Window [first(mjd#7, false) windowspecdefinition(object_id#132, mjd#7 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#110, row_number() windowspecdefinition(object_id#132, mjd#7 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#107], [object_id#132], [mjd#7 DESC NULLS LAST]\n",
      "         :                          +- *(27) Sort [object_id#132 ASC NULLS FIRST, mjd#7 DESC NULLS LAST], false, 0\n",
      "         :                             +- Exchange hashpartitioning(object_id#132, 200)\n",
      "         :                                +- *(26) Sort [object_id#132 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST], true, 0\n",
      "         :                                   +- Exchange rangepartitioning(object_id#132 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST, 200)\n",
      "         :                                      +- *(25) Sort [object_id#132 ASC NULLS FIRST, mjd#7 DESC NULLS LAST], true, 0\n",
      "         :                                         +- Exchange rangepartitioning(object_id#132 ASC NULLS FIRST, mjd#7 DESC NULLS LAST, 200)\n",
      "         :                                            +- *(24) Project [object_id#132, CASE WHEN isnull(mjd#33) THEN cast(padMJD#1 as double) ELSE mjd#33 END AS mjd#7]\n",
      "         :                                               +- SortMergeJoin [object_id#132, rownum#0], [object_id#32, rownum#6], LeftOuter\n",
      "         :                                                  :- *(21) Sort [object_id#132 ASC NULLS FIRST, rownum#0 ASC NULLS FIRST], false, 0\n",
      "         :                                                  :  +- Exchange hashpartitioning(object_id#132, rownum#0, 200)\n",
      "         :                                                  :     +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "         :                                                  :        :- *(18) HashAggregate(keys=[object_id#132], functions=[])\n",
      "         :                                                  :        :  +- ReusedExchange [object_id#132], Exchange hashpartitioning(object_id#25, 200)\n",
      "         :                                                  :        +- ReusedExchange [rownum#0], BroadcastExchange IdentityBroadcastMode\n",
      "         :                                                  +- *(23) Sort [object_id#32 ASC NULLS FIRST, rownum#6 ASC NULLS FIRST], false, 0\n",
      "         :                                                     +- Exchange hashpartitioning(object_id#32, rownum#6, 200)\n",
      "         :                                                        +- Window [row_number() windowspecdefinition(object_id#32, mjd#33 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#6], [object_id#32], [mjd#33 DESC NULLS LAST]\n",
      "         :                                                           +- *(22) Sort [object_id#32 ASC NULLS FIRST, mjd#33 DESC NULLS LAST], false, 0\n",
      "         :                                                              +- Exchange hashpartitioning(object_id#32, 200)\n",
      "         :                                                                 +- HiveTableScan [object_id#32, mjd#33], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#32, mjd#33, passband#34, flux#35, flux_err#36, detected#37]\n",
      "         +- *(35) Sort [object_id#44 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(object_id#44, 200)\n",
      "               +- *(34) Project [object_id#44, ddf#49, hostgal_specz#50, hostgal_photoz#51, mwebv#54, CASE WHEN (hostgal_photoz#51 > 0.0) THEN 1 ELSE 0 END AS photoz_positive#114, CASE WHEN (target#55 = 6) THEN 0 WHEN (target#55 = 15) THEN 1 WHEN (target#55 = 16) THEN 2 WHEN (target#55 = 42) THEN 3 WHEN (target#55 = 52) THEN 4 WHEN (target#55 = 53) THEN 5 WHEN (target#55 = 62) THEN 6 WHEN (target#55 = 64) THEN 7 WHEN (target#55 = 65) THEN 8 WHEN (target#55 = 67) THEN 9 WHEN (target#55 = 88) THEN 10 WHEN (target#55 = 90) THEN 11 WHEN (target#55 = 92) THEN 12 WHEN (target#55 = 95) THEN 13 WHEN (target#55 = 99) THEN 14 ELSE 14 END AS mapped_target#115, metaVal#70]\n",
      "                  +- *(34) BroadcastHashJoin [object_id#44], [object_id#71], Inner, BuildLeft\n",
      "                     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                     :  +- *(32) Filter isnotnull(object_id#44)\n",
      "                     :     +- HiveTableScan [object_id#44, ddf#49, hostgal_specz#50, hostgal_photoz#51, mwebv#54, target#55], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#44, ra#45, decl#46, gal_l#47, gal_b#48, ddf#49, hostgal_specz#50, hostgal_photoz#51, hostgal_photoz_err#52, distmod#53, mwebv#54, target#55]\n",
      "                     +- *(34) HashAggregate(keys=[object_id#71], functions=[max(flux#74), min(flux#74)])\n",
      "                        +- ReusedExchange [object_id#71, max#184, min#185], Exchange hashpartitioning(object_id#71, 200)\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the feature vector table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='overwrite'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "TABLE='training_set_vectors' #- original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_elephas'\n",
    "\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "me=sqlContext.sql(\"select * from training_set_flat_augmented_vectors\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "me.explain()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
