{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Elephas specific training and test sets\n",
    "\n",
    "This notebook demonstrates the creation and augmentation of the training set table, as well as the creation of the test set vector table.\n",
    "\n",
    "These tables have the following features -\n",
    "* Calculated fields\n",
    "* Augmented (training set only)\n",
    "* feature arrays are unpadded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the test vector set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testVectorsDF=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select ts.object_id,\n",
    "        0 target,\n",
    "       array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "       double(hostgal_specz) as specz,\n",
    "       MAP(\n",
    "            'mjd', 0,\n",
    "            'passband',passband,\n",
    "            'flux',flux / mods.HistModifier,\n",
    "            'flux_err',flux_err / mods.HistModifier,\n",
    "            'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'detected',0,\n",
    "            'source_wavelength', case \n",
    "                                    when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    end,\n",
    "            'received_wavelength', 0\n",
    "    \n",
    "       ) AS kv\n",
    "    from test_set_compressed ts \n",
    "        inner join test_set_metadata tsm \n",
    "            on ts.object_id = tsm.object_id \n",
    "        inner join (\n",
    "            select\n",
    "            object_id,\n",
    "            log2(max(flux)- min(flux)) flux_pow,\n",
    "            pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "            log2(max(flux)- min(flux))/10 as metaVal\n",
    "            from test_set_compressed\n",
    "            group by object_id\n",
    "    \n",
    "        ) mods\n",
    "        on ts.object_id = mods.object_id\n",
    "    WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    ") \n",
    "select object_id, target,meta,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "collect_list(float(a.kv['mjd'])) as mjd,\n",
    "collect_list(float(a.kv['flux'])) as flux,\n",
    "collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "collect_list(int(a.kv['detected'])) as detected,\n",
    "collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "from rawData a\n",
    "group by object_id, target,meta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='overwrite'\n",
    "FORMAT='parquet'\n",
    "TABLE='elephas_test_set' \n",
    "\n",
    "testVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the base training set table\n",
    "\n",
    "Note that the creation statement is identical to the SQL statemen to create the full test set; the only difference is that we are pulling the feature data from the initial training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select ts.object_id,\n",
    "        case \n",
    "                when target= 6 then 0\n",
    "                when target= 15 then 1\n",
    "                when target= 16 then 2\n",
    "                when target= 42 then 3\n",
    "                when target= 52 then 4\n",
    "                when target= 53 then 5\n",
    "                when target= 62 then 6\n",
    "                when target= 64 then 7\n",
    "                when target= 65 then 8\n",
    "                when target= 67 then 9\n",
    "                when target= 88 then 10\n",
    "                when target= 90 then 11\n",
    "                when target= 92 then 12\n",
    "                when target= 95 then 13\n",
    "                when target= 99 then 14\n",
    "                else 14\n",
    "                end target,\n",
    "       array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "       double(hostgal_specz) as specz,\n",
    "       MAP(\n",
    "            'mjd', 0,\n",
    "            'passband',passband,\n",
    "            'flux',flux / mods.HistModifier,\n",
    "            'flux_err',flux_err / mods.HistModifier,\n",
    "            'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'detected',0,\n",
    "            'source_wavelength', case \n",
    "                                    when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    end,\n",
    "            'received_wavelength', 0\n",
    "    \n",
    "       ) AS kv\n",
    "    from training_set ts \n",
    "        inner join training_set_metadata tsm \n",
    "            on ts.object_id = tsm.object_id \n",
    "        inner join (\n",
    "            select\n",
    "            object_id,\n",
    "            log2(max(flux)- min(flux)) flux_pow,\n",
    "            pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "            log2(max(flux)- min(flux))/10 as metaVal\n",
    "            from training_set\n",
    "            group by object_id\n",
    "    \n",
    "        ) mods\n",
    "        on ts.object_id = mods.object_id\n",
    "    WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    ") \n",
    "select object_id, target,meta,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "collect_list(float(a.kv['mjd'])) as mjd,\n",
    "collect_list(float(a.kv['flux'])) as flux,\n",
    "collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "collect_list(int(a.kv['detected'])) as detected,\n",
    "collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "from rawData a\n",
    "group by object_id, target,meta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='overwrite'\n",
    "FORMAT='parquet'\n",
    "TABLE='elephas_training_set' \n",
    "\n",
    "trainingVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the training set\n",
    "\n",
    "This procedure utilises Spark temporary tables, which are Spark dataframes registered in memory during a Pyspark session; they can then be accessed in data selection queries when generating further dataframes. Refer here\n",
    "\n",
    "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f\n",
    "\n",
    "Note that in this example, a simple loop is used to augment the training set to the required number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "130779836\n",
      "finished!\n",
      "1\n",
      "130779836\n",
      "finished!\n",
      "2\n",
      "130779836\n",
      "finished!\n",
      "3\n",
      "130779836\n",
      "finished!\n",
      "4\n",
      "130779836\n",
      "finished!\n",
      "5\n",
      "130779836\n",
      "finished!\n",
      "6\n",
      "130779836\n",
      "finished!\n",
      "7\n",
      "130779836\n",
      "finished!\n",
      "8\n",
      "130779836\n",
      "finished!\n",
      "9\n",
      "130779836\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    ##\n",
    "    ## First, we create the augmented metadata table\n",
    "    ##\n",
    "\n",
    "    sql= \"\"\"\n",
    "    with NEW_Metadata as (\n",
    "    select object_id,\n",
    "    ra,decl,gal_l,gal_b,ddf,hostgal_specz,\n",
    "    rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5) hostgal_photoz,\n",
    "    hostgal_photoz_err, distmod,mwebv,target\n",
    "    from training_set_metadata\n",
    "    )\n",
    "    select object_id, ra,decl,gal_l,gal_b,ddf,hostgal_specz,hostgal_photoz,hostgal_photoz_err,distmod,mwebv,target\n",
    "    from NEW_Metadata    \n",
    "    \"\"\"\n",
    "    \n",
    "    newMetadataDF=sqlContext.sql(sql)\n",
    "    newMetadataDF.registerTempTable(\"AUGMENTED_METADATA\")\n",
    "    \n",
    "    ## \n",
    "    ## Next, the augmented training set\n",
    "    ##\n",
    "    \n",
    "    sql=\"\"\"\n",
    "    with New_Training_set as(\n",
    "        select ts.object_id, mjd, passband,flux,\n",
    "        rand()*((flux+flux_err)-((flux-flux_err)/1.5))+((flux-flux_err)/1.5)  newFlux,\n",
    "        flux_err,detected,\n",
    "        (1+hostgal_photoz)/( 1+ (rand()*((hostgal_photoz+hostgal_photoz_err)-((hostgal_photoz-hostgal_photoz_err)/1.5))+((hostgal_photoz-hostgal_photoz_err)/1.5))) dt\n",
    "        from training_set ts\n",
    "            inner join training_set_metadata tsm\n",
    "                on ts.object_id = tsm.object_id\n",
    "    )\n",
    "    select object_id,\n",
    "    mjd*dt as mjd,  passband, newFlux as flux, flux_err, detected\n",
    "    from New_Training_set    \n",
    "    \"\"\"\n",
    "    \n",
    "    newTrainingSetDF=sqlContext.sql(sql)\n",
    "    newTrainingSetDF.registerTempTable(\"AUGMENTED_TRAINING_SET\")\n",
    "    \n",
    "    ##\n",
    "    ## Now that we have the augmented metadata and training data, we create the augmented data in the same manner \n",
    "    ## that we created the initial training set, as well as the test set vectors.\n",
    "    ##\n",
    "    \n",
    "    sql=\"\"\"\n",
    "    with rawData as\n",
    "    (\n",
    "        select ts.object_id,\n",
    "            case \n",
    "                    when target= 6 then 0\n",
    "                    when target= 15 then 1\n",
    "                    when target= 16 then 2\n",
    "                    when target= 42 then 3\n",
    "                    when target= 52 then 4\n",
    "                    when target= 53 then 5\n",
    "                    when target= 62 then 6\n",
    "                    when target= 64 then 7\n",
    "                    when target= 65 then 8\n",
    "                    when target= 67 then 9\n",
    "                    when target= 88 then 10\n",
    "                    when target= 90 then 11\n",
    "                    when target= 92 then 12\n",
    "                    when target= 95 then 13\n",
    "                    when target= 99 then 14\n",
    "                    else 14\n",
    "                    end target,\n",
    "           array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "           double(hostgal_specz) as specz,\n",
    "           MAP(\n",
    "                'mjd', 0,\n",
    "                'passband',passband,\n",
    "                'flux',flux / mods.HistModifier,\n",
    "                'flux_err',flux_err / mods.HistModifier,\n",
    "                'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "                'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "                'detected',0,\n",
    "                'source_wavelength', case \n",
    "                                        when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                        end,\n",
    "                'received_wavelength', 0\n",
    "\n",
    "           ) AS kv\n",
    "        from {} ts \n",
    "            inner join {} tsm \n",
    "                on ts.object_id = tsm.object_id \n",
    "            inner join (\n",
    "                select\n",
    "                object_id,\n",
    "                log2(max(flux)- min(flux)) flux_pow,\n",
    "                pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "                log2(max(flux)- min(flux))/10 as metaVal\n",
    "                from training_set\n",
    "                group by object_id\n",
    "\n",
    "            ) mods\n",
    "            on ts.object_id = mods.object_id\n",
    "        WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    "    ) \n",
    "    select object_id, target,meta,\n",
    "    collect_list(int(a.kv['passband']))as band,\n",
    "    collect_list(float(a.kv['mjd'])) as mjd,\n",
    "    collect_list(float(a.kv['flux'])) as flux,\n",
    "    collect_list(float(a.kv['flux_err'])) as flux_err,\n",
    "    collect_list(int(a.kv['detected'])) as detected,\n",
    "    collect_list(float(a.kv['fwd_int'])) as fwd_int,\n",
    "    collect_list(float(a.kv['bwd_int'])) as bwd_int,\n",
    "    collect_list(float(a.kv['source_wavelength'])) as source_wavelength,\n",
    "    collect_list(float(a.kv['received_wavelength'])) as received_wavelength\n",
    "    from rawData a\n",
    "    group by object_id, target,meta    \n",
    "    \"\"\".format(\"AUGMENTED_TRAINING_SET\", \"AUGMENTED_METADATA\")\n",
    "    \n",
    "    ##\n",
    "    ## Points to note:\n",
    "    ## the .format() directive of the SQL statement above illustrates the use of the in memory registered Spark\n",
    "    ## tables that we created above.\n",
    "    ##\n",
    "    \n",
    "    trainingVectorsDF=sqlContext.sql(sql)\n",
    "    trainingVectorsDF.count()\n",
    "    MODE='append'\n",
    "    FORMAT='parquet'\n",
    "    TABLE='elephas_training_set'\n",
    "    trainingVectorsDF.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n",
    "    \n",
    "    print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196200"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF=sqlContext.sql(\"select * from elephas_training_set\")\n",
    "testDF.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
