{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third model test notebook.\n",
    "\n",
    "* unpadded\n",
    "* not augmented\n",
    "* metadata table incorporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 25\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 2 #1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data(list_x, list_y = None):\n",
    "    X = {}\n",
    "    for k in list_x[0].keys():\n",
    "\n",
    "        list = [x[k] for x in list_x]\n",
    "        X[k] = np.concatenate(list)\n",
    "\n",
    "    if list_y is None:\n",
    "        return X\n",
    "    else:\n",
    "        return X, np.concatenate(list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df):\n",
    "    #x=np.array(raw_vectorsDF.select('target').collect())\n",
    "    \n",
    "    all_y = np.array(df.select('target').collect(), dtype = 'int32')\n",
    "\n",
    "    y_count = np.unique(all_y, return_counts=True)[1]\n",
    "\n",
    "    wtable = np.ones(len(classes))\n",
    "\n",
    "    for i in range(0, y_count.shape[0]):\n",
    "        wtable[i] = y_count[i] / all_y.shape[0]\n",
    "\n",
    "    return wtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(itemslist):\n",
    "\n",
    "    keys = itemslist[0].keys()\n",
    "    print('creating X')\n",
    "    X = {\n",
    "            'id': np.array([i['id'] for i in itemslist], dtype='int32'),\n",
    "            'meta': np.array([i['meta'] for i in itemslist]),\n",
    "            'band': pad_sequences([i['band'] for i in itemslist], maxlen=sequence_len, dtype='int32'),\n",
    "            'hist': pad_sequences([i['hist'] for i in itemslist], maxlen=sequence_len, dtype='float32'),\n",
    "        }\n",
    "    print('creating Y')\n",
    "    Y = to_categorical([i['target'] for i in itemslist], num_classes=len(classes))\n",
    "\n",
    "    X['hist'][:,:,0] = 0 # remove abs time\n",
    "#    X['hist'][:,:,1] = 0 # remove flux\n",
    "#    X['hist'][:,:,2] = 0 # remove flux err\n",
    "    X['hist'][:,:,3] = 0 # remove detected flag\n",
    "#    X['hist'][:,:,4] = 0 # remove fwd intervals\n",
    "#    X['hist'][:,:,5] = 0 # remove bwd intervals\n",
    "#    X['hist'][:,:,6] = 0 # remove source wavelength\n",
    "    X['hist'][:,:,7] = 0 # remove received wavelength\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_intervals(sample):\n",
    "\n",
    "    hist = sample['hist']\n",
    "    band = sample['band']\n",
    "\n",
    "    hist[:,4] = np.ediff1d(hist[:,0], to_begin = [0])\n",
    "    hist[:,5] = np.ediff1d(hist[:,0], to_end = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_sample(s, augmentate=True):\n",
    "    c = copy.deepcopy(s)\n",
    "\n",
    "    if not augmentate:\n",
    "        return c\n",
    "\n",
    "    band = []\n",
    "    hist = []\n",
    "\n",
    "    drop_rate = 0.3\n",
    "\n",
    "    # drop some records\n",
    "    for k in range(s['band'].shape[0]):\n",
    "        if random.uniform(0, 1) >= drop_rate:\n",
    "            band.append(s['band'][k])\n",
    "            hist.append(s['hist'][k])\n",
    "\n",
    "    c['hist'] = np.array(hist, dtype='float32')\n",
    "    c['band'] = np.array(band, dtype='int32')\n",
    "\n",
    "    set_intervals(c)\n",
    "            \n",
    "    new_z = random.normalvariate(c['meta'][5], c['meta'][6] / 1.5) # hostgal_photoz and hostgal_photoz_err\n",
    "    new_z = max(new_z, 0)\n",
    "    new_z = min(new_z, 5)\n",
    "\n",
    "    dt = (1 + c['meta'][5]) / (1 + new_z) # hostgal_photoz\n",
    "    c['meta'][5] = new_z\n",
    "\n",
    "    # augmentation for flux\n",
    "    c['hist'][:,1] = np.random.normal(c['hist'][:,1], c['hist'][:,2] / 1.5) # flux and flux_err\n",
    "\n",
    "    # multiply time intervals and wavelength to apply augmentation for red shift\n",
    "    c['hist'][:,0] *= dt\n",
    "    c['hist'][:,4] *= dt\n",
    "    c['hist'][:,5] *= dt\n",
    "    c['hist'][:,6] *= dt\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_counts(samples, wtable, augmentate):\n",
    "    maxpr = np.max(wtable)\n",
    "    counts = maxpr / wtable\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    for s in samples:\n",
    "\n",
    "        index += 1\n",
    "        print('Normalizing {0}/{1}   '.format(index, len(samples)), end='\\r')\n",
    "\n",
    "        res.append(s)\n",
    "        count = int(3 * counts[s['target']]) - 1\n",
    "\n",
    "        for i in range(0, count):\n",
    "            res.append(copy_sample(s, augmentate))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentate(samples, gl_count, exgl_count):\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    for s in samples:\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "        if index % 1000 == 0:\n",
    "            print('Augmenting {0}/{1}   '.format(index, len(samples)), end='\\r')\n",
    "\n",
    "        count = gl_count if (s['meta'][8] == 0) else exgl_count\n",
    "\n",
    "        for i in range(0, count):\n",
    "            res.append(copy_sample(s))\n",
    "\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(raw_vectors_df, extragalactic=None, use_specz=False):\n",
    "\n",
    "    samples = []\n",
    "    list_objects = map(lambda row: row.asDict(), raw_vectorsDF.collect())\n",
    "    object_vectors = {object['object_id']: object for object in list_objects}\n",
    "\n",
    "\n",
    "    for key in object_vectors.keys():\n",
    "\n",
    "        i=object_vectors.get(key)\n",
    "\n",
    "        id=i.get('object_id')\n",
    "\n",
    "        sample = {}\n",
    "        sample['id'] = int(id)\n",
    "\n",
    "        # 'object_id', 'target', 'meta', 'specz', 'band', 'hist'\n",
    "        sample['target'] = np.where(classes == int(i.get('target')))[0][0] # positional index of the classes array\n",
    "\n",
    "        meta=np.array(i.get('meta'), dtype='float32')\n",
    "\n",
    "        sample['meta'] = np.zeros(10, dtype = 'float32')\n",
    "\n",
    "            #sample['meta'][4] = meta['ddf']\t\t\t\t\tfrom meta column array meta[0]\n",
    "            #sample['meta'][5] = meta['hostgal_photoz']\t\t\tfrom meta column array meta[2]\n",
    "            #sample['meta'][6] = meta['hostgal_photoz_err']\t\tfrom meta column array meta[3]\n",
    "            #sample['meta'][7] = meta['mwebv']\t\t\t\t\tfrom meta column array meta[4]\n",
    "            #sample['meta'][8] = float(meta['hostgal_photoz']) > 0  returns True or false\n",
    "\n",
    "            #sample['specz'] = float(meta['hostgal_specz'])\t\tfrom meta column array meta[1]\n",
    "\n",
    "\n",
    "        sample['meta'][4] = meta[0]\n",
    "        sample['meta'][5] = meta[2]\n",
    "        sample['meta'][6] = meta[3]\n",
    "        sample['meta'][7] = meta[4]\n",
    "        sample['meta'][8] = float(meta[2]) > 0\n",
    "\n",
    "        sample['specz'] = float(meta[1])    \n",
    "\n",
    "        if use_specz:\n",
    "            sample['meta'][5] = float(meta['hostgal_specz'])\n",
    "            sample['meta'][6] = 0.0\n",
    "\n",
    "        z = float(sample['meta'][5])\n",
    "        \n",
    "        ## Note! refer note regarding the table creation in the notebook \"ThirdModelTest.ipynb\"\n",
    "        ## as the table using in this example does not cast a collect_list to an array.\n",
    "        ## Therefore, a reshape is not necessary!\n",
    "\n",
    "        j=i.get('hist')\n",
    "        mjd=np.array(j[0][0], dtype='float32')\n",
    "        #r,c=mjd.shape\n",
    "        \n",
    "        #mjd.reshape(c,)\n",
    "       \n",
    "        band=np.array(i.get('band') , dtype='int32')\n",
    "        #band=np.array(  j[0][1], dtype='int32') #.reshape(c,) # passband\n",
    "        flux=np.array( j[0][1], dtype='float32') #.reshape(c,) # flux\n",
    "        flux_err=np.array( j[0][2], dtype='float32') #.reshape(c,) # flux_err\n",
    "        detected=np.array( j[0][3], dtype='int32') #.reshape(c,) # Detected\n",
    "\n",
    "        mjd -= mjd[0]\n",
    "        mjd /= 100 # Earth time shift in day*100\n",
    "        mjd /= (z + 1) # Object time shift in day*100\n",
    "\n",
    "\n",
    "        received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "        received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "        source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "\n",
    "\n",
    "        sample['band'] = band + 1\n",
    "\n",
    "        sample['hist'] = np.zeros((flux.shape[0], 8), dtype='float32')\n",
    "        sample['hist'][:,0] = mjd\n",
    "        sample['hist'][:,1] = flux\n",
    "        sample['hist'][:,2] = flux_err\n",
    "        sample['hist'][:,3] = detected\n",
    "\n",
    "        sample['hist'][:,6] = (source_wavelength/1000)\n",
    "        sample['hist'][:,7] = (received_wavelength/1000)\n",
    "\n",
    "        set_intervals(sample)\n",
    "\n",
    "        flux_max = np.max(flux)\n",
    "        flux_min = np.min(flux)\n",
    "        flux_pow = math.log2(flux_max - flux_min)\n",
    "        sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "        sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "        sample['meta'][9] = flux_pow / 10\n",
    "\n",
    "        samples.append(sample)\n",
    "\n",
    "        if len(samples) % 1000 == 0:\n",
    "            print('Converting data {0}'.format(len(samples)), end='\\r')\n",
    "\n",
    "        if len(samples) >= limit:\n",
    "            break\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mywloss(y_true,y_pred):\n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def multi_weighted_logloss(y_ohe, y_p, wtable):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    nb_pos = wtable\n",
    "\n",
    "    if nb_pos[-1] == 0:\n",
    "        nb_pos[-1] = 1\n",
    "\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss / y_ohe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, Y, size=80):\n",
    "\n",
    "    hist_input = Input(shape=X['hist'][0].shape, name='hist')\n",
    "    meta_input = Input(shape=X['meta'][0].shape, name='meta')\n",
    "    band_input = Input(shape=X['band'][0].shape, name='band')\n",
    "\n",
    "    band_emb = Embedding(8, 8)(band_input)\n",
    "\n",
    "    hist = concatenate([hist_input, band_emb])\n",
    "    hist = TimeDistributed(Dense(40, activation='relu'))(hist)\n",
    "\n",
    "    rnn = Bidirectional(GRU(size, return_sequences=True))(hist)\n",
    "    rnn = SpatialDropout1D(0.5)(rnn)\n",
    "\n",
    "    gmp = GlobalMaxPool1D()(rnn)\n",
    "    gmp = Dropout(0.5)(gmp)\n",
    "\n",
    "    x = concatenate([meta_input, gmp])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    output = Dense(15, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[hist_input, meta_input, band_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(i, samples_train, samples_valid):\n",
    "    start_augment=time.time()\n",
    "    start_augmentCpu=time.clock()\n",
    "    \n",
    "    samples_train += augmentate(samples_train, augment_count, augment_count)\n",
    "    \n",
    "    elapsed_augment=time.time() - start_augment\n",
    "    elapsed_augmentCpu=time.clock() - start_augmentCpu\n",
    "\n",
    "    patience = 1000000 // len(samples_train) + 5\n",
    "\n",
    "    start_trainingVectors=time.time()\n",
    "    start_trainingVectorsCpu=time.clock()\n",
    "\n",
    "    train_x, train_y = get_keras_data(samples_train)\n",
    "\n",
    "    elapsed_training_Vectors=time.time() - start_trainingVectors\n",
    "    elapsed_training_VectorsCpu=time.clock() - start_trainingVectorsCpu\n",
    "\n",
    "    print(len(samples_train))\n",
    "    \n",
    "    del samples_train\n",
    "    \n",
    "    start_validationVectors=time.time()\n",
    "    start_validationVectorsCpu=time.clock()\n",
    "\n",
    "    valid_x, valid_y = get_keras_data(samples_valid)\n",
    "    del samples_valid\n",
    "    \n",
    "    elapsed_validation_Vectors=time.time() - start_validationVectors\n",
    "    elapsed_validation_VectorsCpu=time.clock() - start_validationVectorsCpu\n",
    "    \n",
    "    \n",
    "    model = get_model(train_x, train_y)\n",
    "\n",
    "    if i == 1: model.summary()\n",
    "    model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    print('Training model {0} of {1}, Patience: {2}'.format(i, num_models, patience))\n",
    "    filename = 'model_{0:03d}.hdf5'.format(i)\n",
    "    callbacks = [EarlyStopping(patience=patience, verbose=1), ModelCheckpoint(filename, save_best_only=True)]\n",
    "\n",
    "    model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=max_epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "\n",
    "    model = load_model(filename, custom_objects={'mywloss': mywloss})\n",
    "\n",
    "    preds = model.predict(valid_x, batch_size=batch_size2)\n",
    "    loss = multi_weighted_logloss(valid_y, preds, wtable)\n",
    "    acc = accuracy_score(np.argmax(valid_y, axis=1), np.argmax(preds,axis=1))\n",
    "    print('MW Loss: {0:.4f}, Accuracy: {1:.4f}'.format(loss, acc))\n",
    "    \n",
    "    return  elapsed_augment,elapsed_augmentCpu,\\\n",
    "            elapsed_training_Vectors,elapsed_training_VectorsCpu,\\\n",
    "            elapsed_validation_Vectors,elapsed_validation_VectorsCpu, \\\n",
    "            train_x, train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from hive...\n"
     ]
    }
   ],
   "source": [
    "print('Loading train data from hive...')\n",
    "\n",
    "start_train=time.time()\n",
    "start_trainCpu=time.clock()\n",
    "\n",
    "raw_vectorsDF=sqlContext.sql(\"select * from training_raw_vectors_unpadded_no_calcs\")\n",
    "\n",
    "elapsed_train=time.time()-start_train\n",
    "elapsed_trainCpu=time.clock()-start_trainCpu\n",
    "wtable = get_wtable(raw_vectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- meta: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- hist: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- mjd: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_vectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data 7000\n"
     ]
    }
   ],
   "source": [
    "start_samples=time.time()\n",
    "start_samplesC=time.clock()\n",
    "    \n",
    "samples =  get_data(raw_vectorsDF, \\\n",
    "                    extragalactic=None, use_specz=use_specz)\n",
    "elapsed_samples=time.time() - start_samples\n",
    "elapsed_samplesC=time.clock() - start_samplesC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting 7000/7063   \n",
      "creating X\n",
      "creating Y\n",
      "183638\n",
      "creating X\n",
      "creating Y\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "band (InputLayer)               (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hist (InputLayer)               (None, 256, 8)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 256, 8)       64          band[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 16)      0           hist[0][0]                       \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 256, 40)      680         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 256, 160)     58080       time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 256, 160)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 160)          0           spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "meta (InputLayer)               (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 160)          0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 170)          0           meta[0][0]                       \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          21888       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          16512       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 15)           1935        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 99,159\n",
      "Trainable params: 99,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training model 1 of 1, Patience: 10\n",
      "Train on 183638 samples, validate on 785 samples\n",
      "Epoch 1/2\n",
      " - 1365s - loss: 1.5377 - acc: 0.3146 - val_loss: 1.0070 - val_acc: 0.5197\n",
      "Epoch 2/2\n",
      " - 1361s - loss: 1.0370 - acc: 0.4699 - val_loss: 0.8645 - val_acc: 0.5427\n",
      "MW Loss: 0.8422, Accuracy: 0.5427\n",
      "2888.1229293346405\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, num_models+1):\n",
    "\n",
    "    samples_train, samples_valid = train_test_split(samples, test_size=valid_size, random_state=42*i)\n",
    "    len(samples_train)\n",
    "    \n",
    "    start_train=time.time()\n",
    "    elapsed_augment,elapsed_augmentCpu,\\\n",
    "            elapsed_training_Vectors,elapsed_training_VectorsCpu,\\\n",
    "            elapsed_validation_Vectors,elapsed_validation_VectorsCpu, \\\n",
    "            train_x, train_y = \\\n",
    "            train_model(i, samples_train, samples_valid)\n",
    "    elapsed_train=time.time()-start_train\n",
    "    print(elapsed_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
