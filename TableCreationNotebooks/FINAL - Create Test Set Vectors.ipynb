{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full test vector dataframe using Spark temp tables\n",
    "\n",
    "## First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '8g'),\\\n",
    "                           ('spark.driver.memory', '8g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "#--conf spark.io.compression.codec=snappy\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # manual garbag collection to stop leaks on Collect() gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 35\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to this great article, SQL at Scale with Apache Spark SQL and DataFrames\n",
    "\n",
    "https://towardsdatascience.com/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f\n",
    "\n",
    "#### Benchmark snippets that may be usefull\n",
    "https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# set up hive session for broadcast joins\n",
    "optSql=\"\"\"\n",
    "set hive.cbo.enable=true;\n",
    "set hive.auto.convert.join=true;\n",
    "set hive.auto.convert.join.noconditionaltask=true;\n",
    "set hive.auto.convert.join.noconditionaltask.size=20971520\n",
    "set hive.auto.convert.join.use.nonstaged=true;\n",
    "set hive.mapjoin.smalltable.filesize = 30000000; \n",
    "\"\"\"\n",
    "sqlContext.sql(optSql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the padded training set\n",
    "We do this to create a standard set of features for all objects; in this case, the value provided by sequence_len (which in this case is 256).\n",
    "\n",
    "We're also going to use Spark registered tables to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = \"test_set_compressed\"\n",
    "training_metadata = \"test_set_metadata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the SQL equivalent of the keras PAD_SEQUENCES function. \n",
    "\n",
    "We create a baseline table consisting of object_ids and a 0 value for every feature as well as a rownumber. This is accomplished by a cartesian join (cross join in Hive) between the cnt nested table and the objects nesed table, resulting the baseline nested table which has 256 records for each object, with all features set to zero.\n",
    "\n",
    "Next, we create a train_set nested table containing all the training set information, ordered by the mjd value descending. this pads the data from the last value to the first as per PAD_SEQUENCES,\n",
    "\n",
    "We then create the padded training set as a left join from the baseline nested table to the train_set nested table, padded to a consistent 236 values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedSQL=\"\"\"\n",
    "with\n",
    "cnt\n",
    "as\n",
    "(\n",
    "    select rownum from \n",
    "    (\n",
    "        select row_number() over (ORDER BY object_id) as rownum\n",
    "        from {}\n",
    "    ) a\n",
    "    where rownum <=256\n",
    "),\n",
    "objects as (select object_id, 0 padMJD, 0 padPassband,0 padFlux, 0 padFlux_err,0 padDetected from {} group by object_id),\n",
    "baseline as (select * from objects CROSS JOIN cnt ), -- cartesian product with 256 values to use as the baseline\n",
    "train_set as (select *, row_number() over (partition by object_id order by mjd desc) as rownum from {}),\n",
    "paddedRev as (\n",
    "    select baseline.object_id, --train_set.mjd, baseline.padMJD,\n",
    "    case when train_set.mjd is null then baseline.padMJD else train_set.mjd end mjd,\n",
    "    case when train_set.passband is null then baseline.padPassband else train_set.passband end passband,\n",
    "    case when train_set.flux is null then baseline.padFlux else train_set.mjd end flux,\n",
    "    case when train_set.flux_err is null then baseline.padFlux_err else train_set.flux_err end flux_err,\n",
    "    case when train_set.detected is null then baseline.padDetected else train_set.detected end detected\n",
    "    from baseline left outer join train_set on baseline.object_id = train_set.object_id and baseline.rownum=train_set.rownum\n",
    "    order by baseline.object_id, mjd desc\n",
    ")\n",
    "select object_id, mjd, passband, flux, flux_err, detected from paddedRev order by object_id, mjd\n",
    "\"\"\".format(training_metadata,training_set,training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the padded training data dataframe and create a Spark temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedTrainingSet_DF = sqlContext.sql(paddedSQL)\n",
    "paddedTrainingSet_DF.registerTempTable(\"PADDED_TRAINING_SET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training set metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataSQL=\"\"\"select * from {}\"\"\".format(training_metadata)\n",
    "metadata_DF = sqlContext.sql(metadataSQL)\n",
    "metadata_DF.registerTempTable(\"TRAINING_SET_METADATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, now we have the padded training set and the training set metadata\n",
    "\n",
    "#### Create the training set padded feature vectors dataframe and instantiate it as a hive table.\n",
    "\n",
    "Points to note.\n",
    "\n",
    "- The following SQL could all be inciorporated into one statement, but in the interests of clarity, we have broken it down into relevent component parts\n",
    "- Calculating the MJD intervals utilises SQL WINDOW functionality. This needs to be used carefully, because wondow functionality will cause SPark to perform a hash sort which is a very expensive operation and on larger datasets can cause a spill to disk which is to be avoided if at all possible. \n",
    "- Two separate tables need to be utilised for these sorts, because Hive doesn not support a WINDOW statement on the same field (mjd) with different ORDER BY clauses in the OVER (PARTITION BY ... ORDER BY ...) WINDOW. Hence, we have CTE1 and CTE2 tables for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the basic data from the padded training_set we created earlier, \"PADDED_TRAINING_SET\"\n",
    "CTE1_sql=\"\"\"\n",
    "        select object_id, mjd,\n",
    "        mjd - first_value(mjd) over w as mjdInt,\n",
    "        case when lag(mjd) OVER w is null then\n",
    "            0\n",
    "        else\n",
    "            mjd - lag(mjd) over w \n",
    "        end as deltaMjd,\n",
    "        passband,\n",
    "        flux,\n",
    "        flux_err,\n",
    "        detected,\n",
    "        row_number() OVER w as rownum\n",
    "        from {}\n",
    "        WINDOW w AS (PARTITION BY object_id ORDER BY mjd)\n",
    "\"\"\".format(\"PADDED_TRAINING_SET\")\n",
    "\n",
    "CTE1_df=sqlContext.sql(CTE1_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in memory table\n",
    "CTE1_df.registerTempTable(\"CTE1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTE2_sql=\"\"\"\n",
    "        select object_id,\n",
    "        first_value(mjd) OVER x - mjd as rval,\n",
    "        row_number() OVER x as rownum\n",
    "        from {}\n",
    "        WINDOW x AS (PARTITION BY object_id ORDER BY mjd DESC)\n",
    "\"\"\".format(\"PADDED_TRAINING_SET\")\n",
    "\n",
    "CTE2_df=sqlContext.sql(CTE2_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in memory table\n",
    "CTE2_df.registerTempTable(\"CTE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metadata we need for the feature vectors\n",
    "meta_sql=\"\"\"\n",
    "        select object_id, gal_l, gal_b, ddf, hostgal_specz, hostgal_photoz, hostgal_photoz_err, mwebv,\n",
    "        case when hostgal_photoz > 0 \n",
    "            then 1  -- CAST(1 AS BOOLEAN)\n",
    "            else 0 --CAST(0 AS BOOLEAN)\n",
    "            end as photoz_positive\n",
    "        from {}\n",
    "\"\"\".format(\"TRAINING_SET_METADATA\")\n",
    "\n",
    "meta_df=sqlContext.sql(meta_sql)\n",
    "meta_df.registerTempTable(\"meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the intermediate table\n",
    "\n",
    "This table sets up the arrays for the object metadata, spaced out to ten elements and create the key value pairs for the mjd, passband, flux etc arrays that will be created in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_sql=\"\"\"\n",
    "        select CTE1.object_id,\n",
    "        array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,photoz_positive,0) as meta,\n",
    "        double(hostgal_specz) as specz,\n",
    "        MAP(\n",
    "            'interval', mjdInt,\n",
    "            'deltaMjd', deltaMjd,\n",
    "            'passband',passband,\n",
    "            'rval', rval,\n",
    "            'flux',flux,\n",
    "            'flux_err',flux_err,\n",
    "            'detected',detected,\n",
    "            'received_wavelength', case \n",
    "                                    when CTE1.passband = 0 then 357\n",
    "                                    when CTE1.passband = 1 then 477\n",
    "                                    when CTE1.passband = 2 then 621\n",
    "                                    when CTE1.passband = 3 then 754\n",
    "                                    when CTE1.passband = 4 then 871\n",
    "                                    else  1004\n",
    "                                    end,\n",
    "            'source_wavelength', case \n",
    "                                    when CTE1.passband = 0 then 357 / (meta.hostgal_photoz + 1)\n",
    "                                    when CTE1.passband = 1 then 477 / (meta.hostgal_photoz + 1)\n",
    "                                    when CTE1.passband = 2 then 621 / (meta.hostgal_photoz + 1)\n",
    "                                    when CTE1.passband = 3 then 754 / (meta.hostgal_photoz + 1)\n",
    "                                    when CTE1.passband = 4 then 871 / (meta.hostgal_photoz + 1)\n",
    "                                    else 1004 / (meta.hostgal_photoz + 1)\n",
    "                                    end\n",
    "        \n",
    "        ) AS kv\n",
    "        from CTE1 \n",
    "            inner join CTE2\n",
    "                on CTE1.object_id=CTE2.object_id\n",
    "                and CTE1. rownum=CTE2.rownum\n",
    "            inner join meta\n",
    "                on CTE1.object_id = meta.object_id\n",
    "\"\"\"\n",
    "\n",
    "struct_df=sqlContext.sql(struct_sql)\n",
    "struct_df.registerTempTable(\"struct\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "        received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "        source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "        \n",
    "                    'received_frequency', case \n",
    "                                    when CTE1.passband = 0 then 300000 /357\n",
    "                                    when CTE1.passband = 1 then 300000 /477\n",
    "                                    when CTE1.passband = 2 then 300000 /621\n",
    "                                    when CTE1.passband = 3 then 300000 /754\n",
    "                                    when CTE1.passband = 4 then 300000 /871\n",
    "                                    else  300000 / 1004\n",
    "                                    end,\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CTE1_df.unpersist()\n",
    "CTE2_df.unpersist()\n",
    "meta_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full training set of feature vectors, padded to 256 elements.\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,target,specz,\n",
    "array( collect_list(a.kv['passband']) ) as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'interval',             array( collect_list(a.kv['interval']) ),\n",
    "    'deltaMjd',             array( collect_list(a.kv['deltaMjd']) ),\n",
    "    'rval',                 array( collect_list(a.kv['rval']) ),\n",
    "    'flux',                 array( collect_list(a.kv['flux']) ),\n",
    "    'flux_err',             array( collect_list(a.kv['flux_err']) ),\n",
    "    'detected',             array( collect_list(a.kv['detected']) ),\n",
    "    'source_wavelength',    array( collect_list(a.kv['source_wavelength']) ),\n",
    "    'received_wavelength',  array( collect_list(a.kv['received_wavelength']) )\n",
    "    )\n",
    ") as hist\n",
    "from struct a\n",
    "group by object_id, meta,target,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the next well illustrates the simplified structure, for ease of creating assembled feature vectors for Elephas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,target,specz,\n",
    "collect_list(a.kv['passband'])  as band,\n",
    "collect_list(a.kv['interval']) as interval,\n",
    "collect_list(a.kv['deltaMjd']) as deltaMjd,\n",
    "collect_list(a.kv['rval']) as rval,\n",
    "collect_list(a.kv['flux']) as flux,\n",
    "collect_list(a.kv['flux_err']) as flux_err,\n",
    "collect_list(a.kv['detected']) as detected,\n",
    "collect_list(a.kv['source_wavelength']) as source_wavelength,\n",
    "collect_list(a.kv['received_wavelength']) as received_wavelength\n",
    "\n",
    "from struct a\n",
    "group by object_id, meta,target,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This on is an experiment for the Keras Elepha pipeline, using the custom model from the plasticc_rnn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,meta,specz,\n",
    "array( collect_list(a.kv['passband']) ) as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'interval',             collect_list(a.kv['interval']) ,\n",
    "    'deltaMjd',             collect_list(a.kv['deltaMjd']) ,\n",
    "    'rval',                 collect_list(a.kv['rval']) ,\n",
    "    'flux',                 collect_list(a.kv['flux']) ,\n",
    "    'flux_err',             collect_list(a.kv['flux_err']) ,\n",
    "    'detected',             collect_list(a.kv['detected']) ,\n",
    "    'source_wavelength',    collect_list(a.kv['source_wavelength']) ,\n",
    "    'received_wavelength',  collect_list(a.kv['received_wavelength']) \n",
    "    )\n",
    ") as hist\n",
    "from struct a\n",
    "group by object_id, meta,specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "struct_df.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = false)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hist: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- interval: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- deltaMjd: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- rval: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- source_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- received_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [object_id#24, meta#103, specz#104, band#116, hist#117]\n",
      "   +- InMemoryRelation [object_id#24, meta#103, specz#104, band#116, hist#117], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- ObjectHashAggregate(keys=[object_id#24, meta#103, specz#104], functions=[collect_list(kv#105[passband], 0, 0), collect_list(kv#105[interval], 0, 0), collect_list(kv#105[deltaMjd], 0, 0), collect_list(kv#105[rval], 0, 0), collect_list(kv#105[flux], 0, 0), collect_list(kv#105[flux_err], 0, 0), collect_list(kv#105[detected], 0, 0), collect_list(kv#105[source_wavelength], 0, 0), collect_list(kv#105[received_wavelength], 0, 0)])\n",
      "            +- ObjectHashAggregate(keys=[object_id#24, meta#103, specz#104], functions=[partial_collect_list(kv#105[passband], 0, 0), partial_collect_list(kv#105[interval], 0, 0), partial_collect_list(kv#105[deltaMjd], 0, 0), partial_collect_list(kv#105[rval], 0, 0), partial_collect_list(kv#105[flux], 0, 0), partial_collect_list(kv#105[flux_err], 0, 0), partial_collect_list(kv#105[detected], 0, 0), partial_collect_list(kv#105[source_wavelength], 0, 0), partial_collect_list(kv#105[received_wavelength], 0, 0)])\n",
      "               +- *(31) Project [object_id#24, array(0.0, 0.0, 0.0, 0.0, cast(ddf#48 as double), hostgal_specz#49, hostgal_photoz#50, mwebv#53, cast(photoz_positive#93 as double), 0.0) AS meta#103, hostgal_specz#49 AS specz#104, map(interval, mjdInt#65, deltaMjd, deltaMjd#66, passband, cast(passband#8 as double), rval, rval#82, flux, flux#9, flux_err, flux_err#10, detected, cast(detected#11 as double), received_wavelength, cast(CASE WHEN (passband#8 = 0) THEN 357 WHEN (passband#8 = 1) THEN 477 WHEN (passband#8 = 2) THEN 621 WHEN (passband#8 = 3) THEN 754 WHEN (passband#8 = 4) THEN 871 ELSE 1004 END as double), source_wavelength, CASE WHEN (passband#8 = 0) THEN (357.0 / (hostgal_photoz#50 + 1.0)) WHEN (passband#8 = 1) THEN (477.0 / (hostgal_photoz#50 + 1.0)) WHEN (passband#8 = 2) THEN (621.0 / (hostgal_photoz#50 + 1.0)) WHEN (passband#8 = 3) THEN (754.0 / (hostgal_photoz#50 + 1.0)) WHEN (passband#8 = 4) THEN (871.0 / (hostgal_photoz#50 + 1.0)) ELSE (1004.0 / (hostgal_photoz#50 + 1.0)) END) AS kv#105]\n",
      "                  +- *(31) SortMergeJoin [object_id#24], [object_id#43], Inner\n",
      "                     :- *(28) Sort [object_id#24 ASC NULLS FIRST], false, 0\n",
      "                     :  +- Exchange hashpartitioning(object_id#24, 200)\n",
      "                     :     +- *(27) Project [object_id#24, mjdInt#65, deltaMjd#66, passband#8, flux#9, flux_err#10, detected#11, rval#82]\n",
      "                     :        +- *(27) SortMergeJoin [object_id#24, rownum#67], [object_id#106, rownum#83], Inner\n",
      "                     :           :- *(13) Sort [object_id#24 ASC NULLS FIRST, rownum#67 ASC NULLS FIRST], false, 0\n",
      "                     :           :  +- Exchange hashpartitioning(object_id#24, rownum#67, 200)\n",
      "                     :           :     +- *(12) Project [object_id#24, (mjd#7 - _we0#70) AS mjdInt#65, CASE WHEN isnull(_we1#71) THEN 0.0 ELSE (mjd#7 - _we2#72) END AS deltaMjd#66, passband#8, flux#9, flux_err#10, detected#11, rownum#67]\n",
      "                     :           :        +- *(12) Filter isnotnull(rownum#67)\n",
      "                     :           :           +- Window [first(mjd#7, false) windowspecdefinition(object_id#24, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#70, lag(mjd#7, 1, null) windowspecdefinition(object_id#24, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we1#71, lag(mjd#7, 1, null) windowspecdefinition(object_id#24, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we2#72, row_number() windowspecdefinition(object_id#24, mjd#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#67], [object_id#24], [mjd#7 ASC NULLS FIRST]\n",
      "                     :           :              +- *(11) Sort [object_id#24 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST], false, 0\n",
      "                     :           :                 +- Exchange hashpartitioning(object_id#24, 200)\n",
      "                     :           :                    +- *(10) Sort [object_id#24 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST], true, 0\n",
      "                     :           :                       +- Exchange rangepartitioning(object_id#24 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST, 200)\n",
      "                     :           :                          +- *(9) Sort [object_id#24 ASC NULLS FIRST, mjd#7 DESC NULLS LAST], true, 0\n",
      "                     :           :                             +- Exchange rangepartitioning(object_id#24 ASC NULLS FIRST, mjd#7 DESC NULLS LAST, 200)\n",
      "                     :           :                                +- *(8) Project [object_id#24, CASE WHEN isnull(mjd#32) THEN cast(padMJD#1 as double) ELSE mjd#32 END AS mjd#7, CASE WHEN isnull(passband#33) THEN padPassband#2 ELSE cast(passband#33 as int) END AS passband#8, CASE WHEN isnull(flux#34) THEN cast(padFlux#3 as double) ELSE mjd#32 END AS flux#9, CASE WHEN isnull(flux_err#35) THEN cast(padFlux_err#4 as double) ELSE flux_err#35 END AS flux_err#10, CASE WHEN isnull(detected#36) THEN padDetected#5 ELSE cast(detected#36 as int) END AS detected#11]\n",
      "                     :           :                                   +- SortMergeJoin [object_id#24, rownum#0], [object_id#31, rownum#6], LeftOuter\n",
      "                     :           :                                      :- *(5) Sort [object_id#24 ASC NULLS FIRST, rownum#0 ASC NULLS FIRST], false, 0\n",
      "                     :           :                                      :  +- Exchange hashpartitioning(object_id#24, rownum#0, 200)\n",
      "                     :           :                                      :     +- CartesianProduct\n",
      "                     :           :                                      :        :- *(2) HashAggregate(keys=[object_id#24], functions=[])\n",
      "                     :           :                                      :        :  +- Exchange hashpartitioning(object_id#24, 200)\n",
      "                     :           :                                      :        :     +- *(1) HashAggregate(keys=[object_id#24], functions=[])\n",
      "                     :           :                                      :        :        +- *(1) Filter isnotnull(object_id#24)\n",
      "                     :           :                                      :        :           +- HiveTableScan [object_id#24], HiveTableRelation `plasticc`.`test_set_compressed`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#24, mjd#25, passband#26, flux#27, flux_err#28, detected#29]\n",
      "                     :           :                                      :        +- *(4) Project [rownum#0]\n",
      "                     :           :                                      :           +- *(4) Filter (isnotnull(rownum#0) && (rownum#0 <= 256))\n",
      "                     :           :                                      :              +- Window [row_number() windowspecdefinition(object_id#13 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#0], [object_id#13 ASC NULLS FIRST]\n",
      "                     :           :                                      :                 +- *(3) Sort [object_id#13 ASC NULLS FIRST], false, 0\n",
      "                     :           :                                      :                    +- Exchange SinglePartition\n",
      "                     :           :                                      :                       +- HiveTableScan [object_id#13], HiveTableRelation `plasticc`.`test_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#13, ra#14, decl#15, gal_l#16, gal_b#17, ddf#18, hostgal_specz#19, hostgal_photoz#20, hostgal_photoz_err#21, distmod#22, mwebv#23]\n",
      "                     :           :                                      +- *(7) Sort [object_id#31 ASC NULLS FIRST, rownum#6 ASC NULLS FIRST], false, 0\n",
      "                     :           :                                         +- Exchange hashpartitioning(object_id#31, rownum#6, 200)\n",
      "                     :           :                                            +- Window [row_number() windowspecdefinition(object_id#31, mjd#32 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#6], [object_id#31], [mjd#32 DESC NULLS LAST]\n",
      "                     :           :                                               +- *(6) Sort [object_id#31 ASC NULLS FIRST, mjd#32 DESC NULLS LAST], false, 0\n",
      "                     :           :                                                  +- Exchange hashpartitioning(object_id#31, 200)\n",
      "                     :           :                                                     +- HiveTableScan [object_id#31, mjd#32, passband#33, flux#34, flux_err#35, detected#36], HiveTableRelation `plasticc`.`test_set_compressed`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#31, mjd#32, passband#33, flux#34, flux_err#35, detected#36]\n",
      "                     :           +- *(26) Sort [object_id#106 ASC NULLS FIRST, rownum#83 ASC NULLS FIRST], false, 0\n",
      "                     :              +- Exchange hashpartitioning(object_id#106, rownum#83, 200)\n",
      "                     :                 +- *(25) Project [object_id#106, (_we0#86 - mjd#7) AS rval#82, rownum#83]\n",
      "                     :                    +- *(25) Filter isnotnull(rownum#83)\n",
      "                     :                       +- Window [first(mjd#7, false) windowspecdefinition(object_id#106, mjd#7 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#86, row_number() windowspecdefinition(object_id#106, mjd#7 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#83], [object_id#106], [mjd#7 DESC NULLS LAST]\n",
      "                     :                          +- *(24) Sort [object_id#106 ASC NULLS FIRST, mjd#7 DESC NULLS LAST], false, 0\n",
      "                     :                             +- Exchange hashpartitioning(object_id#106, 200)\n",
      "                     :                                +- *(23) Sort [object_id#106 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST], true, 0\n",
      "                     :                                   +- Exchange rangepartitioning(object_id#106 ASC NULLS FIRST, mjd#7 ASC NULLS FIRST, 200)\n",
      "                     :                                      +- *(22) Sort [object_id#106 ASC NULLS FIRST, mjd#7 DESC NULLS LAST], true, 0\n",
      "                     :                                         +- Exchange rangepartitioning(object_id#106 ASC NULLS FIRST, mjd#7 DESC NULLS LAST, 200)\n",
      "                     :                                            +- *(21) Project [object_id#106, CASE WHEN isnull(mjd#32) THEN cast(padMJD#1 as double) ELSE mjd#32 END AS mjd#7]\n",
      "                     :                                               +- SortMergeJoin [object_id#106, rownum#0], [object_id#31, rownum#6], LeftOuter\n",
      "                     :                                                  :- *(18) Sort [object_id#106 ASC NULLS FIRST, rownum#0 ASC NULLS FIRST], false, 0\n",
      "                     :                                                  :  +- Exchange hashpartitioning(object_id#106, rownum#0, 200)\n",
      "                     :                                                  :     +- CartesianProduct\n",
      "                     :                                                  :        :- *(15) HashAggregate(keys=[object_id#106], functions=[])\n",
      "                     :                                                  :        :  +- ReusedExchange [object_id#106], Exchange hashpartitioning(object_id#24, 200)\n",
      "                     :                                                  :        +- *(17) Project [rownum#0]\n",
      "                     :                                                  :           +- *(17) Filter (isnotnull(rownum#0) && (rownum#0 <= 256))\n",
      "                     :                                                  :              +- Window [row_number() windowspecdefinition(object_id#13 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#0], [object_id#13 ASC NULLS FIRST]\n",
      "                     :                                                  :                 +- *(16) Sort [object_id#13 ASC NULLS FIRST], false, 0\n",
      "                     :                                                  :                    +- ReusedExchange [object_id#13], Exchange SinglePartition\n",
      "                     :                                                  +- *(20) Sort [object_id#31 ASC NULLS FIRST, rownum#6 ASC NULLS FIRST], false, 0\n",
      "                     :                                                     +- Exchange hashpartitioning(object_id#31, rownum#6, 200)\n",
      "                     :                                                        +- Window [row_number() windowspecdefinition(object_id#31, mjd#32 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rownum#6], [object_id#31], [mjd#32 DESC NULLS LAST]\n",
      "                     :                                                           +- *(19) Sort [object_id#31 ASC NULLS FIRST, mjd#32 DESC NULLS LAST], false, 0\n",
      "                     :                                                              +- Exchange hashpartitioning(object_id#31, 200)\n",
      "                     :                                                                 +- HiveTableScan [object_id#31, mjd#32], HiveTableRelation `plasticc`.`test_set_compressed`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#31, mjd#32, passband#33, flux#34, flux_err#35, detected#36]\n",
      "                     +- *(30) Sort [object_id#43 ASC NULLS FIRST], false, 0\n",
      "                        +- Exchange hashpartitioning(object_id#43, 200)\n",
      "                           +- *(29) Project [object_id#43, ddf#48, hostgal_specz#49, hostgal_photoz#50, mwebv#53, CASE WHEN (hostgal_photoz#50 > 0.0) THEN 1 ELSE 0 END AS photoz_positive#93]\n",
      "                              +- *(29) Filter isnotnull(object_id#43)\n",
      "                                 +- HiveTableScan [ddf#48, hostgal_photoz#50, hostgal_specz#49, mwebv#53, object_id#43], HiveTableRelation `plasticc`.`test_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#43, ra#44, decl#45, gal_l#46, gal_b#47, ddf#48, hostgal_specz#49, hostgal_photoz#50, hostgal_photoz_err#51, distmod#52, mwebv#53]\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the feature vector table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "#TABLE='training_set_vectors' - original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_elephas'\n",
    "TABLE='test_set_vectors'\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='orc'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "#TABLE='training_set_vectors' - original full hist ARRAY - STRUCT - ARRAY\n",
    "#TABLE='training_set_elephas'\n",
    "TABLE='test_set_vectors_orc'\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "me.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
