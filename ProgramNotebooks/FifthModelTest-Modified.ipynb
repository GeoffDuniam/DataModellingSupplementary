{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '8g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50)\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 1),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','LightCurve Demo - JupyterHub Elephas implementation')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler,VectorAssembler,OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import udf, col, array, lit\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model # model and load_model from Plasticc\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 25\n",
    "#batch_size = 128\n",
    "#batch_size2 = 512\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000000\n",
    "sequence_len = 256\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df):\n",
    "    \n",
    "    all_y = np.array(df.select('target').collect(), dtype = 'int32') \n",
    "\n",
    "    y_count = np.unique(all_y, return_counts=True)[1]\n",
    "\n",
    "    wtable = np.ones(len(classes))\n",
    "\n",
    "    for i in range(0, y_count.shape[0]):\n",
    "        wtable[i] = y_count[i] / all_y.shape[0]\n",
    "\n",
    "    return wtable    \n",
    "\n",
    "def mywloss(y_true,y_pred):\n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def multi_weighted_logloss(y_ohe, y_p, wtable):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    nb_pos = wtable\n",
    "\n",
    "    if nb_pos[-1] == 0:\n",
    "        nb_pos[-1] = 1\n",
    "\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss / y_ohe.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/backend/permute_dimensions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_model(train_df, input_dim, size=80):\n",
    "    def get_meta(x):\n",
    "        x=x[:,0:10]\n",
    "        return x\n",
    "    \n",
    "    def get_band(x):\n",
    "        x=x[:,10:266]\n",
    "        return x\n",
    "    \n",
    "    def get_hist(x):\n",
    "        # x=x[:,266:input_dim]  -- this is before we passed in the shape from input dim - to avoid two select calls\n",
    "        x=x[:,266:input_dim[0]]\n",
    "        x=Reshape((8,256))(x)\n",
    "        x=K.permute_dimensions(x, (0,2,1))\n",
    "        return x\n",
    "    \n",
    "    #raw_input  = Input(shape=train_df.select(\"features\").first()[0].shape, name='raw')\n",
    "    raw_input  = Input(shape=input_dim, name='raw')\n",
    "    \n",
    "    hist_input = Lambda(get_hist,  name=\"hist\")(raw_input)\n",
    "    meta_input = Lambda(get_meta,  name=\"meta\")(raw_input)\n",
    "    band_input = Lambda(get_band,  name=\"band\")(raw_input)\n",
    "    \n",
    "    band_emb = Embedding(8, 8)(band_input)\n",
    "    hist = concatenate([hist_input, band_emb])\n",
    "    hist = TimeDistributed(Dense(40, activation='relu'))(hist)\n",
    "    \n",
    "    rnn = Bidirectional(GRU(size, return_sequences=True))(hist)\n",
    "    rnn = SpatialDropout1D(0.5)(rnn)\n",
    "    \n",
    "    gmp = GlobalMaxPool1D()(rnn)\n",
    "    gmp = Dropout(0.5)(gmp)\n",
    "    \n",
    "    x = concatenate([meta_input, gmp])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output = Dense(15, activation='softmax')(x)\n",
    "    model = Model(inputs=[raw_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF Functions for vector creation - definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(x, sequence_len=sequence_len):\n",
    "    x = np.pad(x, (sequence_len,0), 'constant', constant_values=(0))\n",
    "    x= x[len(x)-sequence_len:len(x)]\n",
    "    return x\n",
    "\n",
    "def fwd_intervals(x):\n",
    "    x=np.ediff1d(x, to_begin = [0])\n",
    "    return x\n",
    "\n",
    "def bwd_intervals(x):\n",
    "    x=np.ediff1d(x, to_end = [0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues returning numpy arrays to pyspark UDFs.\n",
    "You'll get pickel errors because the funtion returns NumPy types which are not compatible with DataFrame API. You have to cast the function return back to a list, and then caset to Spark comparible data types.\n",
    "\n",
    "https://stackoverflow.com/questions/44965762/is-it-possible-to-store-a-numpy-array-in-a-spark-dataframe-column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF Declarations for vector manipulation\n",
    "\n",
    "Note that we have included the UDF toDenseUDF - this is necessary because the Spark ML class VectorAssembler will cast the assembled vector to a sparse vector if there are a large number of zeros in the vector. This not usually a problem, but for this problem we do need to pass in a static dense vector into the Keras model in order to properly extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categorical = udf(\n",
    "    lambda arr:\n",
    "        [int(i+1 == arr) for i in range(num_classes)], \n",
    "        ArrayType(iType)       \n",
    ")\n",
    "\n",
    "get_padded_float_vectors = udf(\n",
    "    lambda arr: pad_array(arr).tolist(), \n",
    "    ArrayType(fType)\n",
    ")\n",
    "\n",
    "get_padded_int_vectors = udf(\n",
    "    lambda arr: pad_array(arr).tolist(), \n",
    "    ArrayType(iType)\n",
    ")\n",
    "\n",
    "toDenseUdf = udf(\n",
    "    lambda arr: Vectors.dense(arr.toArray()), \n",
    "    VectorUDT()\n",
    ")\n",
    "\n",
    "fwd_udf = udf(\n",
    "    lambda arr: fwd_intervals(arr).tolist(), \n",
    "    ArrayType(fType)\n",
    ")\n",
    "\n",
    "bwd_udf = udf(\n",
    "    lambda arr: bwd_intervals(arr).tolist(), \n",
    "    ArrayType(fType)\n",
    ")\n",
    "\n",
    "to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF=sqlContext.sql(\"\"\"\n",
    "select object_id, target,meta, band, mjd, flux, flux_err, detected, fwd_int, bwd_int, \n",
    "source_wavelength, received_wavelength\n",
    "from elephas_training_set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtable=get_wtable(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training set dataframe with the feature vectors\n",
    "\n",
    "This is how we convert the target to a categegorical, looks like ElephaseSTIMATOE doesn't want it\n",
    "\n",
    "                             to_vector(target_categorical(\"target\")).alias(\"targetV\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF = trainingVectorsDF.select(\\\n",
    "                       \"object_id\",target_categorical(\"target\").alias(\"target\"),\n",
    "                             \"meta\",                           \n",
    "                             get_padded_int_vectors(\"band\").alias(\"band\"),\n",
    "                             get_padded_float_vectors(\"mjd\").alias(\"mjd\"),\n",
    "                             get_padded_float_vectors(\"flux\").alias(\"flux\"),\n",
    "                             get_padded_float_vectors(\"flux_err\").alias(\"flux_err\"),\n",
    "                             get_padded_int_vectors(\"detected\").alias(\"detected\"),\n",
    "                             fwd_udf(get_padded_float_vectors(\"fwd_int\")).alias(\"fwd_int\"),\n",
    "                             bwd_udf(get_padded_float_vectors(\"bwd_int\")).alias(\"bwd_int\"),\n",
    "                             get_padded_float_vectors(\"source_wavelength\").alias(\"source_wavelength\"),\n",
    "                             get_padded_float_vectors(\"received_wavelength\").alias(\"received_wavelength\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(sc,trainingVectorsDF, sequence_len=sequence_len, num_classes=num_classes):\n",
    "\n",
    "    r=trainingVectorsDF.count()\n",
    "    \n",
    "    idArr=np.array(trainingVectorsDF.select('object_id').collect(), dtype='int32').reshape(r,)\n",
    "    #idArr.reshape(r,)\n",
    "    meta_len=10\n",
    "    \n",
    "    metaArr=np.array(trainingVectorsDF.select('meta').collect(), dtype='float32').reshape(r,meta_len)\n",
    "    bandArr= np.array(trainingVectorsDF.select('band').collect() , dtype='int32').reshape(r,sequence_len)\n",
    "\n",
    "    histArray=np.zeros((r,sequence_len,8), dtype='float32') \n",
    "    # this will work brilliantly as get_keras_data sets three columns to zeros anyway\n",
    "    \n",
    "    #mjd=np.array(trainingVectorsDF.select('mjd').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    flux=np.array(trainingVectorsDF.select('flux').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    flux_err=np.array(trainingVectorsDF.select('flux_err').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    #detect=np.array(trainingVectorsDF.select('detected').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    fwd_int=np.array(trainingVectorsDF.select('fwd_int').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    bwd_int=np.array(trainingVectorsDF.select('bwd_int').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    source_wavelength=np.array(trainingVectorsDF.select('source_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    #received_wavelength=np.array(trainingVectorsDF.select('received_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "    \n",
    "    #as per the baseline program, we remove the abs time, detected and receoved_wavelength data\n",
    "\n",
    "    #histArray[:,:,0]=mjd\n",
    "    histArray[:,:,1]=flux\n",
    "    histArray[:,:,2]=flux_err\n",
    "    #histArray[:,:,3]=detect\n",
    "    histArray[:,:,4]=fwd_int\n",
    "    histArray[:,:,5]=bwd_int\n",
    "    histArray[:,:,6]=source_wavelength\n",
    "    #histArray[:,:,7]=received_wavelength\n",
    "\n",
    "    # Create the final vector dictionary\n",
    "    X = {\n",
    "            'id': idArr,\n",
    "            'meta': metaArr,\n",
    "            'band': bandArr,\n",
    "            'hist': histArray\n",
    "        }\n",
    "    # and the encoded target vector\n",
    "    Y = np.array(trainingVectorsDF.select('target').collect(), dtype='int32').reshape(r, num_classes)  \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_keras_data(sc,trainingVectorsDF, sequence_len, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': array([     3910,      4173,     15718, ...,  98720151, 115870585,\n",
       "        119178558], dtype=int32),\n",
       " 'meta': array([[0.       , 0.       , 0.       , ..., 0.009    , 1.       ,\n",
       "         0.7357895],\n",
       "        [0.       , 0.       , 0.       , ..., 0.019    , 1.       ,\n",
       "         0.8051275],\n",
       "        [0.       , 0.       , 0.       , ..., 0.008    , 1.       ,\n",
       "         0.6469907],\n",
       "        ...,\n",
       "        [0.       , 0.       , 0.       , ..., 0.021    , 1.       ,\n",
       "         0.7495351],\n",
       "        [0.       , 0.       , 0.       , ..., 0.046    , 0.       ,\n",
       "         0.8119878],\n",
       "        [0.       , 0.       , 0.       , ..., 0.107    , 1.       ,\n",
       "         0.8051755]], dtype=float32),\n",
       " 'band': array([[0, 1, 0, ..., 2, 0, 3],\n",
       "        [0, 0, 2, ..., 5, 4, 1],\n",
       "        [3, 3, 0, ..., 2, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 4, 5, 5],\n",
       "        [0, 0, 0, ..., 1, 1, 5],\n",
       "        [0, 0, 0, ..., 4, 4, 0]], dtype=int32),\n",
       " 'hist': array([[[ 0.00000000e+00, -8.94139241e-03,  1.49680069e-02, ...,\n",
       "           2.55993652e+00,  9.73529667e-02,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  1.14461230e-02,  1.09050870e-02, ...,\n",
       "           2.52919922e+01,  1.30076662e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  7.85993878e-03,  9.63881426e-03, ...,\n",
       "           3.66795654e+01,  9.73529667e-02,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  1.24113830e-02,  7.99315050e-03, ...,\n",
       "           5.57812500e+01,  1.69345081e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  3.84469144e-03,  8.65007378e-03, ...,\n",
       "           1.72656250e+00,  9.73529667e-02,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -3.08103045e-03,  1.22865094e-02, ...,\n",
       "           0.00000000e+00,  2.05613837e-01,  0.00000000e+00]],\n",
       " \n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  2.13399623e-02,  1.07255829e-02, ...,\n",
       "           8.78344116e+01,  2.35813960e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  2.85199471e-03,  5.92121435e-03, ...,\n",
       "           4.47058868e+00,  4.10197407e-01,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  2.20950413e-02,  3.04256883e-02, ...,\n",
       "           1.77094727e+01,  6.63185477e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  1.05653415e-02,  8.49744678e-03, ...,\n",
       "           1.75691895e+02,  5.75333238e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -2.20121606e-03,  2.99586286e-03, ...,\n",
       "           0.00000000e+00,  3.15079153e-01,  0.00000000e+00]],\n",
       " \n",
       "        [[ 0.00000000e+00, -6.92148833e-03,  1.52135985e-02, ...,\n",
       "           3.74470215e+01,  2.08113179e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  2.52912670e-01,  1.31599056e-02, ...,\n",
       "           1.53264160e+01,  2.08113179e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -1.20726023e-02,  2.58226264e-02, ...,\n",
       "           1.85220947e+01,  9.85363424e-02,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  3.55953842e-01,  1.13952765e-02, ...,\n",
       "           2.33027344e+01,  1.71403557e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  8.63474343e-05,  2.42037531e-02, ...,\n",
       "           2.18012695e+02,  9.85363424e-02,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -1.60694879e-03,  1.86948199e-02, ...,\n",
       "           0.00000000e+00,  9.85363424e-02,  0.00000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  5.71092814e-02,  5.20327091e-02, ...,\n",
       "           1.41069336e+01,  5.58297575e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -1.86265036e-01,  1.17517158e-01, ...,\n",
       "           5.77130127e+00,  6.43548489e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -2.25736111e-01,  1.62886441e-01, ...,\n",
       "           0.00000000e+00,  6.43548489e-01,  0.00000000e+00]],\n",
       " \n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00, -2.53652632e-02,  1.00312056e-02, ...,\n",
       "           8.97094727e-01,  4.76999998e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -2.41302382e-02,  8.50160234e-03, ...,\n",
       "           6.95922852e+00,  4.76999998e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -8.55372101e-02,  2.76059836e-01, ...,\n",
       "           0.00000000e+00,  1.00399995e+00,  0.00000000e+00]],\n",
       " \n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 0.00000000e+00,  9.00525972e-02,  5.34268394e-02, ...,\n",
       "           1.21419678e+01,  6.59748495e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  9.18604620e-03,  5.17987870e-02, ...,\n",
       "           9.85742188e+00,  6.59748495e-01,  0.00000000e+00],\n",
       "         [ 0.00000000e+00, -3.55725479e-03,  4.65668254e-02, ...,\n",
       "           0.00000000e+00,  2.70413578e-01,  0.00000000e+00]]],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=trainingVectorsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaArr=np.array(trainingVectorsDF.select('meta').collect(), dtype='float32').reshape(r,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandArr= np.array(trainingVectorsDF.select('band').collect() , dtype='int32').reshape(r,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histArray=np.zeros((r,sequence_len,8), dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux=np.array(trainingVectorsDF.select('flux').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "flux_err=np.array(trainingVectorsDF.select('flux_err').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "fwd_int=np.array(trainingVectorsDF.select('fwd_int').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "bwd_int=np.array(trainingVectorsDF.select('bwd_int').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "source_wavelength=np.array(trainingVectorsDF.select('source_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histArray[:,:,0]=mjd\n",
    "histArray[:,:,1]=flux\n",
    "histArray[:,:,2]=flux_err\n",
    "#histArray[:,:,3]=detect\n",
    "histArray[:,:,4]=fwd_int\n",
    "histArray[:,:,5]=bwd_int\n",
    "histArray[:,:,6]=source_wavelength\n",
    "#histArray[:,:,7]=received_wavelength\n",
    "\n",
    "# Create the final vector dictionary\n",
    "X = {\n",
    "        'meta': metaArr,\n",
    "        'band': bandArr,\n",
    "        'hist': histArray\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augVec=sqlContext.sql(\"select * from training_set_augmented_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augVec.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=trainingVectorsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandArr= np.array(augVec.select('band').collect()) #, dtype='int32') #.reshape(r,sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandArr.reshape(196200, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_keras_data(sc,trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idArr=np.array(vectors_df.select('object_id').collect(), dtype='int32')\n",
    "idArr.reshape(r,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_len=10\n",
    "start=time.time()\n",
    "metaArr=np.array(trainingVectorsDF.select('meta').collect(), dtype='float32').reshape(r,meta_len)\n",
    "bandArr= np.array(trainingVectorsDF.select('band').collect() , dtype='int32').reshape(r,sequence_len)\n",
    "\n",
    "histArray=np.zeros((r,sequence_len,8), dtype='float32') \n",
    "\n",
    "mjd=np.array(trainingVectorsDF.select('mjd').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "flux=np.array(trainingVectorsDF.select('flux').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "flux_err=np.array(trainingVectorsDF.select('flux_err').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "detect=np.array(trainingVectorsDF.select('detect').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "fwd_int=np.array(trainingVectorsDF.select('fwd_int').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "bwd_int=np.array(trainingVectorsDF.select('bwd_int').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "source_wavelength=np.array(trainingVectorsDF.select('source_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "received_wavelength=np.array(trainingVectorsDF.select('received_wavelength').collect(), dtype='float32').reshape(r,sequence_len)\n",
    "\n",
    "end=time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histArray[:,:,0]=mjd\n",
    "histArray[:,:,1]=flux\n",
    "histArray[:,:,2]=flux_err\n",
    "histArray[:,:,3]=detect\n",
    "histArray[:,:,4]=fwd_int\n",
    "histArray[:,:,5]=bwd_int\n",
    "histArray[:,:,6]=source_wavelength\n",
    "histArray[:,:,7]=received_wavelength\n",
    "\n",
    "# Create the final vector dictionary\n",
    "X = {\n",
    "        'id': idArr,\n",
    "        'meta': metaArr,\n",
    "        'band': bandArr,\n",
    "        'hist': histArray\n",
    "    }\n",
    "# and the encoded target vector\n",
    "Y = np.array(vectors_df.select('target').collect(), dtype='int32')\n",
    "\n",
    "#return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdf=trainingVectorsDF.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=df_pdf[\"target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF = trainingVectorsDF.select(\\\n",
    "                       \"object_id\",\"target\", #target_categorical(\"target\").alias(\"target\"),\n",
    "                             to_vector(\"meta\").alias(\"meta\"),                           \n",
    "                             to_vector(get_padded_int_vectors(\"band\")).alias(\"band\"),\n",
    "                             to_vector(get_padded_float_vectors(\"mjd\")).alias(\"mjd\"),\n",
    "                             to_vector(get_padded_float_vectors(\"flux\")).alias(\"flux\"),\n",
    "                             to_vector(get_padded_float_vectors(\"flux_err\")).alias(\"flux_err\"),\n",
    "                             to_vector(get_padded_int_vectors(\"detected\")).alias(\"detect\"),\n",
    "                             to_vector(fwd_udf(get_padded_float_vectors(\"fwd_int\"))).alias(\"fwd_int\"),\n",
    "                             to_vector(bwd_udf(get_padded_float_vectors(\"bwd_int\"))).alias(\"bwd_int\"),\n",
    "                             to_vector(get_padded_float_vectors(\"source_wavelength\")).alias(\"source_wavelength\"),\n",
    "                             to_vector(get_padded_float_vectors(\"received_wavelength\")).alias(\"received_wavelength\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = ['object_id', 'target']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.6, .3, .1]\n",
    "seed = 42 # seed=0L  validation_df, \n",
    "train_df, test_df, validation_df = trainingVectorsDF.select(\"object_id\",\"target\", toDenseUdf(\"features\").alias(\"features\")).randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.array(train_df.select(\"features\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True , withMean=True)\n",
    "fitted_scaler = scaler.fit(trainingVectorsDF)\n",
    "trainingVectorsDF = fitted_scaler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hist = [\"mjd\", \"flux\", \"flux_err\", \"detect\", \"fwd_int\", \"bwd_int\", \"source_wavelength\", \"received_wavelength\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x in hist],\n",
    "    outputCol='hist')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaled_features = [\"meta\", \"band\", \"hist\"]\n",
    "for x in trainingVectorsDF.columns:\n",
    "    if x in scaled_features:\n",
    "        input_col = x\n",
    "        output_col=\"scaled_\"+x\n",
    "        \n",
    "        scaler = StandardScaler(inputCol=input_col, outputCol=output_col, withStd=True , withMean=True)\n",
    "        fitted_scaler = scaler.fit(trainingVectorsDF)\n",
    "        trainingVectorsDF = fitted_scaler.transform(trainingVectorsDF)\n",
    "        print(output_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaled_features = [\"scaled_meta\", \"scaled_band\", \"scaled_hist\"]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x in scaled_features],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ignore = ['object_id', 'target']\n",
    "for x in trainingVectorsDF.columns:\n",
    "    if x not in ignore:\n",
    "        input_col = x\n",
    "        output_col=\"scaled_\"+x\n",
    "        \n",
    "        scaler = StandardScaler(inputCol=input_col, outputCol=output_col, withStd=True , withMean=True)\n",
    "        fitted_scaler = scaler.fit(trainingVectorsDF)\n",
    "        train_df = fitted_scaler.transform(trainingVectorsDF)\n",
    "        print(output_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training, test and validation splits\n",
    "\n",
    "Note that this is where we apply the toDenseUDF function to ensure that the features vector is a dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = [.7, .3]\n",
    "seed = 42 # seed=0L  validation_df, \n",
    "train_df, test_df = trainingVectorsDF.select( toDenseUdf(\"scaled_features\").alias(\"features\"), \"target\").randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.6, .3, .1]\n",
    "seed = 42 # seed=0L  validation_df, \n",
    "train_df, test_df, validation_df = trainingVectorsDF.select( toDenseUdf(\"features\").alias(\"features\"), \"target\").randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.take(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.repartition(800)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfPartitions=train_df.rdd.getNumPartitions()\n",
    "print(dfPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(classes)\n",
    "#input_dim = len(train_df.select(\"features\").first()[0])\n",
    "input_dim = train_df.select(\"features\").first()[0].shape\n",
    "\n",
    "print(f\"We have {num_classes} classes and {input_dim[0]} features\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Defining Transformers\n",
    "\n",
    "Up until now, we basically just read in raw data. Luckily, ```Spark ML``` has quite a few preprocessing features available, so the only thing we will ever have to do is define transformations of data frames.\n",
    "\n",
    "To proceed, we will first transform category strings to double values. This is done by a so called ```StringIndexer```. Note that we carry out the actual transformation here already, but that is just for demonstration purposes. All we really need is too define ```string_indexer``` to put it into a pipeline later on."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "string_indexer = StringIndexer(inputCol=\"target\", outputCol=\"index_category\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder = OneHotEncoder(inputCol=\"target\", outputCol=\"targetVec\")\n",
    "\n",
    "#pipeline = Pipeline(stages=[encoder])\n",
    "#pipeline.fit(train_df).transform(train_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(train_df, input_dim)\n",
    "print(\"hi there\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n",
    "print(\"hi there\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if i == 1: \n",
    "    model.summary()\n",
    "    print(\"Hi there\")\n",
    "    model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Elephas model\n",
    "\n",
    "To lift the above Keras ```model``` to Spark, we define an ```Estimator``` on top of it. An ```Estimator``` is Spark's incarnation of a model that still has to be trained. It essentially only comes with only a single (required) method, namely ```fit```. Once we call ```fit``` on a data frame, we get back a ```Model```, which is a trained model with a ```transform``` method to predict labels.\n",
    "\n",
    "We do this by initializing an ```ElephasEstimator``` and setting a few properties. As by now our input data frame will have many columns, we have to tell the model where to find features and labels by column name. Then we provide serialized versions of our Keras model. We can not plug in keras models into the ```Estimator``` directly, as Spark will have to serialize them anyway for communication with workers, so it's better to provide the serialization ourselves. In fact, while pyspark knows how to serialize ```model```, it is extremely inefficient and can break if models become too large. Spark ML is especially picky (and rightly so) about parameters and more or less prohibits you from providing non-atomic types and arrays of the latter. Most of the remaining parameters are optional and rather self explainatory. Plus, many of them you know if you have ever run a keras model before. We just include them here to show the full set of training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam=optimizers.nadam(lr=0.01)\n",
    "adam=optimizers.nadam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = 'nadam'\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"features\")             # These two come directly from pyspark,\n",
    "estimator.setLabelCol(\"target\")                 # hence the camel case. Sorry :)\n",
    "estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(80)  # We just use one worker here. Feel free to adapt it.\n",
    "estimator.set_epochs(max_epochs) \n",
    "estimator.set_batch_size(batch_size) # was 128\n",
    "estimator.set_verbosity(2) # was 1\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"asynchronous\") # Was synchronous\n",
    "estimator.set_loss(mywloss) # was(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['accuracy']) ##(['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we set up the pipeline.\n",
    "\n",
    "Looks very similar to SparkFlow, n'est ce pas? This could be an interesting comparison!\n",
    "\n",
    "Defining pipelines is really as easy as listing pipeline stages. We can provide any configuration of Transformers and Estimators really, but here we simply take the three components defined earlier. Note that string_indexer and scaler and interchangable, while estimator somewhat obviously has to come last in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And train the model\n",
    "\n",
    "Note that at this stage, the only method we can call is ''fit''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "fitted_pipeline = pipeline.fit(train_df) # Fit model to data\n",
    "elapsedTime=time.time()-start\n",
    "print(f\"Model trained in {elapsedTime} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "Model_Location=datetime.now().strftime('/user/hduser/RawData/Models/ElephasModel5_' + str(max_epochs) + '-Epochs-' + str(batch_size) + '-BatchSize-%H%M_%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fitted_pipeline.stages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = elephas.ml_model\n",
    "a.__file__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fitted_pipeline.stages[-1].save('HiThere.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we run the transform so we can get the predictions\n",
    "\n",
    "However, this will require modifications to the class file for ElephasEstimator and ElephasTransformer located here\n",
    "\n",
    " ~/.virtualenvs/Elephas/lib/python3.6/site-packages/elephas/ml_model.py\n",
    " \n",
    "because the \\_transform method as written runs the predict process using the model.predict_classes method. A keras Sequential() model class has this method, but the Model() class does not - you have to use the 'predict' method.\n",
    "\n",
    "What this entails is that you have to rewrite that method and overload it to incorporate the predict method if you're training on a keras Model class instead of a Sequential model.\n",
    "\n",
    "The overloaded method is described here - \n",
    "\n",
    "https://github.com/maxpumperla/elephas/issues/111\n",
    "\n",
    "and we will include a copy of the modified class statement in th github\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prediction = fitted_pipeline.transform(train_df,'Model') # Evaluate on train data.\n",
    "prediction = fitted_pipeline.transform(train_df) # <-- The same code evaluates test data.\n",
    "pnl = prediction.select(\"target\", \"prediction\")\n",
    "pnl.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df=validation_df.repartition(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = fitted_pipeline.stages[-1]._transform(validation_df, useModel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl=pred.select(\"target\",\"prediction\")\n",
    "pnl.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like prediction_and_label is a dataframe, not an RDD, so we need to cast it to an RDD in order to use .map\n",
    "prediction_and_label = pnl.rdd.map(lambda row: (row.target, row.prediction))\n",
    "metrics = MulticlassMetrics(prediction_and_label)\n",
    "print(metrics.precision())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.array([0.1118802 , 0.07325512, 0.01614694, 0.02659923, 0.07440449,\n",
    "        0.08224725, 0.06605115, 0.02206622, 0.0542385 , 0.04232709,\n",
    "        0.05887228, 0.0560729 , 0.12828934, 0.10508711, 0.08246218], dtype='float32')\n",
    "x2=np.array([0.11386207, 0.09637289, 0.01490068, 0.02821412, 0.06656378,\n",
    "        0.080683  , 0.07982644, 0.01722281, 0.04762489, 0.03941712,\n",
    "        0.07437494, 0.06128084, 0.08523843, 0.09439979, 0.10001823], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20393/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
