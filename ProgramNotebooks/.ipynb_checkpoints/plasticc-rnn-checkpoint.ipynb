{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 25\n",
    "batch_size = 128\n",
    "batch_size2 = 512\n",
    "#batch_size = 1000\n",
    "#batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 50 #1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data(list_x, list_y = None):\n",
    "    print(\"append_data\")\n",
    "    X = {}\n",
    "    for k in list_x[0].keys():\n",
    "\n",
    "        list = [x[k] for x in list_x]\n",
    "        X[k] = np.concatenate(list)\n",
    "\n",
    "    if list_y is None:\n",
    "        return X\n",
    "    else:\n",
    "        return X, np.concatenate(list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df):\n",
    "    print(\"get_wtable\")\n",
    "    all_y = np.array(df['target'], dtype = 'int32')\n",
    "\n",
    "    y_count = np.unique(all_y, return_counts=True)[1]\n",
    "\n",
    "    wtable = np.ones(len(classes))\n",
    "\n",
    "    for i in range(0, y_count.shape[0]):\n",
    "        wtable[i] = y_count[i] / all_y.shape[0]\n",
    "\n",
    "    return wtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(itemslist): # This function generates the X and Y datasets for the model\n",
    "    print(\"get_keras_data\")\n",
    "    keys = itemslist[0].keys()\n",
    "    X = {\n",
    "            'id': np.array([i['id'] for i in itemslist], dtype='int32'),\n",
    "            'meta': np.array([i['meta'] for i in itemslist]),\n",
    "            'band': pad_sequences([i['band'] for i in itemslist], maxlen=sequence_len, dtype='int32'),\n",
    "            'hist': pad_sequences([i['hist'] for i in itemslist], maxlen=sequence_len, dtype='float32'),\n",
    "        }\n",
    "\n",
    "    Y = to_categorical([i['target'] for i in itemslist], num_classes=len(classes))\n",
    "\n",
    "    X['hist'][:,:,0] = 0 # remove abs time\n",
    "#    X['hist'][:,:,1] = 0 # remove flux\n",
    "#    X['hist'][:,:,2] = 0 # remove flux err\n",
    "    X['hist'][:,:,3] = 0 # remove detected flag\n",
    "#    X['hist'][:,:,4] = 0 # remove fwd intervals\n",
    "#    X['hist'][:,:,5] = 0 # remove bwd intervals\n",
    "#    X['hist'][:,:,6] = 0 # remove source wavelength\n",
    "    X['hist'][:,:,7] = 0 # remove received wavelength\n",
    "    \n",
    "#    print(\"dimensions of the X dictionary\")\n",
    "#    print(X['meta'].shape)\n",
    "#    print(\"Size of the X dictionary in KB\")\n",
    "#    print(sys.getsizeof(X['id'])/10**6)\n",
    "#    print(sys.getsizeof(X['meta'])/10**6)\n",
    "#    print(sys.getsizeof(X['band'])/10**6)\n",
    "#    print(sys.getsizeof(X['hist'])/10**6)\n",
    "#    totalSize=sys.getsizeof(X['id'])/10**6\\\n",
    "#    +sys.getsizeof(X['meta'])/10**6\\\n",
    "#    +sys.getsizeof(X['band'])/10**6\\\n",
    "#    +sys.getsizeof(X['hist'])/10**6\n",
    "#    print(\"total size\")\n",
    "#    print(totalSize)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_intervals(sample):\n",
    "    #print(\"set_intervals\")\n",
    "\n",
    "    hist = sample['hist']\n",
    "    band = sample['band']\n",
    "\n",
    "    hist[:,4] = np.ediff1d(hist[:,0], to_begin = [0])\n",
    "    hist[:,5] = np.ediff1d(hist[:,0], to_end = [0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_sample(s, augmentate=True):\n",
    "    #print(\"copy_samples\")\n",
    "    c = copy.deepcopy(s)\n",
    "\n",
    "    if not augmentate:\n",
    "        return c\n",
    "\n",
    "    band = []\n",
    "    hist = []\n",
    "\n",
    "    drop_rate = 0.3\n",
    "\n",
    "    # drop some records\n",
    "    for k in range(s['band'].shape[0]):\n",
    "        if random.uniform(0, 1) >= drop_rate:\n",
    "            band.append(s['band'][k])\n",
    "            hist.append(s['hist'][k])\n",
    "\n",
    "    c['hist'] = np.array(hist, dtype='float32')\n",
    "    c['band'] = np.array(band, dtype='int32')\n",
    "\n",
    "    set_intervals(c)\n",
    "            \n",
    "    new_z = random.normalvariate(c['meta'][5], c['meta'][6] / 1.5)\n",
    "    new_z = max(new_z, 0)\n",
    "    new_z = min(new_z, 5)\n",
    "\n",
    "    dt = (1 + c['meta'][5]) / (1 + new_z)\n",
    "    c['meta'][5] = new_z\n",
    "\n",
    "    # augmentation for flux\n",
    "    c['hist'][:,1] = np.random.normal(c['hist'][:,1], c['hist'][:,2] / 1.5)\n",
    "\n",
    "    # multiply time intervals and wavelength to apply augmentation for red shift\n",
    "    c['hist'][:,0] *= dt\n",
    "    c['hist'][:,4] *= dt\n",
    "    c['hist'][:,5] *= dt\n",
    "    c['hist'][:,6] *= dt\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_counts(samples, wtable, augmentate):\n",
    "    print(\"normalize_counts\")\n",
    "    maxpr = np.max(wtable)\n",
    "    counts = maxpr / wtable\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    for s in samples:\n",
    "\n",
    "        index += 1\n",
    "        print('Normalizing {0}/{1}   '.format(index, len(samples)), end='\\r')\n",
    "\n",
    "        res.append(s)\n",
    "        count = int(3 * counts[s['target']]) - 1\n",
    "\n",
    "        for i in range(0, count):\n",
    "            res.append(copy_sample(s, augmentate))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentate(samples, gl_count, exgl_count):\n",
    "    print(\"augmentate\")\n",
    "\n",
    "    res = []\n",
    "    index = 0\n",
    "    for s in samples:\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "        if index % 1000 == 0:\n",
    "            print('Augmenting {0}/{1}   '.format(index, len(samples)), end='\\r')\n",
    "\n",
    "        count = gl_count if (s['meta'][8] == 0) else exgl_count\n",
    "\n",
    "        for i in range(0, count):\n",
    "            res.append(copy_sample(s))\n",
    "\n",
    "    print()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_df, meta_df, extragalactic=None, use_specz=False):\n",
    "    print(\"get_data\")\n",
    "    \n",
    "    # Me\n",
    "    i=0\n",
    "\n",
    "    samples = []\n",
    "    groups = data_df.groupby('object_id')\n",
    "\n",
    "    for g in groups:\n",
    "\n",
    "        id = g[0]\n",
    "\n",
    "        sample = {}\n",
    "        sample['id'] = int(id)\n",
    "\n",
    "        #object_id,ra,decl,gal_l,gal_b,ddf,hostgal_specz,hostgal_photoz,hostgal_photoz_err,distmod,mwebv,target\n",
    "        #615,349.046051,-61.943836,320.796530,-51.753706,1,0.0000,0.0000,0.0000,nan,0.017,92\n",
    "        meta = meta_df.loc[meta_df['object_id'] == id] ## joins the meta to the training data\n",
    "\n",
    "        if extragalactic == True and float(meta['hostgal_photoz']) == 0:\n",
    "            continue # returns control to the top of the for group, ignoring the following statements\n",
    "\n",
    "        if extragalactic == False and float(meta['hostgal_photoz']) > 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if 'target' in meta:\n",
    "            sample['target'] = np.where(classes == int(meta['target']))[0][0] # positional index of the classes array\n",
    "        else:\n",
    "            sample['target'] = len(classes) - 1\n",
    "\n",
    "        sample['meta'] = np.zeros(10, dtype = 'float32')\n",
    "\n",
    "        sample['meta'][4] = meta['ddf']\n",
    "        sample['meta'][5] = meta['hostgal_photoz']\n",
    "        sample['meta'][6] = meta['hostgal_photoz_err']\n",
    "        sample['meta'][7] = meta['mwebv']\n",
    "        sample['meta'][8] = float(meta['hostgal_photoz']) > 0\n",
    "\n",
    "        sample['specz'] = float(meta['hostgal_specz'])\n",
    "\n",
    "        if use_specz:\n",
    "            sample['meta'][5] = float(meta['hostgal_specz'])\n",
    "            sample['meta'][6] = 0.0\n",
    "\n",
    "        z = float(sample['meta'][5]) # If use_specz z becomes hostgal_specz; otherwise it's hostgal_photoz\n",
    "\n",
    "        #object_id,mjd,passband,flux,flux_err,detected\n",
    "        #615,59750.4229,2,-544.810303,3.622952,1\n",
    "\n",
    "\n",
    "        mjd      = np.array(g[1]['mjd'],      dtype='float32') # these create the pivoted arrays\n",
    "        band     = np.array(g[1]['passband'], dtype='int32')\n",
    "        flux     = np.array(g[1]['flux'],     dtype='float32')\n",
    "        flux_err = np.array(g[1]['flux_err'], dtype='float32')\n",
    "        detected = np.array(g[1]['detected'], dtype='float32')\n",
    "\n",
    "        mjd -= mjd[0] # this calculates the deltas\n",
    "        mjd /= 100 # Earth time shift in day*100\n",
    "        mjd /= (z + 1) # Object time shift in day*100\n",
    "\n",
    "\n",
    "        received_wavelength = passbands[band] # Earth wavelength in nm\n",
    "        received_freq = 300000 / received_wavelength # Earth frequency in THz\n",
    "        source_wavelength = received_wavelength / (z + 1) # Object wavelength in nm\n",
    "\n",
    "\n",
    "        sample['band'] = band + 1 # creates the band array from passband; converts 0 - 5 to 1 - 6\n",
    "\n",
    "        sample['hist'] = np.zeros((flux.shape[0], 8), dtype='float32') # 2 dimensional array\n",
    "        sample['hist'][:,0] = mjd\n",
    "        sample['hist'][:,1] = flux\n",
    "        sample['hist'][:,2] = flux_err\n",
    "        sample['hist'][:,3] = detected\n",
    "\n",
    "        sample['hist'][:,6] = (source_wavelength/1000)\n",
    "        sample['hist'][:,7] = (received_wavelength/1000)\n",
    "\n",
    "        set_intervals(sample) ## => this does wierd shit\n",
    "\n",
    "\n",
    "        flux_max = np.max(flux)\n",
    "        flux_min = np.min(flux)\n",
    "        flux_pow = math.log2(flux_max - flux_min)\n",
    "        sample['hist'][:,1] /= math.pow(2, flux_pow)\n",
    "        sample['hist'][:,2] /= math.pow(2, flux_pow)\n",
    "        sample['meta'][9] = flux_pow / 10\n",
    "\n",
    "\n",
    "        samples.append(sample)\n",
    "        \n",
    "        i=+1\n",
    "\n",
    "        if len(samples) % 1000 == 0:\n",
    "            print('Converting data {0}'.format(len(samples)), end='\\r')\n",
    "\n",
    "        if len(samples) >= limit:\n",
    "            break\n",
    "        if i >= 4:\n",
    "            break\n",
    "\n",
    "\n",
    "    print()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading train data...')\n",
    "\n",
    "train_meta = pd.read_csv('../input/training_set_metadata.csv')\n",
    "train_data = pd.read_csv('../input/training_set.csv')\n",
    "\n",
    "\n",
    "wtable = get_wtable(train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mywloss(y_true,y_pred):\n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_ohe, y_p, wtable):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    nb_pos = wtable\n",
    "\n",
    "    if nb_pos[-1] == 0:\n",
    "        nb_pos[-1] = 1\n",
    "\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss / y_ohe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, Y, size=80):\n",
    "\n",
    "    hist_input = Input(shape=X['hist'][0].shape, name='hist')\n",
    "    meta_input = Input(shape=X['meta'][0].shape, name='meta')\n",
    "    band_input = Input(shape=X['band'][0].shape, name='band')\n",
    "\n",
    "    band_emb = Embedding(8, 8)(band_input)\n",
    "\n",
    "    hist = concatenate([hist_input, band_emb])\n",
    "    hist = TimeDistributed(Dense(40, activation='relu'))(hist)\n",
    "\n",
    "    rnn = Bidirectional(GRU(size, return_sequences=True))(hist)\n",
    "    rnn = SpatialDropout1D(0.5)(rnn)\n",
    "\n",
    "    gmp = GlobalMaxPool1D()(rnn)\n",
    "    gmp = Dropout(0.5)(gmp)\n",
    "\n",
    "    x = concatenate([meta_input, gmp])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    output = Dense(15, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[hist_input, meta_input, band_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(i, samples_train, samples_valid):\n",
    "\n",
    "    samples_train += augmentate(samples_train, augment_count, augment_count)\n",
    "    patience = 1000000 // len(samples_train) + 5\n",
    "\n",
    "    train_x, train_y = get_keras_data(samples_train)\n",
    "    del samples_train\n",
    "    valid_x, valid_y = get_keras_data(samples_valid)\n",
    "    del samples_valid\n",
    "\n",
    "    print(\"All done for this test.\")\n",
    "   \n",
    "    model = get_model(train_x, train_y)\n",
    "\n",
    "    if i == 1: model.summary()\n",
    "    model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    print('Training model {0} of {1}, Patience: {2}'.format(i, num_models, patience))\n",
    "    filename = 'model_{0:03d}.hdf5'.format(i)\n",
    "    callbacks = [EarlyStopping(patience=patience, verbose=1), ModelCheckpoint(filename, save_best_only=True)]\n",
    "\n",
    "    model.fit(train_x, train_y, validation_data=(valid_x, valid_y), epochs=max_epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "\n",
    "    model = load_model(filename, custom_objects={'mywloss': mywloss})\n",
    "\n",
    "    preds = model.predict(valid_x, batch_size=batch_size2)\n",
    "    loss = multi_weighted_logloss(valid_y, preds, wtable)\n",
    "    acc = accuracy_score(np.argmax(valid_y, axis=1), np.argmax(preds,axis=1))\n",
    "    print('MW Loss: {0:.4f}, Accuracy: {1:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_data(train_data, train_meta, use_specz=use_specz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_models+1):\n",
    "    print(i)\n",
    "\n",
    "    samples_train, samples_valid = train_test_split(samples, test_size=valid_size, random_state=42*i)\n",
    "\n",
    "    train_model(i, samples_train, samples_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_elapsed=time.time() - pgm_start\n",
    "pgm_elapsedCpu=time.clock() - pgm_startCpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pgm_elapsed)\n",
    "print(pgm_elapsedCpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 for PLAsTiCC Baseline",
   "language": "python",
   "name": "plasticc-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
