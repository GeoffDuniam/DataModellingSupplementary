{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training set basic vector dataframe\n",
    "\n",
    "## Second model\n",
    "* Unpadded, \n",
    "* No calculations\n",
    "* Metadata table is not incorporated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional - we set up a customised Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full raw training set of feature vectors, unpadded, no calculations\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays.\n",
    "\n",
    "#### Note that there is no cast or collect_list to array (array (collect_list(...) )\n",
    "This changes the shape of the retrieved aray in the programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,\n",
    "array( collect_list(float(a.kv['mjd'])) ) as mjd,\n",
    "array( collect_list(int(a.kv['passband'])) ) as passband,\n",
    "array( collect_list( float(  a.kv['flux'] )  ) ) as flux,\n",
    "array( collect_list( float(  a.kv['flux_err']  ) ) ) as flux_err,\n",
    "array( collect_list(int(   a.kv['detected']   )) ) as detected\n",
    "from\n",
    "(\n",
    "select object_id,\n",
    "   MAP('mjd', mjd,'passband',passband,'flux',flux,'flux_err',flux_err,'detected',detected) AS kv\n",
    "from training_set\n",
    "\n",
    ") a\n",
    "group by object_id\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- specz: double (nullable = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- hist: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- mjd: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- passband: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [object_id#5, target#22, meta#0, specz#1, band#3, hist#4]\n",
      "   +- InMemoryRelation [object_id#5, target#22, meta#0, specz#1, band#3, hist#4], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- ObjectHashAggregate(keys=[object_id#5, target#22, meta#0, specz#1], functions=[collect_list(kv#2[passband], 0, 0), collect_list(kv#2[mjd], 0, 0), collect_list(kv#2[passband], 0, 0), collect_list(kv#2[flux], 0, 0), collect_list(kv#2[flux_err], 0, 0), collect_list(kv#2[detected], 0, 0)])\n",
      "            +- Exchange hashpartitioning(object_id#5, target#22, meta#0, specz#1, 200)\n",
      "               +- ObjectHashAggregate(keys=[object_id#5, target#22, meta#0, specz#1], functions=[partial_collect_list(kv#2[passband], 0, 0), partial_collect_list(kv#2[mjd], 0, 0), partial_collect_list(kv#2[passband], 0, 0), partial_collect_list(kv#2[flux], 0, 0), partial_collect_list(kv#2[flux_err], 0, 0), partial_collect_list(kv#2[detected], 0, 0)])\n",
      "                  +- *(2) Project [object_id#5, target#22, array(cast(ddf#16 as double), hostgal_specz#17, hostgal_photoz#18, hostgal_photoz_err#19, mwebv#21) AS meta#0, hostgal_specz#17 AS specz#1, map(mjd, mjd#6, passband, cast(passband#7 as double), flux, flux#8, flux_err, flux_err#9, detected, cast(detected#10 as double)) AS kv#2]\n",
      "                     +- *(2) BroadcastHashJoin [object_id#5], [object_id#11], Inner, BuildRight\n",
      "                        :- *(2) Filter isnotnull(object_id#5)\n",
      "                        :  +- HiveTableScan [object_id#5, mjd#6, passband#7, flux#8, flux_err#9, detected#10], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#5, mjd#6, passband#7, flux#8, flux_err#9, detected#10]\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                           +- *(1) Filter isnotnull(object_id#11)\n",
      "                              +- HiveTableScan [object_id#11, ddf#16, hostgal_specz#17, hostgal_photoz#18, hostgal_photoz_err#19, mwebv#21, target#22], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#11, ra#12, decl#13, gal_l#14, gal_b#15, ddf#16, hostgal_specz#17, hostgal_photoz#18, hostgal_photoz_err#19, distmod#20, mwebv#21, target#22]\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the vector table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='append'\n",
    "FORMAT='parquet'\n",
    "TABLE='full_training_pivot'\n",
    "\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
