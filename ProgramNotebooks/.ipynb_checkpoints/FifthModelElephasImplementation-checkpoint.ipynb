{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '8g'),\\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50)\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 1),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','LightCurve Demo - JupyterHub Elephas implementation')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler,VectorAssembler,OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import udf, col, array, lit\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import ArrayType, FloatType,IntegerType, DataType, DoubleType\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model # model and load_model from Plasticc\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iType=IntegerType()\n",
    "dType=DoubleType()\n",
    "fType=FloatType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 25\n",
    "#batch_size = 128\n",
    "#batch_size2 = 512\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95, 99], dtype='int32')\n",
    "class_names = ['class_6','class_15','class_16','class_42','class_52','class_53','class_62','class_64','class_65','class_67','class_88','class_90','class_92','class_95','class_99']\n",
    "class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1, 99: 1}\n",
    "\n",
    "# LSST passbands (nm)  u    g    r    i    z    y      \n",
    "passbands = np.array([357, 477, 621, 754, 871, 1004], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000000\n",
    "sequence_len = 256\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wtable(df):\n",
    "    \n",
    "    all_y = np.array(df.select('target').collect(), dtype = 'int32') \n",
    "\n",
    "    y_count = np.unique(all_y, return_counts=True)[1]\n",
    "\n",
    "    wtable = np.ones(len(classes))\n",
    "\n",
    "    for i in range(0, y_count.shape[0]):\n",
    "        wtable[i] = y_count[i] / all_y.shape[0]\n",
    "\n",
    "    return wtable    \n",
    "\n",
    "def mywloss(y_true,y_pred):\n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def multi_weighted_logloss(y_ohe, y_p, wtable):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    nb_pos = wtable\n",
    "\n",
    "    if nb_pos[-1] == 0:\n",
    "        nb_pos[-1] = 1\n",
    "\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss / y_ohe.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/backend/permute_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_df, input_dim, size=80):\n",
    "    def get_meta(x):\n",
    "        x=x[:,0:10]\n",
    "        return x\n",
    "    \n",
    "    def get_band(x):\n",
    "        x=x[:,10:266]\n",
    "        return x\n",
    "    \n",
    "    def get_hist(x):\n",
    "        # x=x[:,266:input_dim]  -- this is before we passed in the shape from input dim - to avoid two select calls\n",
    "        x=x[:,266:input_dim[0]]\n",
    "        x=Reshape((8,256))(x)\n",
    "        x=K.permute_dimensions(x, (0,2,1))\n",
    "        return x\n",
    "    \n",
    "    #raw_input  = Input(shape=train_df.select(\"features\").first()[0].shape, name='raw')\n",
    "    raw_input  = Input(shape=input_dim, name='raw')\n",
    "    \n",
    "    hist_input = Lambda(get_hist,  name=\"hist\")(raw_input)\n",
    "    meta_input = Lambda(get_meta,  name=\"meta\")(raw_input)\n",
    "    band_input = Lambda(get_band,  name=\"band\")(raw_input)\n",
    "    \n",
    "    band_emb = Embedding(8, 8)(band_input)\n",
    "    hist = concatenate([hist_input, band_emb])\n",
    "    hist = TimeDistributed(Dense(40, activation='relu'))(hist)\n",
    "    \n",
    "    rnn = Bidirectional(GRU(size, return_sequences=True))(hist)\n",
    "    rnn = SpatialDropout1D(0.5)(rnn)\n",
    "    \n",
    "    gmp = GlobalMaxPool1D()(rnn)\n",
    "    gmp = Dropout(0.5)(gmp)\n",
    "    \n",
    "    x = concatenate([meta_input, gmp])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output = Dense(15, activation='softmax')(x)\n",
    "    model = Model(inputs=[raw_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF=sqlContext.sql(\"\"\"\n",
    "select object_id, target,meta, band, mjd, flux, flux_err, detected, fwd_int, bwd_int, \n",
    "source_wavelength, received_wavelength\n",
    "from elephas_training_set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtable=get_wtable(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF Functions for vector creation - definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(x, sequence_len=sequence_len):\n",
    "    x = np.pad(x, (sequence_len,0), 'constant', constant_values=(0))\n",
    "    x= x[len(x)-sequence_len:len(x)]\n",
    "    return x\n",
    "\n",
    "def fwd_intervals(x):\n",
    "    x=np.ediff1d(x, to_begin = [0])\n",
    "    return x\n",
    "\n",
    "def bwd_intervals(x):\n",
    "    x=np.ediff1d(x, to_end = [0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues returning numpy arrays to pyspark UDFs.\n",
    "You'll get pickle errors because the funtion returns NumPy types which are not compatible with DataFrame API. You have to cast the function return back to a list, and then caset to Spark comparible data types.\n",
    "\n",
    "https://stackoverflow.com/questions/44965762/is-it-possible-to-store-a-numpy-array-in-a-spark-dataframe-column\n",
    "\n",
    "Example - \n",
    "\n",
    "\n",
    "get_padded_float_vectors = udf(\n",
    "    lambda arr: pad_array(arr).tolist(), \n",
    "    ArrayType(fType)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF Declarations for vector manipulation\n",
    "Note that we have included the UDF toDenseUDF - this is necessary because the Spark ML class VectorAssembler will cast the assembled vector to a sparse vector if there are a large number of zeros in the vector. This nor usually a problem, but for this problem we do need to pass in a static dense vector intop the Keras model in order to properly extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categorical = udf(\n",
    "    lambda arr:\n",
    "        [int(i+1 == arr) for i in range(num_classes)], \n",
    "        ArrayType(iType)       \n",
    ")\n",
    "\n",
    "get_padded_float_vectors = udf(\n",
    "    lambda arr: pad_array(arr), \n",
    "    ArrayType(fType)\n",
    ")\n",
    "\n",
    "get_padded_int_vectors = udf(\n",
    "    lambda arr: pad_array(arr), \n",
    "    ArrayType(iType)\n",
    ")\n",
    "\n",
    "toDenseUdf = udf(\n",
    "    lambda arr: Vectors.dense(arr.toArray()), \n",
    "    VectorUDT()\n",
    ")\n",
    "\n",
    "fwd_udf = udf(\n",
    "    lambda arr: fwd_intervals(arr), \n",
    "    ArrayType(fType)\n",
    ")\n",
    "\n",
    "bwd_udf = udf(\n",
    "    lambda arr: bwd_intervals(arr), \n",
    "    ArrayType(fType)\n",
    ")\n",
    "\n",
    "to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training set dataframe with the feature vectors\n",
    "\n",
    "This is how we convert the target to a categegorical, looks like ElephaseSTIMATOE doesn't want it\n",
    "\n",
    "                             to_vector(target_categorical(\"target\")).alias(\"targetV\"),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingVectorsDF = trainingVectorsDF.select(\\\n",
    "                       \"object_id\",\"target\", #target_categorical(\"target\").alias(\"target\"),\n",
    "                             to_vector(\"meta\").alias(\"meta\"),                           \n",
    "                             to_vector(get_padded_int_vectors(\"band\")).alias(\"band\"),\n",
    "                             to_vector(get_padded_float_vectors(\"mjd\")).alias(\"mjd\"),\n",
    "                             to_vector(get_padded_float_vectors(\"flux\")).alias(\"flux\"),\n",
    "                             to_vector(get_padded_float_vectors(\"flux_err\")).alias(\"flux_err\"),\n",
    "                             to_vector(get_padded_int_vectors(\"detected\")).alias(\"detect\"),\n",
    "                             to_vector(fwd_udf(get_padded_float_vectors(\"fwd_int\"))).alias(\"fwd_int\"),\n",
    "                             to_vector(bwd_udf(get_padded_float_vectors(\"bwd_int\"))).alias(\"bwd_int\"),\n",
    "                             to_vector(get_padded_float_vectors(\"source_wavelength\")).alias(\"source_wavelength\"),\n",
    "                             to_vector(get_padded_float_vectors(\"received_wavelength\")).alias(\"received_wavelength\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- meta: vector (nullable = true)\n",
      " |-- band: vector (nullable = true)\n",
      " |-- mjd: vector (nullable = true)\n",
      " |-- flux: vector (nullable = true)\n",
      " |-- flux_err: vector (nullable = true)\n",
      " |-- detect: vector (nullable = true)\n",
      " |-- fwd_int: vector (nullable = true)\n",
      " |-- bwd_int: vector (nullable = true)\n",
      " |-- source_wavelength: vector (nullable = true)\n",
      " |-- received_wavelength: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we create the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = ['object_id', 'target']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- meta: vector (nullable = true)\n",
      " |-- band: vector (nullable = true)\n",
      " |-- mjd: vector (nullable = true)\n",
      " |-- flux: vector (nullable = true)\n",
      " |-- flux_err: vector (nullable = true)\n",
      " |-- detect: vector (nullable = true)\n",
      " |-- fwd_int: vector (nullable = true)\n",
      " |-- bwd_int: vector (nullable = true)\n",
      " |-- source_wavelength: vector (nullable = true)\n",
      " |-- received_wavelength: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True , withMean=True)\n",
    "fitted_scaler = scaler.fit(trainingVectorsDF)\n",
    "trainingVectorsDF = fitted_scaler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hist = [\"mjd\", \"flux\", \"flux_err\", \"detect\", \"fwd_int\", \"bwd_int\", \"source_wavelength\", \"received_wavelength\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x in hist],\n",
    "    outputCol='hist')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaled_features = [\"meta\", \"band\", \"hist\"]\n",
    "for x in trainingVectorsDF.columns:\n",
    "    if x in scaled_features:\n",
    "        input_col = x\n",
    "        output_col=\"scaled_\"+x\n",
    "        \n",
    "        scaler = StandardScaler(inputCol=input_col, outputCol=output_col, withStd=True , withMean=True)\n",
    "        fitted_scaler = scaler.fit(trainingVectorsDF)\n",
    "        trainingVectorsDF = fitted_scaler.transform(trainingVectorsDF)\n",
    "        print(output_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaled_features = [\"scaled_meta\", \"scaled_band\", \"scaled_hist\"]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x in scaled_features],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingVectorsDF.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingVectorsDF=assembler.transform(trainingVectorsDF)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ignore = ['object_id', 'target']\n",
    "for x in trainingVectorsDF.columns:\n",
    "    if x not in ignore:\n",
    "        input_col = x\n",
    "        output_col=\"scaled_\"+x\n",
    "        \n",
    "        scaler = StandardScaler(inputCol=input_col, outputCol=output_col, withStd=True , withMean=True)\n",
    "        fitted_scaler = scaler.fit(trainingVectorsDF)\n",
    "        train_df = fitted_scaler.transform(trainingVectorsDF)\n",
    "        print(output_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingVectorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#trainingVectorsDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training, test and validation splits\n",
    "\n",
    "Note that this is where we apply the toDenseUDF function to ensure that the features vector is a dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = [.7, .3]\n",
    "seed = 42 # seed=0L  validation_df, \n",
    "train_df, test_df = trainingVectorsDF.select( toDenseUdf(\"scaled_features\").alias(\"features\"), \"target\").randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.6, .3, .1]\n",
    "seed = 42 # seed=0L  validation_df, \n",
    "train_df, test_df, validation_df = trainingVectorsDF.select( toDenseUdf(\"features\").alias(\"features\"), \"target\").randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.take(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.repartition(800)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfPartitions=train_df.rdd.getNumPartitions()\n",
    "print(dfPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 15 classes and 2314 features\n"
     ]
    }
   ],
   "source": [
    "nb_classes = len(classes)\n",
    "#input_dim = len(train_df.select(\"features\").first()[0])\n",
    "input_dim = train_df.select(\"features\").first()[0].shape\n",
    "\n",
    "print(f\"We have {num_classes} classes and {input_dim[0]} features\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Defining Transformers\n",
    "\n",
    "Up until now, we basically just read in raw data. Luckily, ```Spark ML``` has quite a few preprocessing features available, so the only thing we will ever have to do is define transformations of data frames.\n",
    "\n",
    "To proceed, we will first transform category strings to double values. This is done by a so called ```StringIndexer```. Note that we carry out the actual transformation here already, but that is just for demonstration purposes. All we really need is too define ```string_indexer``` to put it into a pipeline later on."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "string_indexer = StringIndexer(inputCol=\"target\", outputCol=\"index_category\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder = OneHotEncoder(inputCol=\"target\", outputCol=\"targetVec\")\n",
    "\n",
    "#pipeline = Pipeline(stages=[encoder])\n",
    "#pipeline.fit(train_df).transform(train_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get and compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1031 02:48:32.892550 140343809632000 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1031 02:48:32.941244 140343809632000 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1031 02:48:32.994191 140343809632000 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1031 02:48:33.590679 140343809632000 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1031 02:48:33.601851 140343809632000 deprecation.py:506] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there\n"
     ]
    }
   ],
   "source": [
    "model = get_model(train_df, input_dim)\n",
    "print(\"hi there\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "raw (InputLayer)                (None, 2314)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "band (Lambda)                   (None, 256)          0           raw[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "hist (Lambda)                   (None, 256, 8)       0           raw[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 8)       64          band[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 16)      0           hist[0][0]                       \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 256, 40)      680         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 160)     58080       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 256, 160)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 160)          0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "meta (Lambda)                   (None, 10)           0           raw[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 160)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 170)          0           meta[0][0]                       \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          21888       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 15)           1935        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 99,159\n",
      "Trainable params: 99,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 02:48:33.791811 140343809632000 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n",
    "print(\"hi there\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if i == 1: \n",
    "    model.summary()\n",
    "    print(\"Hi there\")\n",
    "    model.compile(optimizer=optimizer, loss=mywloss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Elephas model\n",
    "\n",
    "To lift the above Keras ```model``` to Spark, we define an ```Estimator``` on top of it. An ```Estimator``` is Spark's incarnation of a model that still has to be trained. It essentially only comes with only a single (required) method, namely ```fit```. Once we call ```fit``` on a data frame, we get back a ```Model```, which is a trained model with a ```transform``` method to predict labels.\n",
    "\n",
    "We do this by initializing an ```ElephasEstimator``` and setting a few properties. As by now our input data frame will have many columns, we have to tell the model where to find features and labels by column name. Then we provide serialized versions of our Keras model. We can not plug in keras models into the ```Estimator``` directly, as Spark will have to serialize them anyway for communication with workers, so it's better to provide the serialization ourselves. In fact, while pyspark knows how to serialize ```model```, it is extremely inefficient and can break if models become too large. Spark ML is especially picky (and rightly so) about parameters and more or less prohibits you from providing non-atomic types and arrays of the latter. Most of the remaining parameters are optional and rather self explainatory. Plus, many of them you know if you have ever run a keras model before. We just include them here to show the full set of training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 02:48:33.891923 140343809632000 deprecation_wrapper.py:119] From /home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adam=optimizers.nadam(lr=0.01)\n",
    "adam=optimizers.nadam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_40db93cba63f4db85f82"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimizer = 'nadam'\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"features\")             # These two come directly from pyspark,\n",
    "estimator.setLabelCol(\"target\")                 # hence the camel case. Sorry :)\n",
    "estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(80)  # We just use one worker here. Feel free to adapt it.\n",
    "estimator.set_epochs(2) # was max_epochs\n",
    "estimator.set_batch_size(batch_size) # was 128\n",
    "estimator.set_verbosity(2) # was 1\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\") # Was synchronous\n",
    "estimator.set_loss(mywloss) # was(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['accuracy']) ##(['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we set up the pipeline.\n",
    "\n",
    "Looks very similar to SparkFlow, n'est ce pas? This could be an interesting comparison!\n",
    "\n",
    "Defining pipelines is really as easy as listing pipeline stages. We can provide any configuration of Transformers and Estimators really, but here we simply take the three components defined earlier. Note that string_indexer and scaler and interchangable, while estimator somewhat obviously has to come last in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And train the model\n",
    "\n",
    "Note that at this stage, the only method we can call is ''fit''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hduser/.virtualenvs/Elephas/lib/python3.6/site-packages/keras/engine/saving.py:473: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(yaml_string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n",
      "Model trained in 547.3875677585602 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "fitted_pipeline = pipeline.fit(train_df) # Fit model to data\n",
    "elapsedTime=time.time()-start\n",
    "print(f\"Model trained in {elapsedTime} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "Model_Location=datetime.now().strftime('/user/hduser/RawData/Models/ElephasModel5_' + str(max_epochs) + '-Epochs-' + str(batch_size) + '-BatchSize-%H%M_%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fitted_pipeline.stages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = elephas.ml_model\n",
    "a.__file__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fitted_pipeline.stages[-1].save('HiThere.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we run the transform so we can get the predictions\n",
    "\n",
    "However, this will require modifications to the class file for ElephasEstimator and ElephasTransformer located here\n",
    "\n",
    " ~/.virtualenvs/Elephas/lib/python3.6/site-packages/elephas/ml_model.py\n",
    " \n",
    "because the \\_transform method as written runs the predict process using the model.predict_classes method. A keras Sequential() model class has this method, but the Model() class does not - you have to use the 'predict' method.\n",
    "\n",
    "What this entails is that you have to rewrite that method and overload it to incorporate the predict method if you're training on a keras Model class instead of a Sequential model.\n",
    "\n",
    "The overloaded method is described here - \n",
    "\n",
    "https://github.com/maxpumperla/elephas/issues/111\n",
    "\n",
    "and we will include a copy of the modified class statement in th github\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prediction = fitted_pipeline.transform(train_df,'Model') # Evaluate on train data.\n",
    "prediction = fitted_pipeline.transform(train_df) # <-- The same code evaluates test data.\n",
    "pnl = prediction.select(\"target\", \"prediction\")\n",
    "pnl.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df=validation_df.repartition(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE!\n",
    "\n",
    "Enable the display code in the ml_model.py definition on the driver machine to get the details, but it won't work if you run the job on batch because display is an ipython class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there - here are the predictions in array form'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([0.0000000e+00, 4.8938852e-29, 2.4161126e-38, 8.9783408e-18,\n",
       "        7.5355738e-36, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.4967731e-26, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
       " array([0.00000000e+00, 4.30397502e-27, 1.42300264e-35, 1.09210975e-16,\n",
       "        2.37756259e-33, 0.00000000e+00, 7.72808525e-37, 0.00000000e+00,\n",
       "        1.35205217e-24, 0.00000000e+00, 2.23152579e-36, 1.00000000e+00,\n",
       "        1.89933272e-37, 0.00000000e+00, 0.00000000e+00], dtype=float32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 1.0, 0.4054, 0.2102, 0.018, 1.0, 0.7537, 2.0, 3.0, 2.0, 1.0, 0.0, 5.0, 0.0, 0.0, 3.0, 1.0, 5.0, 1.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 3.0, 3.0, 4.0, 0.0, 2.0, 5.0, 5.0, 3.0, 4.0, 1.0, 1.0, 5.0, 0.0, 1.0, 5.0, 4.0, 3.0, 0.0, 5.0, 3.0, 2.0, 1.0, 5.0, 1.0, 5.0, 5.0, 2.0, 3.0, 5.0, 0.0, 4.0, 1.0, 3.0, 0.0, 5.0, 0.0, 5.0, 3.0, 4.0, 5.0, 3.0, 0.0, 4.0, 1.0, 2.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 0.0, 5.0, 4.0, 2.0, 3.0, 4.0, 3.0, 3.0, 3.0, 4.0, 3.0, 1.0, 2.0, 1.0, 4.0, 5.0, 2.0, 0.0, 3.0, 0.0, 3.0, 4.0, 4.0, 1.0, 2.0, 3.0, 5.0, 5.0, 0.0, 5.0, 3.0, 0.0, 0.0, 3.0, 0.0, 3.0, 5.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 0.0, 0.0, 2.0, 3.0, 3.0, 5.0, 5.0, 0.0, 0.0, 4.0, 1.0, 2.0, 4.0, 2.0, 2.0, 2.0, 5.0, 3.0, 1.0, 1.0, 5.0, 2.0, 4.0, 3.0, 4.0, 1.0, 0.0, 5.0, 2.0, 1.0, 1.0, 1.0, 3.0, 0.0, 5.0, 0.0, 1.0, 3.0, 3.0, 1.0, 0.0, 0.0, 4.0, 0.0, 4.0, 2.0, 4.0, 1.0, 1.0, 4.0, 1.0, 4.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 0.0, 1.0, 2.0, 1.0, 5.0, 2.0, 4.0, 4.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 0.0, 5.0, 0.0, 2.0, 4.0, 3.0, 5.0, 3.0, 1.0, 2.0, 5.0, 2.0, 0.0, 0.0, 3.0, 4.0, 1.0, 5.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 5.0, 2.0, 0.0, 4.0, 4.0, 2.0, 1.0, 0.0, 2.0, 4.0, 0.0, 5.0, 3.0, 2.0, 3.0, 3.0, 4.0, 0.0, 1.0, 5.0, 1.0, 3.0, 3.0, 4.0, 4.0, 0.0, 3.0, 0.0, 4.0, 0.0, 5.0, 3.0, 3.0, 4.0, 0.0, 1.0, 4.0, 1.0, 0.0, 2.0, 0.0, 5.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0018, 0.0884, -0.0044, 0.0103, 0.0763, 0.0352, 0.0085, 0.0038, 0.1187, 0.0024, 0.023, 0.0067, -0.0118, 0.3857, -0.009, 0.0015, -0.0077, -0.0075, 0.0121, 0.294, 0.0265, 0.0019, -0.0021, -0.0078, -0.0047, 0.0007, 0.1315, 0.1371, -0.0014, -0.0081, -0.0036, 0.045, 0.0166, -0.0004, -0.0147, -0.0011, 0.0436, -0.0047, 0.0099, -0.006, -0.0056, -0.0046, 0.0843, -0.0675, -0.0081, 0.0019, -0.011, 0.0149, 0.0143, 0.0055, 0.0042, 0.0215, 0.1094, -0.0056, -0.0227, -0.008, -0.021, -0.0264, 0.6493, 0.0863, 0.017, 0.0125, 0.0039, 0.7813, -0.013, 0.0141, 0.0018, 0.0729, 0.0029, -0.009, 0.1071, -0.0062, 0.1808, -0.0008, 0.0533, 0.0126, 0.0033, 0.0768, -0.0191, 0.1763, 0.0048, 0.016, 0.0353, -0.0061, -0.0191, -0.0066, 0.0268, 0.0992, 0.1022, 0.0308, 0.0113, -0.0013, 0.1442, -0.0104, -0.0272, 0.0711, 0.0581, 0.0006, 0.0586, 0.0122, 0.0408, 0.0129, 0.6758, 0.0039, 0.0109, -0.0534, -0.0045, 0.0045, 0.0031, -0.0048, 0.4516, 0.0571, 0.0046, -0.0048, 0.0088, -0.0181, 0.0148, -0.001, 0.0226, 0.0007, 0.0034, 0.0104, 0.0173, 0.0035, 0.0175, 0.0649, 0.005, 0.0088, 0.0316, 0.003, 0.0083, 0.045, 0.0799, 0.0039, -0.0129, 0.0016, -0.0102, -0.0003, 0.0204, -0.0116, 0.015, 0.0023, -0.0134, 0.0102, 0.001, 0.0122, -0.023, -0.0081, 0.018, 0.002, -0.0107, -0.008, 0.0005, 0.0109, 0.4321, -0.0069, 0.0856, -0.0052, -0.0021, 0.0003, 0.0066, 0.0181, 0.4113, -0.0109, 0.008, 0.0069, 0.0109, 0.0333, 0.0225, -0.0023, 0.0085, -0.0078, -0.0068, 0.0176, 0.0264, 0.1554, -0.0023, 0.0608, -0.0049, 0.0174, 0.0557, 0.0381, 0.0118, 0.0134, 0.7318, 0.0092, 0.4109, -0.003, 0.1772, 0.0101, 0.0904, -0.011, -0.0069, 0.1931, -0.0156, -0.0123, 0.4912, -0.0566, -0.0002, 0.002, -0.0056, 0.1579, -0.0097, 0.0029, 0.0008, -0.0191, 0.0125, -0.0009, 0.0045, -0.0012, -0.0059, -0.0118, 0.4591, 0.0107, 0.0574, -0.0017, 0.0038, 0.0031, 0.0082, 0.0053, 0.0634, -0.0031, 0.0106, 0.0232, 0.5477, 0.027, 0.31, 0.0111, 0.0236, 0.0786, 0.0164, 0.0267, 0.1742, 0.0247, 0.011, -0.0096, 0.1854, 0.0071, -0.0077, 0.0164, -0.0016, -0.0036, -0.0154, 0.091, 0.0079, -0.0087, 0.0066, -0.0167, 0.0073, 0.0086, 0.0117, -0.0056, 0.009, 0.0237, 0.0351, 0.0005, 0.0056, 0.0141, 0.0047, 0.0048, 0.0171, 0.0339, 0.0098, 0.0104, 0.01, 0.0046, 0.0333, 0.0075, 0.0118, 0.0123, 0.0086, 0.0102, 0.0319, 0.0126, 0.0129, 0.0122, 0.0139, 0.0091, 0.005, 0.0257, 0.0408, 0.0072, 0.0188, 0.005, 0.0081, 0.0371, 0.0148, 0.016, 0.0289, 0.0127, 0.0086, 0.0063, 0.026, 0.0082, 0.0071, 0.0064, 0.0276, 0.0041, 0.0314, 0.0455, 0.0079, 0.0079, 0.0361, 0.0153, 0.0178, 0.005, 0.0081, 0.0085, 0.028, 0.0104, 0.0294, 0.0073, 0.0126, 0.042, 0.0084, 0.0136, 0.0163, 0.0124, 0.009, 0.0122, 0.0402, 0.015, 0.019, 0.0302, 0.0188, 0.0072, 0.0381, 0.0116, 0.007, 0.008, 0.0115, 0.0118, 0.0079, 0.0083, 0.0107, 0.0074, 0.0133, 0.0056, 0.0165, 0.0125, 0.0315, 0.0125, 0.0154, 0.0103, 0.0136, 0.0143, 0.0169, 0.0118, 0.0044, 0.0087, 0.0126, 0.0317, 0.0306, 0.009, 0.0401, 0.0143, 0.0171, 0.0108, 0.0102, 0.016, 0.0152, 0.0401, 0.006, 0.0076, 0.0098, 0.0081, 0.0058, 0.0087, 0.0143, 0.0157, 0.0092, 0.0125, 0.0116, 0.0318, 0.0413, 0.0113, 0.0073, 0.0144, 0.0058, 0.0048, 0.0136, 0.0058, 0.0103, 0.0096, 0.0274, 0.009, 0.0056, 0.0139, 0.0376, 0.0059, 0.0137, 0.0105, 0.0165, 0.0038, 0.0142, 0.0511, 0.0084, 0.0036, 0.018, 0.0053, 0.0085, 0.0138, 0.031, 0.0092, 0.0057, 0.0102, 0.0086, 0.0126, 0.0087, 0.016, 0.0133, 0.01, 0.0121, 0.011, 0.016, 0.0051, 0.0075, 0.0099, 0.0108, 0.0147, 0.0135, 0.0061, 0.0121, 0.0144, 0.0073, 0.0133, 0.0119, 0.0197, 0.0104, 0.0134, 0.019, 0.0116, 0.0041, 0.0401, 0.006, 0.0143, 0.0136, 0.0054, 0.0107, 0.009, 0.0156, 0.0101, 0.0057, 0.01, 0.061, 0.013, 0.0122, 0.0177, 0.0145, 0.0271, 0.0095, 0.0112, 0.011, 0.0264, 0.0039, 0.0119, 0.0113, 0.0098, 0.0167, 0.0048, 0.0353, 0.0149, 0.0087, 0.004, 0.0112, 0.0124, 0.0098, 0.016, 0.0289, 0.0103, 0.0106, 0.0138, 0.0151, 0.0123, 0.0141, 0.0126, 0.0063, 0.0163, 0.01, 0.0461, 0.0096, 0.0047, 0.0136, 0.009, 0.016, 0.0196, 0.0042, 0.032, 0.0143, 0.0107, 0.0094, 0.0108, 0.0171, 0.0114, 0.0123, 0.0145, 0.0125, 0.0078, 0.0282, 0.0075, 0.0125, 0.0174, 0.0123, 0.0134, 0.0118, 0.0054, 0.0085, 0.006, 0.0136, 0.0246, 0.0132, 0.0052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.2903, 74.3647, 9.6389, 45.3306, 43.4224, 67.03, 78.3979, 25.8916, 0.1655, 166.9282, 89.0791, 208.9165, 2.9844, 19.1221, 25.7212, 154.0278, 34.8916, 59.6235, 19.2441, 98.1562, 11.3604, 17.3721, 24.6211, 8.9595, 23.9624, 37.7412, 36.1357, 126.7446, 6.6172, 150.0005, 68.5767, 31.1021, 7.9287, 29.6904, 52.1323, 18.2676, 82.353, 104.6738, 8.7466, 20.7798, 39.2847, 20.6226, 112.1982, 47.2905, 44.6553, 32.186, 91.6646, 63.7339, 22.5942, 5.6094, 6.8052, 38.687, 8.0103, 46.6094, 128.1167, 57.3408, 43.4141, 81.0801, 102.4619, 16.5322, 42.5469, 11.6934, 20.1606, 4.4922, 57.0796, 30.5981, 60.207, 60.4204, 7.9697, 14.2227, 54.3921, 65.0225, 19.0811, 22.5527, 45.6177, 64.5459, 18.6528, 42.4766, 7.0483, 20.7119, 59.1577, 16.7119, 102.624, 24.7822, 0.4868, 3.4097, 10.895, 9.4365, 56.2959, 36.7524, 96.9712, 116.9458, 92.1655, 55.7705, 51.498, 31.251, 16.6797, 1.9189, 0.1191, 8.04, 39.1162, 73.6094, 71.3115, 135.5234, 14.1963, 117.5957, 8.1748, 66.5664, 55.1289, 55.3672, 31.5459, 1.5693, 77.3213, 29.7021, 60.207, 151.7188, 37.748, 51.1709, 19.9609, 49.4443, 46.4785, 10.9053, 6.0391, 8.3086, 98.707, 36.916, 63.0088, 30.9287, 22.4355, 6.3076, 72.8545, 16.5938, 5.1006, 45.1426, 21.1826, 40.3525, 61.1719, 90.9795, 2.1377, 27.793, 225.8887, 20.5342, 231.9316, 34.0127, 66.9531, 6.0098, 14.2031, 147.1934, 13.1279, 170.1357, 11.2881, 65.8418, 32.9746, 24.2568, 47.1143, 26.3994, 53.0381, 7.3877, 209.3564, 109.3584, 30.9209, 38.3037, 75.6953, 27.1729, 49.9053, 37.1943, 20.8887, 34.0312, 30.7432, 63.1357, 46.6064, 7.2959, 59.4062, 161.0811, 1.8311, 17.043, 42.2549, 229.4639, 43.7871, 26.2256, 155.3047, 34.5957, 22.3223, 181.4199, 104.5781, 133.5557, 7.3164, 17.54, 167.8916, 148.5186, 81.0576, 110.1738, 27.542, 0.5771, 82.2881, 72.5605, 12.5791, 20.2344, 161.5293, 28.2637, 122.6895, 202.4385, 68.3086, 32.54, 11.9365, 68.0684, 3.7754, 62.7178, 105.6523, 228.5, 92.5117, 25.2695, 109.3271, 18.3447, 86.9463, 87.5244, 6.4434, 0.3711, 97.0059, 45.3193, 124.916, 140.6631, 28.8428, 30.165, 209.5918, 134.7285, 215.1836, 149.0273, 0.334, 173.2578, 125.4121, 9.457, 40.3848, 34.4375, 90.666, 165.7324, 183.0332, 17.9727, 117.2656, 193.2051, 227.7891, 16.3477, 110.2207, 82.5957, 28.8066, 12.0391, 0.1523, 82.8613, 234.4766, 153.0703, 36.6504, 114.3301, 195.8887, 5.4355, 138.9297, 8.2903, 74.3647, 9.6389, 45.3306, 43.4224, 67.03, 78.3979, 25.8916, 0.1655, 166.9282, 89.0791, 208.9165, 2.9844, 19.1221, 25.7212, 154.0278, 34.8916, 59.6235, 19.2441, 98.1562, 11.3604, 17.3721, 24.6211, 8.9595, 23.9624, 37.7412, 36.1357, 126.7446, 6.6172, 150.0005, 68.5767, 31.1021, 7.9287, 29.6904, 52.1323, 18.2676, 82.353, 104.6738, 8.7466, 20.7798, 39.2847, 20.6226, 112.1982, 47.2905, 44.6553, 32.186, 91.6646, 63.7339, 22.5942, 5.6094, 6.8052, 38.687, 8.0103, 46.6094, 128.1167, 57.3408, 43.4141, 81.0801, 102.4619, 16.5322, 42.5469, 11.6934, 20.1606, 4.4922, 57.0796, 30.5981, 60.207, 60.4204, 7.9697, 14.2227, 54.3921, 65.0225, 19.0811, 22.5527, 45.6177, 64.5459, 18.6528, 42.4766, 7.0483, 20.7119, 59.1577, 16.7119, 102.624, 24.7822, 0.4868, 3.4097, 10.895, 9.4365, 56.2959, 36.7524, 96.9712, 116.9458, 92.1655, 55.7705, 51.498, 31.251, 16.6797, 1.9189, 0.1191, 8.04, 39.1162, 73.6094, 71.3115, 135.5234, 14.1963, 117.5957, 8.1748, 66.5664, 55.1289, 55.3672, 31.5459, 1.5693, 77.3213, 29.7021, 60.207, 151.7188, 37.748, 51.1709, 19.9609, 49.4443, 46.4785, 10.9053, 6.0391, 8.3086, 98.707, 36.916, 63.0088, 30.9287, 22.4355, 6.3076, 72.8545, 16.5938, 5.1006, 45.1426, 21.1826, 40.3525, 61.1719, 90.9795, 2.1377, 27.793, 225.8887, 20.5342, 231.9316, 34.0127, 66.9531, 6.0098, 14.2031, 147.1934, 13.1279, 170.1357, 11.2881, 65.8418, 32.9746, 24.2568, 47.1143, 26.3994, 53.0381, 7.3877, 209.3564, 109.3584, 30.9209, 38.3037, 75.6953, 27.1729, 49.9053, 37.1943, 20.8887, 34.0312, 30.7432, 63.1357, 46.6064, 7.2959, 59.4062, 161.0811, 1.8311, 17.043, 42.2549, 229.4639, 43.7871, 26.2256, 155.3047, 34.5957, 22.3223, 181.4199, 104.5781, 133.5557, 7.3164, 17.54, 167.8916, 148.5186, 81.0576, 110.1738, 27.542, 0.5771, 82.2881, 72.5605, 12.5791, 20.2344, 161.5293, 28.2637, 122.6895, 202.4385, 68.3086, 32.54, 11.9365, 68.0684, 3.7754, 62.7178, 105.6523, 228.5, 92.5117, 25.2695, 109.3271, 18.3447, 86.9463, 87.5244, 6.4434, 0.3711, 97.0059, 45.3193, 124.916, 140.6631, 28.8428, 30.165, 209.5918, 134.7285, 215.1836, 149.0273, 0.334, 173.2578, 125.4121, 9.457, 40.3848, 34.4375, 90.666, 165.7324, 183.0332, 17.9727, 117.2656, 193.2051, 227.7891, 16.3477, 110.2207, 82.5957, 28.8066, 12.0391, 0.1523, 82.8613, 234.4766, 153.0703, 36.6504, 114.3301, 195.8887, 5.4355, 138.9297, 0.0, 0.5131, 0.623, 0.5131, 0.3941, 0.295, 0.8296, 0.295, 0.295, 0.623, 0.3941, 0.8296, 0.3941, 0.623, 0.7197, 0.623, 0.7197, 0.8296, 0.623, 0.623, 0.623, 0.7197, 0.295, 0.5131, 0.8296, 0.8296, 0.623, 0.7197, 0.3941, 0.3941, 0.8296, 0.295, 0.3941, 0.8296, 0.7197, 0.623, 0.295, 0.8296, 0.623, 0.5131, 0.3941, 0.8296, 0.3941, 0.8296, 0.8296, 0.5131, 0.623, 0.8296, 0.295, 0.7197, 0.3941, 0.623, 0.295, 0.8296, 0.295, 0.8296, 0.623, 0.7197, 0.8296, 0.623, 0.295, 0.7197, 0.3941, 0.5131, 0.7197, 0.8296, 0.7197, 0.7197, 0.8296, 0.7197, 0.295, 0.8296, 0.7197, 0.5131, 0.623, 0.7197, 0.623, 0.623, 0.623, 0.7197, 0.623, 0.3941, 0.5131, 0.3941, 0.7197, 0.8296, 0.5131, 0.295, 0.623, 0.295, 0.623, 0.7197, 0.7197, 0.3941, 0.5131, 0.623, 0.8296, 0.8296, 0.295, 0.8296, 0.623, 0.295, 0.295, 0.623, 0.295, 0.623, 0.8296, 0.3941, 0.3941, 0.5131, 0.5131, 0.5131, 0.623, 0.295, 0.295, 0.5131, 0.623, 0.623, 0.8296, 0.8296, 0.295, 0.295, 0.7197, 0.3941, 0.5131, 0.7197, 0.5131, 0.5131, 0.5131, 0.8296, 0.623, 0.3941, 0.3941, 0.8296, 0.5131, 0.7197, 0.623, 0.7197, 0.3941, 0.295, 0.8296, 0.5131, 0.3941, 0.3941, 0.3941, 0.623, 0.295, 0.8296, 0.295, 0.3941, 0.623, 0.623, 0.3941, 0.295, 0.295, 0.7197, 0.295, 0.7197, 0.5131, 0.7197, 0.3941, 0.3941, 0.7197, 0.3941, 0.7197, 0.623, 0.3941, 0.623, 0.3941, 0.3941, 0.3941, 0.3941, 0.3941, 0.623, 0.295, 0.3941, 0.5131, 0.3941, 0.8296, 0.5131, 0.7197, 0.7197, 0.5131, 0.623, 0.5131, 0.623, 0.5131, 0.5131, 0.295, 0.8296, 0.295, 0.5131, 0.7197, 0.623, 0.8296, 0.623, 0.3941, 0.5131, 0.8296, 0.5131, 0.295, 0.295, 0.623, 0.7197, 0.3941, 0.8296, 0.3941, 0.295, 0.5131, 0.295, 0.5131, 0.3941, 0.295, 0.8296, 0.5131, 0.295, 0.7197, 0.7197, 0.5131, 0.3941, 0.295, 0.5131, 0.7197, 0.295, 0.8296, 0.623, 0.5131, 0.623, 0.623, 0.7197, 0.295, 0.3941, 0.8296, 0.3941, 0.623, 0.623, 0.7197, 0.7197, 0.295, 0.623, 0.295, 0.7197, 0.295, 0.8296, 0.623, 0.623, 0.7197, 0.295, 0.3941, 0.7197, 0.3941, 0.295, 0.5131, 0.295, 0.8296, 0.5131, 0.5131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), target=11),\n",
       "  (12.0,)),\n",
       " (Row(features=DenseVector([0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.018, 0.0, 0.6382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.0303, -0.0234, -0.012, -0.0552, 0.0068, 0.0049, 0.0239, -0.0128, -0.0408, -0.0053, 0.0235, 0.01, -0.0715, 0.0794, 0.0028, -0.0222, -0.0298, -0.0503, 0.0288, 0.3324, 0.0003, 0.0481, 0.0454, -0.111, 0.0169, 0.0, 0.0139, 0.0086, -0.1498, -0.0014, 0.0063, -0.0057, 0.0459, -0.0595, 0.1172, 0.0154, 0.0636, -0.0402, -0.0081, -0.0274, -0.0112, 0.0123, -0.0245, -0.1071, -0.081, -0.0301, 0.0214, 0.0667, 0.0422, 0.0178, 0.0013, 0.0059, 0.0632, 0.054, -0.0046, 0.0088, -0.0143, 0.0036, -0.0104, -0.0485, 0.1634, 0.0075, -0.0341, 0.0265, -0.0731, 0.1103, -0.0172, 0.0045, 0.0258, -0.0066, 0.0758, 0.0134, 0.0027, -0.0295, -0.0002, 0.1613, 0.0138, -0.0228, -0.0046, -0.009, -0.0168, 0.041, -0.0266, -0.7614, -0.0156, 0.0042, -0.01, 0.0326, -0.0458, -0.0018, -0.0067, 0.0125, -0.0387, 0.1051, 0.0176, 0.3291, 0.017, -0.0152, -0.0703, 0.1302, 0.0121, -0.0164, -0.0206, 0.0124, 0.0012, -0.0171, -0.0248, 0.0344, 0.0189, 0.1541, 0.0182, 0.0477, -0.0025, -0.0296, -0.08, -0.0136, -0.0056, 0.0162, 0.0117, -0.0012, -0.011, 0.0626, 0.0428, -0.0084, 0.0028, -0.0285, 0.013, -0.0177, 0.0021, 0.0164, 0.0062, 0.0605, -0.0897, 0.0344, 0.0341, 0.0351, 0.0013, 0.0006, 0.0048, 0.0524, 0.1114, 0.0126, 0.0102, -0.0208, 0.0411, -0.056, 0.0364, 0.0137, 0.0417, 0.0702, 0.1452, 0.0073, -0.0082, 0.0373, 0.0923, 0.0503, -0.0213, 0.0666, 0.001, -0.0041, 0.0224, 0.0238, -0.0394, -0.0066, 0.0072, -0.0056, 0.0317, -0.0225, 0.0515, 0.0284, -0.0272, -0.0026, 0.087, -0.222, 0.0016, 0.0525, 0.049, -0.0766, 0.0783, -0.0021, -0.0346, 0.0051, -0.0117, -0.0061, 0.0081, 0.0181, -0.0391, 0.0352, -0.0536, 0.0117, -0.007, -0.007, -0.046, 0.0481, -0.0011, 0.0267, 0.0155, 0.0507, 0.1034, 0.0117, 0.0096, -0.0755, 0.0022, -0.0759, 0.021, -0.006, -0.0114, -0.0652, 0.0533, 0.0046, -0.027, -0.0186, -0.0007, -0.0393, -0.0084, -0.0458, 0.0117, -0.0011, 0.0017, 0.0512, 0.0222, -0.1049, -0.0005, -0.0103, 0.0304, 0.0239, -0.157, -0.0123, 0.022, 0.0078, 0.0851, -0.1136, 0.0071, -0.0003, 0.0346, -0.0182, -0.002, -0.0061, -0.008, -0.0155, -0.0368, -0.1862, -0.0049, 0.0049, 0.0438, 0.0199, 0.171, 0.0109, -0.0022, 0.0215, 0.0753, 0.2332, 0.0091, -0.007, 0.0, 0.0339, 0.03, 0.0303, 0.0343, 0.0391, 0.0164, 0.0138, 0.029, 0.0425, 0.1033, 0.0241, 0.0251, 0.0329, 0.0476, 0.1039, 0.0275, 0.0358, 0.037, 0.0486, 0.1132, 0.0156, 0.0153, 0.0312, 0.0475, 0.1091, 0.0309, 0.0379, 0.0392, 0.0475, 0.1036, 0.0327, 0.0213, 0.02, 0.0371, 0.0583, 0.1308, 0.0221, 0.0259, 0.0336, 0.0487, 0.1072, 0.0155, 0.0149, 0.034, 0.0538, 0.1186, 0.0208, 0.0205, 0.0367, 0.0561, 0.1278, 0.0215, 0.0219, 0.0417, 0.0646, 0.1273, 0.0398, 0.0203, 0.0245, 0.0317, 0.0424, 0.0956, 0.0262, 0.033, 0.0363, 0.0472, 0.109, 0.0152, 0.0134, 0.0296, 0.0433, 0.1019, 0.0167, 0.0154, 0.032, 0.0457, 0.1046, 0.026, 0.0216, 0.0205, 0.0186, 0.0169, 0.0331, 0.0576, 0.8178, 0.0223, 0.0292, 0.0342, 0.0445, 0.0972, 0.0143, 0.012, 0.0274, 0.0403, 0.0952, 0.0279, 0.0163, 0.0138, 0.0283, 0.0403, 0.0942, 0.029, 0.0427, 0.0367, 0.048, 0.1096, 0.0152, 0.015, 0.0312, 0.0469, 0.1041, 0.0155, 0.0134, 0.03, 0.0449, 0.1059, 0.0227, 0.0204, 0.0233, 0.0161, 0.0142, 0.0304, 0.05, 0.1231, 0.0148, 0.0161, 0.0291, 0.0429, 0.1001, 0.0195, 0.016, 0.0326, 0.0482, 0.1118, 0.0188, 0.0209, 0.0202, 0.0138, 0.0132, 0.0267, 0.04, 0.0921, 0.0151, 0.0142, 0.0284, 0.0406, 0.0983, 0.0153, 0.0139, 0.0275, 0.0402, 0.0939, 0.0231, 0.0193, 0.0189, 0.019, 0.0188, 0.0206, 0.0202, 0.0198, 0.0253, 0.033, 0.0439, 0.102, 0.0123, 0.011, 0.026, 0.0399, 0.0961, 0.0246, 0.0186, 0.0178, 0.0345, 0.0528, 0.1297, 0.0329, 0.0422, 0.0398, 0.0527, 0.1195, 0.0192, 0.0239, 0.0307, 0.0427, 0.0964, 0.0152, 0.0135, 0.029, 0.0431, 0.1007, 0.0145, 0.0134, 0.0288, 0.0434, 0.1031, 0.022, 0.0276, 0.0314, 0.0429, 0.097, 0.0259, 0.0356, 0.0362, 0.0497, 0.1148, 0.0207, 0.0195, 0.0355, 0.0525, 0.1237, 0.0185, 0.0171, 0.0328, 0.0487, 0.1144, 0.0342, 0.0362, 0.0352, 0.0146, 0.014, 0.0278, 0.0414, 0.0968, 0.0204, 0.019, 0.0354, 0.0525, 0.1229, 0.0203, 0.0192, 0.0354, 0.0528, 0.1248, 0.0146, 0.0146, 0.0271, 0.04, 0.0934, 0.0252, 0.031, 0.0385, 0.0527, 0.1206, 0.0128, 0.0112, 0.0266, 0.0402, 0.0944, 0.0131, 0.0117, 0.027, 0.0409, 0.0962, 0.0311, 0.0354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9127, 1.0023, 0.9931, 1.0008, 1.9819, 0.0076, 0.0077, 0.0109, 0.011, 2.953, 0.0076, 0.0076, 0.0109, 0.011, 3.0181, 0.0076, 0.0076, 0.0109, 0.011, 23.8933, 0.0076, 0.0077, 0.0109, 0.011, 2.8971, 0.0076, 0.0076, 0.011, 0.0109, 23.9353, 0.964, 0.0076, 0.0077, 0.0109, 0.0109, 2.9462, 0.0076, 0.0076, 0.011, 0.0109, 10.9563, 0.0077, 0.0076, 0.0109, 0.011, 2.9353, 0.0076, 0.0076, 0.011, 0.0109, 2.9662, 0.0076, 0.0076, 0.011, 0.0109, 3.954, 8.9982, 0.0076, 0.0076, 0.0109, 0.011, 8.9527, 0.0076, 0.0076, 0.011, 0.0109, 2.9589, 0.0076, 0.0077, 0.0109, 0.011, 2.9624, 0.0077, 0.0076, 0.0109, 0.011, 7.959, 0.9969, 1.0809, 243.2653, 0.0076, 0.0076, 0.011, 0.007, 9.9234, 0.0076, 0.0076, 0.0109, 0.011, 4.8686, 0.0076, 0.0076, 0.011, 0.0109, 5.9628, 4.9926, 0.0076, 0.0076, 0.011, 0.0109, 2.9351, 0.0076, 0.0076, 0.0109, 0.011, 11.9372, 0.0076, 0.0076, 0.011, 0.0109, 2.9446, 0.0076, 0.0077, 0.0109, 0.011, 5.981, 0.9766, 1.0054, 4.1158, 0.0076, 0.0076, 0.011, 0.0109, 12.8198, 0.0076, 0.0077, 0.0109, 0.011, 2.9133, 0.0076, 0.0076, 0.011, 0.0109, 7.9865, 0.9897, 1.0261, 1.9349, 0.0076, 0.0077, 0.0109, 0.011, 13.963, 0.0077, 0.0076, 0.0109, 0.011, 4.9471, 0.0076, 0.0076, 0.011, 0.0109, 0.974, 0.9877, 0.9994, 0.9993, 0.9999, 0.9984, 0.9997, 14.991, 0.0076, 0.0076, 0.011, 0.0109, 4.9609, 0.0076, 0.0076, 0.0109, 0.011, 2.9705, 247.3265, 0.0076, 0.0076, 0.011, 0.0109, 2.835, 0.0077, 0.0076, 0.0109, 0.011, 10.9401, 0.0078, 0.0076, 0.0109, 0.011, 3.9427, 0.0077, 0.0076, 0.0109, 0.011, 11.0213, 0.0077, 0.0076, 0.0109, 0.011, 2.8674, 0.0076, 0.0076, 0.011, 0.0109, 11.0291, 0.0076, 0.0076, 0.011, 0.0109, 2.8603, 0.0076, 0.0076, 0.011, 0.0109, 2.954, 0.0076, 0.0076, 0.0109, 0.011, 5.11, 0.8988, 1.0161, 2.9844, 0.0076, 0.0077, 0.0109, 0.011, 13.8628, 0.0076, 0.0077, 0.0109, 0.011, 2.9717, 0.0076, 0.0076, 0.011, 0.0109, 11.9422, 0.0076, 0.0076, 0.011, 0.0109, 11.967, 0.0077, 0.0076, 0.0109, 0.011, 3.9482, 0.0076, 0.0076, 0.0109, 0.011, 2.9612, 0.0076, 0.0076, 0.011, 0.0109, 1.9633, 0.9984, 0.0, 0.9127, 1.0023, 0.9931, 1.0008, 1.9819, 0.0076, 0.0077, 0.0109, 0.011, 2.953, 0.0076, 0.0076, 0.0109, 0.011, 3.0181, 0.0076, 0.0076, 0.0109, 0.011, 23.8933, 0.0076, 0.0077, 0.0109, 0.011, 2.8971, 0.0076, 0.0076, 0.011, 0.0109, 23.9353, 0.964, 0.0076, 0.0077, 0.0109, 0.0109, 2.9462, 0.0076, 0.0076, 0.011, 0.0109, 10.9563, 0.0077, 0.0076, 0.0109, 0.011, 2.9353, 0.0076, 0.0076, 0.011, 0.0109, 2.9662, 0.0076, 0.0076, 0.011, 0.0109, 3.954, 8.9982, 0.0076, 0.0076, 0.0109, 0.011, 8.9527, 0.0076, 0.0076, 0.011, 0.0109, 2.9589, 0.0076, 0.0077, 0.0109, 0.011, 2.9624, 0.0077, 0.0076, 0.0109, 0.011, 7.959, 0.9969, 1.0809, 243.2653, 0.0076, 0.0076, 0.011, 0.007, 9.9234, 0.0076, 0.0076, 0.0109, 0.011, 4.8686, 0.0076, 0.0076, 0.011, 0.0109, 5.9628, 4.9926, 0.0076, 0.0076, 0.011, 0.0109, 2.9351, 0.0076, 0.0076, 0.0109, 0.011, 11.9372, 0.0076, 0.0076, 0.011, 0.0109, 2.9446, 0.0076, 0.0077, 0.0109, 0.011, 5.981, 0.9766, 1.0054, 4.1158, 0.0076, 0.0076, 0.011, 0.0109, 12.8198, 0.0076, 0.0077, 0.0109, 0.011, 2.9133, 0.0076, 0.0076, 0.011, 0.0109, 7.9865, 0.9897, 1.0261, 1.9349, 0.0076, 0.0077, 0.0109, 0.011, 13.963, 0.0077, 0.0076, 0.0109, 0.011, 4.9471, 0.0076, 0.0076, 0.011, 0.0109, 0.974, 0.9877, 0.9994, 0.9993, 0.9999, 0.9984, 0.9997, 14.991, 0.0076, 0.0076, 0.011, 0.0109, 4.9609, 0.0076, 0.0076, 0.0109, 0.011, 2.9705, 247.3265, 0.0076, 0.0076, 0.011, 0.0109, 2.835, 0.0077, 0.0076, 0.0109, 0.011, 10.9401, 0.0078, 0.0076, 0.0109, 0.011, 3.9427, 0.0077, 0.0076, 0.0109, 0.011, 11.0213, 0.0077, 0.0076, 0.0109, 0.011, 2.8674, 0.0076, 0.0076, 0.011, 0.0109, 11.0291, 0.0076, 0.0076, 0.011, 0.0109, 2.8603, 0.0076, 0.0076, 0.011, 0.0109, 2.954, 0.0076, 0.0076, 0.0109, 0.011, 5.11, 0.8988, 1.0161, 2.9844, 0.0076, 0.0077, 0.0109, 0.011, 13.8628, 0.0076, 0.0077, 0.0109, 0.011, 2.9717, 0.0076, 0.0076, 0.011, 0.0109, 11.9422, 0.0076, 0.0076, 0.011, 0.0109, 11.967, 0.0077, 0.0076, 0.0109, 0.011, 3.9482, 0.0076, 0.0076, 0.0109, 0.011, 2.9612, 0.0076, 0.0076, 0.011, 0.0109, 1.9633, 0.9984, 0.0, 0.0, 0.357, 0.357, 0.357, 0.357, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.357, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.357, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.357, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.357, 0.357, 0.357, 0.357, 0.357, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.357, 0.357, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.621, 0.477, 0.754, 0.871, 1.004, 0.357, 0.357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), target=8),\n",
       "  (12.0,))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = fitted_pipeline.stages[-1]._transform(validation_df, useModel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- target: double (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|prediction|\n",
      "+------+----------+\n",
      "|  11.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   0.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   9.0|      13.0|\n",
      "|   0.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  13.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   6.0|      13.0|\n",
      "|   9.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  12.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  10.0|      13.0|\n",
      "|   6.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   6.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   1.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  12.0|      13.0|\n",
      "|  12.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   0.0|      13.0|\n",
      "|   4.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  12.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   7.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   8.0|      13.0|\n",
      "|   6.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   1.0|      13.0|\n",
      "|   3.0|      13.0|\n",
      "|   6.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   7.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|  10.0|      13.0|\n",
      "|  12.0|      13.0|\n",
      "|  11.0|      13.0|\n",
      "|   2.0|      13.0|\n",
      "|   7.0|      13.0|\n",
      "+------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pnl=pred.select(\"target\",\"prediction\")\n",
    "pnl.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023822855688470347\n"
     ]
    }
   ],
   "source": [
    "# Looks like prediction_and_label is a dataframe, not an RDD, so we need to cast it to an RDD in order to use .map\n",
    "prediction_and_label = pnl.rdd.map(lambda row: (row.target, row.prediction))\n",
    "metrics = MulticlassMetrics(prediction_and_label)\n",
    "print(metrics.precision())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.array([0.1118802 , 0.07325512, 0.01614694, 0.02659923, 0.07440449,\n",
    "        0.08224725, 0.06605115, 0.02206622, 0.0542385 , 0.04232709,\n",
    "        0.05887228, 0.0560729 , 0.12828934, 0.10508711, 0.08246218], dtype='float32')\n",
    "x2=np.array([0.11386207, 0.09637289, 0.01490068, 0.02821412, 0.06656378,\n",
    "        0.080683  , 0.07982644, 0.01722281, 0.04762489, 0.03941712,\n",
    "        0.07437494, 0.06128084, 0.08523843, 0.09439979, 0.10001823], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.664722222222222"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20393/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
