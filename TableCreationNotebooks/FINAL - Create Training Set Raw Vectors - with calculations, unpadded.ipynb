{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training set basic vector dataframe - unpadded, with calculations\n",
    "\n",
    "## First! we set up the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new configuration\n",
    "conf = SparkConf().setAll([('spark.executor.memory', '4g'),\\\n",
    "                           ('spark.driver.memory', '4g'),\\\n",
    "                           ('spark.driver.maxResultSize', 0), \\\n",
    "                           ('spark.shuffle.service.enabled', True), \\\n",
    "                           ('spark.dynamicAllocation.enabled', True), \\\n",
    "                           #('spark.executor.instances', 50), \\\n",
    "                           ('spark.dynamicAllocation.executorIdleTimeout', 600), \\\n",
    "                           ('spark.executor.cores', 4),\\\n",
    "                           ('spark.default.parallelism', 90),\\\n",
    "                           ('spark.executor.memoryOverhead', '4g'),\\\n",
    "                           ('spark.driver.memoryOverhead', '4g'),\\\n",
    "                           ('spark.scheduler.mode', 'FAIR'),\\\n",
    "                           ('spark.kryoserializer.buffer.max', '512m'),\\\n",
    "                           ('spark.app.name','Creating training set vectors - JupyterHub version')])# Show the current options\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                           ('spark.dynamicAllocation.maxExecutors', 90), \\\n",
    "\n",
    "\n",
    "# Stop the old context\n",
    "sc.stop()\n",
    "\n",
    "# And restart the context with the new configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "#import commands\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext, StorageLevel\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "LogFile=datetime.now().strftime('Create_vectors_%H_%M_%d_%m_%Y.log')\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('myapp')\n",
    "hdlr = logging.FileHandler(LogFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # manual garbag collection to stop leaks on Collect() gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_start=time.time()\n",
    "pgm_startCpu=time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_count = 35\n",
    "batch_size = 1000\n",
    "batch_size2 = 5000\n",
    "optimizer = 'nadam'\n",
    "num_models = 1\n",
    "use_specz = False\n",
    "valid_size = 0.1\n",
    "max_epochs = 1000\n",
    "\n",
    "limit = 1000000\n",
    "sequence_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use plasticc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, we create the full raw training set of feature vectors, unpadded, no calculations\n",
    "\n",
    "Next cell illustrates the original structured record with hist as an array of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_df=sqlContext.sql(\"\"\"\n",
    "with rawData as\n",
    "(\n",
    "    select ts.object_id,\n",
    "        case \n",
    "                when target= 6 then 0\n",
    "                when target= 15 then 1\n",
    "                when target= 16 then 2\n",
    "                when target= 42 then 3\n",
    "                when target= 52 then 4\n",
    "                when target= 53 then 5\n",
    "                when target= 62 then 6\n",
    "                when target= 64 then 7\n",
    "                when target= 65 then 8\n",
    "                when target= 67 then 9\n",
    "                when target= 88 then 10\n",
    "                when target= 90 then 11\n",
    "                when target= 92 then 12\n",
    "                when target= 95 then 13\n",
    "                when target= 99 then 14\n",
    "                else 14\n",
    "                end target,\n",
    "       array(0,0,0,0,ddf,hostgal_specz, hostgal_photoz,mwebv,case when hostgal_photoz > 0 then 1 else 0 end ,metaVal) as meta,\n",
    "       double(hostgal_specz) as specz,\n",
    "       MAP(\n",
    "            'mjd', 0,\n",
    "            'passband',passband,\n",
    "            'flux',flux / mods.HistModifier,\n",
    "            'flux_err',flux_err / mods.HistModifier,\n",
    "            'fwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'bwd_int', (mjd - first_value(mjd) over W) / (tsm.hostgal_photoz + 1),\n",
    "            'detected',0,\n",
    "            'source_wavelength', case \n",
    "                                    when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                    end,\n",
    "            'received_wavelength', 0\n",
    "    \n",
    "       ) AS kv\n",
    "    from training_set ts \n",
    "        inner join training_set_metadata tsm \n",
    "            on ts.object_id = tsm.object_id \n",
    "        inner join (\n",
    "            select\n",
    "            object_id,\n",
    "            log2(max(flux)- min(flux)) flux_pow,\n",
    "            pow(2,log2(max(flux)- min(flux)) ) as HistModifier,\n",
    "            log2(max(flux)- min(flux))/10 as metaVal\n",
    "            from training_set\n",
    "            group by object_id\n",
    "    \n",
    "        ) mods\n",
    "        on ts.object_id = mods.object_id\n",
    "    WINDOW W AS (PARTITION BY ts.object_id ORDER BY mjd)\n",
    ") \n",
    "select object_id, target,meta,\n",
    "collect_list(int(a.kv['passband']))as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'mjd', collect_list(float(a.kv['mjd'])) ,\n",
    "    'flux', collect_list(float(a.kv['flux']))  ,\n",
    "    'flux_err', collect_list(float(a.kv['flux_err']))  ,\n",
    "    'detected', collect_list(int(a.kv['detected']))  ,\n",
    "    'fwd_int', collect_list(float(a.kv['fwd_int']))  ,\n",
    "    'bwd_int', collect_list(float(a.kv['bwd_int']))  ,\n",
    "    'source_wavelength', collect_list(float(a.kv['source_wavelength']))  ,\n",
    "    'received_wavelength', collect_list(float(a.kv['received_wavelength']))  \n",
    ")) as hist\n",
    "from rawData a\n",
    "group by object_id, target,meta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getVectorsSql=\"\"\"\n",
    "select object_id,target,meta,specz,\n",
    "collect_list(int(a.kv['passband'])) as band,\n",
    "ARRAY(NAMED_STRUCT(\n",
    "    'mjd',                  collect_list(float(a.kv['mjd'])) ,\n",
    "    'passband',             collect_list(int(a.kv['passband'])) ,\n",
    "    'flux',                 collect_list(float(a.kv['flux'])) ,\n",
    "    'flux_err',             collect_list(float(a.kv['flux_err'])) ,\n",
    "    'detected',             collect_list(int(a.kv['detected'])) ,\n",
    "    'source_wavelength',    collect_list(float(a.kv['source_wavelength'])) ,\n",
    "    'received_wavelength',  collect_list(float(a.kv['received_wavelength'])) \n",
    "    )\n",
    ") as hist\n",
    "from\n",
    "(\n",
    "select ts.object_id,target,array(ddf,hostgal_specz, hostgal_photoz, hostgal_photoz_err, mwebv) as meta,double(hostgal_specz) as specz,\n",
    "   MAP(\n",
    "        'mjd', mjd,\n",
    "        'passband',passband,\n",
    "        'flux',flux,\n",
    "        'flux_err',flux_err,\n",
    "        'detected',detected,\n",
    "        'received_frequency', case \n",
    "                                when ts.passband = 0 then 300000 /357\n",
    "                                when ts.passband = 1 then 300000 /477\n",
    "                                when ts.passband = 2 then 300000 /621\n",
    "                                when ts.passband = 3 then 300000 /754\n",
    "                                when ts.passband = 4 then 300000 /871\n",
    "                                else  300000 / 1004\n",
    "                                end,\n",
    "        'received_wavelength', case \n",
    "                                when ts.passband = 0 then 357/1000\n",
    "                                when ts.passband = 1 then 477/1000\n",
    "                                when ts.passband = 2 then 621/1000\n",
    "                                when ts.passband = 3 then 754/1000\n",
    "                                when ts.passband = 4 then 871/1000\n",
    "                                else  1004/1000\n",
    "                                end,\n",
    "        'source_wavelength', case \n",
    "                                when ts.passband = 0 then 357 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                when ts.passband = 1 then 477 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                when ts.passband = 2 then 621 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                when ts.passband = 3 then 754 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                when ts.passband = 4 then 871 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                else 1004 / (tsm.hostgal_photoz + 1)/1000\n",
    "                                end\n",
    "\n",
    "   ) AS kv\n",
    "from training_set ts inner join training_set_metadata tsm on ts.object_id = tsm.object_id\n",
    ") a\n",
    "group by object_id, target,meta, specz\n",
    "\"\"\"\n",
    "\n",
    "vectors_df=sqlContext.sql(getVectorsSql).cache()   #.persist(StorageLevel.MEMORY_ONLY_SER_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "struct_df.unpersist()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the schema for the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- object_id: integer (nullable = true)\n",
      " |-- target: integer (nullable = false)\n",
      " |-- meta: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- hist: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- mjd: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- flux_err: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- detected: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- fwd_int: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- bwd_int: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- source_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      " |    |    |-- received_wavelength: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "ObjectHashAggregate(keys=[object_id#9, target#5, meta#6], functions=[collect_list(cast(kv#8[passband] as int), 0, 0), collect_list(cast(kv#8[mjd] as float), 0, 0), collect_list(cast(kv#8[flux] as float), 0, 0), collect_list(cast(kv#8[flux_err] as float), 0, 0), collect_list(cast(kv#8[detected] as int), 0, 0), collect_list(cast(kv#8[fwd_int] as float), 0, 0), collect_list(cast(kv#8[bwd_int] as float), 0, 0), collect_list(cast(kv#8[source_wavelength] as float), 0, 0), collect_list(cast(kv#8[received_wavelength] as float), 0, 0)])\n",
      "+- ObjectHashAggregate(keys=[object_id#9, target#5, meta#6], functions=[partial_collect_list(cast(kv#8[passband] as int), 0, 0), partial_collect_list(cast(kv#8[mjd] as float), 0, 0), partial_collect_list(cast(kv#8[flux] as float), 0, 0), partial_collect_list(cast(kv#8[flux_err] as float), 0, 0), partial_collect_list(cast(kv#8[detected] as int), 0, 0), partial_collect_list(cast(kv#8[fwd_int] as float), 0, 0), partial_collect_list(cast(kv#8[bwd_int] as float), 0, 0), partial_collect_list(cast(kv#8[source_wavelength] as float), 0, 0), partial_collect_list(cast(kv#8[received_wavelength] as float), 0, 0)])\n",
      "   +- *(7) Project [object_id#9, target#5, meta#6, map(mjd, 0.0, passband, cast(passband#11 as double), flux, (flux#12 / HistModifier#3), flux_err, (flux_err#13 / HistModifier#3), fwd_int, ((mjd#10 - _we0#41) / (hostgal_photoz#22 + 1.0)), bwd_int, ((mjd#10 - _we1#42) / (hostgal_photoz#22 + 1.0)), detected, 0.0, source_wavelength, CASE WHEN (cast(passband#11 as int) = 0) THEN ((357.0 / (hostgal_photoz#22 + 1.0)) / 1000.0) WHEN (cast(passband#11 as int) = 1) THEN ((477.0 / (hostgal_photoz#22 + 1.0)) / 1000.0) WHEN (cast(passband#11 as int) = 2) THEN ((621.0 / (hostgal_photoz#22 + 1.0)) / 1000.0) WHEN (cast(passband#11 as int) = 3) THEN ((754.0 / (hostgal_photoz#22 + 1.0)) / 1000.0) WHEN (cast(passband#11 as int) = 4) THEN ((871.0 / (hostgal_photoz#22 + 1.0)) / 1000.0) ELSE ((1004.0 / (hostgal_photoz#22 + 1.0)) / 1000.0) END, received_wavelength, 0.0) AS kv#8]\n",
      "      +- Window [first(mjd#10, false) windowspecdefinition(object_id#9, mjd#10 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we0#41, first(mjd#10, false) windowspecdefinition(object_id#9, mjd#10 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS _we1#42], [object_id#9], [mjd#10 ASC NULLS FIRST]\n",
      "         +- *(6) Sort [object_id#9 ASC NULLS FIRST, mjd#10 ASC NULLS FIRST], false, 0\n",
      "            +- *(6) Project [object_id#9, CASE WHEN (target#26 = 6) THEN 0 WHEN (target#26 = 15) THEN 1 WHEN (target#26 = 16) THEN 2 WHEN (target#26 = 42) THEN 3 WHEN (target#26 = 52) THEN 4 WHEN (target#26 = 53) THEN 5 WHEN (target#26 = 62) THEN 6 WHEN (target#26 = 64) THEN 7 WHEN (target#26 = 65) THEN 8 WHEN (target#26 = 67) THEN 9 WHEN (target#26 = 88) THEN 10 WHEN (target#26 = 90) THEN 11 WHEN (target#26 = 92) THEN 12 WHEN (target#26 = 95) THEN 13 WHEN (target#26 = 99) THEN 14 ELSE 14 END AS target#5, array(0.0, 0.0, 0.0, 0.0, cast(ddf#20 as double), hostgal_specz#21, hostgal_photoz#22, mwebv#25, cast(CASE WHEN (hostgal_photoz#22 > 0.0) THEN 1 ELSE 0 END as double), metaVal#4) AS meta#6, passband#11, flux#12, HistModifier#3, flux_err#13, mjd#10, hostgal_photoz#22]\n",
      "               +- *(6) SortMergeJoin [object_id#9], [object_id#27], Inner\n",
      "                  :- *(3) Sort [object_id#9 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(object_id#9, 200)\n",
      "                  :     +- *(2) Project [object_id#9, mjd#10, passband#11, flux#12, flux_err#13, ddf#20, hostgal_specz#21, hostgal_photoz#22, mwebv#25, target#26]\n",
      "                  :        +- *(2) BroadcastHashJoin [object_id#9], [object_id#15], Inner, BuildRight\n",
      "                  :           :- *(2) Filter isnotnull(object_id#9)\n",
      "                  :           :  +- HiveTableScan [object_id#9, mjd#10, passband#11, flux#12, flux_err#13], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#9, mjd#10, passband#11, flux#12, flux_err#13, detected#14]\n",
      "                  :           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
      "                  :              +- *(1) Filter isnotnull(object_id#15)\n",
      "                  :                 +- HiveTableScan [object_id#15, ddf#20, hostgal_specz#21, hostgal_photoz#22, mwebv#25, target#26], HiveTableRelation `plasticc`.`training_set_metadata`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#15, ra#16, decl#17, gal_l#18, gal_b#19, ddf#20, hostgal_specz#21, hostgal_photoz#22, hostgal_photoz_err#23, distmod#24, mwebv#25, target#26]\n",
      "                  +- *(5) Sort [object_id#27 ASC NULLS FIRST], false, 0\n",
      "                     +- *(5) HashAggregate(keys=[object_id#27], functions=[max(flux#30), min(flux#30)])\n",
      "                        +- Exchange hashpartitioning(object_id#27, 200)\n",
      "                           +- *(4) HashAggregate(keys=[object_id#27], functions=[partial_max(flux#30), partial_min(flux#30)])\n",
      "                              +- *(4) Filter isnotnull(object_id#27)\n",
      "                                 +- HiveTableScan [object_id#27, flux#30], HiveTableRelation `plasticc`.`training_set`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [object_id#27, mjd#28, passband#29, flux#30, flux_err#31, detected#32]\n"
     ]
    }
   ],
   "source": [
    "vectors_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, we create the feature vector table in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='overwrite'\n",
    "FORMAT='parquet'\n",
    "#TABLE='training_set_flat_vectors' - this is the flattened model for the elephas sequential test\n",
    "#TABLE='training_set_vectors' - original full hist ARRAY - STRUCT - ARRAY\n",
    "TABLE='training_set_raw_vectors_unpadded_with_calcs'\n",
    "\n",
    "vectors_df.write.mode(MODE).format(FORMAT).saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark Elephas (Spark 2.3.0, python 3.6)",
   "language": "python",
   "name": "elephas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
